\chapter{Programación Dinámica}
\begin{itemize}
    \item Para problemas en los que necesitamos estados anteriores (en fibonacci, para calcular fibonacci(n) necesitamos tener fibonacci($n-1$) y fibonacci($n-2$), y para fibonacci($n-1$) necesitamos, \ldots).
    \item En el camino aparecen requisitos que se repiten (necesitamos calcular varias veces fibonacci(k)). En vez de calcularlo todas las veces, calcularlo una sola vez. Para calcular fibonacci(6) necesitamos 5 veces fibonacci(2).
    \item Antes de calcular el subproblema, mira si lo tienes ya resuelto (si lo tiene, lo usa y si no lo tiene, lo calcula).
    \item Es necesaria una estructura para almacenar las soluciones a los subproblemas, con la finalidad de ahorrar llamadas recursivas.
\end{itemize}
Ejemplos de dónde usar programación dinámica son:
\begin{itemize}
    \item Fibonacci.
    \item Calcular números combinatorios.
    \item Calcular potencias naturales.
    \item Cualquier problema con solapamiento de subproblemas (encontramos subproblemas que se repiten).
\end{itemize}
Una cosa es la programación dinámica y otra es la memorización:
\begin{description}
    \item [Memorización.] Almacenamos en una estructura (como un diccionario) los resultados.
    \item [Programación dinámica.] Una vez que los hemos almacenado, buscamos un patrón para ver cómo se completan las soluciones de alguna forma más eficiente (quitando sobrecarga por la recursividad). Buscamos una forma de rellenar la estructura de datos.
\end{description}

\subsubsection{Cuando aplicar programación dinámica}
Normalmente para problemas de optimización (minimizar o maximizar). La solución al problema la tenemos que ver como un proceso de selección de varias etapas.
\begin{itemize}
    \item Se aplica a problemas que pueden suponer un alto coste computacional que dispone de subestructuras optimales que se solapan (se repiten a lo largo del cálculo de la solución).
\end{itemize}
La eficiencia del algoritmo suele ser polinomial. Normalmente suele ser $O(n\cdot m)$ donde $n$ es el tamaño de la estructura de datos y $m$ el tiempo para cada casilla.

\subsubsection{Comparación con Divide y Vencerás}
\begin{description}
    \item [Divide y vencerás.] \
        \begin{itemize}
            \item Se aplica a subproblemas independientes.
            \item La técnica suele ser descendente.
        \end{itemize}
    \item [Programación dinámica.]  \
        \begin{itemize}
            \item Se aplica a subproblemas que se solapan (que se resuelven más de una vez).
            \item La técnica suele ser ascendente.
            \item Asegura optimilidad pero puede llevar a un algoritmo que no sea eficiente.
        \end{itemize}
\end{description}

\begin{teo}[Principio de Optimalidad de Bellman]\label{principio_optimalidad}
    Una solución óptima está compuesta de subsoluciones óptimas.
\end{teo}
Cuando se cumpla el principio, se podrá utilizar la programción dinámica.
\begin{ejemplo}
    Un ejemplo que no cumple el Principio de Optimalidad de Bellman es el problema del camino más largo entre dos nodos en un grafo. 

    Por tanto, no podemos aplicar programación dinámica para resolverlo.
\end{ejemplo}

\section{Pasos para desarrollar un algoritmo}
\begin{enumerate}
    \item Plantear la solución a un problema como una secuencia de decisiones.
    \item Ver que se verifica el principio de optimalidad~\ref{principio_optimalidad}.
    \item Plantear la solución como una función recursiva y ver la tipología de los subprobelmas.
    \item Ver cómo un problema grande se puede calcular a partir de los problemas más pequeños.
    \item Tratar de buscar un enfoque ascendente (resolver problemas pequeños y resolver problemas mayores).
\end{enumerate}
Ante un problema del estilo buscar un camino óptimo, es capaz de decir el costo del camino pero no de decir el camino. Para ello:
\begin{itemize}
    \item O se puede deducir el camino a partir del costo.
    \item O apuntar en una tabla auxiliar las decisiones tomadas.
\end{itemize}


