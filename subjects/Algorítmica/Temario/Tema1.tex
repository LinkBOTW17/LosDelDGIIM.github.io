\chapter{Eficiencia de Algoritmos}
La asignatura se centrará en eficiencia basada en el tiempo de ejecución (no en la eficiencia en cuanto espacio, memoria usada por el programa).

Para calcular la eficiencia de un algoritmo, tenemos tres métodos:
\begin{itemize}
    \item Método empírico: donde se mide el tiempo real.
    \item Método teórico: donde se mide el tiempo esperado.
    \item Método híbrido: tiempo teórico evitando las constantes mediante resultados empíricos.
\end{itemize}

\begin{prop}[Principio de Invarianza]
Dadas dos implementaciones $I1$, $I2$ de un algoritmo, el tiempo de ejecución para una misma instancia de tamaño $n$, $T_{I1}(n)$ y $T_{I2}(n)$, no diferirá en más de una constante multiplicativa. Es decir, $\exists K > 0$ que verifica: 
\begin{equation*}
T_{I1}(n) \leq K \cdot T_{I2}(n)
\end{equation*}

\end{prop}principio
Por lo que podremos despreciar las constantes.
En un principio, se asumiriá que operaciones básicas como sumas, multiplicaciones, $\ldots$ serán de tiempo constante, salvo excepciones (por ejemplo, multiplicaciones de números de 100000 dígitos).

\begin{definicion}[Notación O]
Se dice que un algoritmo $A$ es de orden $O(f(n))$, donde $f$ es una función $f:\mathbb{N}\rightarrow \mathbb{R}^{+}$, cuando existe una implementación del mismo tamaño cuyo tiempo de ejecución $T_A(n)$ es menor igual que $K \cdot f(n)$, donde $K$ es una constante real positiva a partir de un tamaño grande $n_0$. Formalmente:
\begin{equation*}
A \text{\ es\ } O(f(n)) \Leftrightarrow \exists K \in \mathbb{R}^{+}, \exists n_0 \in \mathbb{N} \mid T_A(n) \leq K \cdot f(n)\quad\forall n \geq n_0
\end{equation*}
\end{definicion}
La notación $O$ nos permite conocer cómo se comportará el algoritmo en términos de eficiencia en instancias del caso pero del problema, como mucho, sabemos que el algoritmo no tardará más de $K \cdot f(n)$ en ejecutarse, en el peor de los casos.

Al decir que el algoritmo $A$ es de orden $O(f(n))$, decimos que siempre podemos encontrar una constante positiva $K$ que para valores muy grandes del caso $n$ (a partir de un $n_0$), el tiempo de ejecución del algoritmo siempre será inferior a $K \cdot f(n)$:
\begin{equation*}
T_A(n) \leq K \cdot f(n)
\end{equation*}

Ejemplos de órdenes de eficiencia son:
\begin{itemize}
    \item Constante, $O(1)$.
    \item Logarítmico, $O(\log(n))$.
    \item Lineal, $O(n)$.
    \item Cuadrático, $O(n^2)$.
    \item Exponencial, $O(a^n)$.
    \item $\vdots$
\end{itemize}

\begin{prop}[Principio de comparación]\label{prop_comparacion}
Para saber si dos órdenes $O(f(n))$ y $O(g(n))$ son equivalentes o no, aplicamos las siguientes reglas:
\begin{gather*}
O(f(n)) \equiv O(g(n)) \Leftrightarrow \lim_{n \to \infty} \dfrac{f(n)}{g(n)} \rightarrow K \in \mathbb{R}^{+}\\
O(f(n)) > O(g(n)) \Leftrightarrow \lim_{n \to \infty} \dfrac{f(n)}{g(n)} \rightarrow \infty\\
O(f(n)) < O(g(n)) \Leftrightarrow \lim_{n \to \infty} \dfrac{f(n)}{g(n)} \rightarrow 0
\end{gather*}
Entendiendo que un órden es menor que otro si es mejor, es decir, más rápido en el caso asintótico.
\end{prop}

\begin{ejemplo}
Si tenemos dos algoritmos $A$ y $B$ con órdenes de eficiencia $O(n^2)$ y $O((4n+1)^2+n)$ respectivamente, tratamos de ver qué algoritmos es más eficiente:
\begin{equation*}
\lim_{n\to \infty} \dfrac{f(n)}{g(n)} = \lim_{n\to\infty}\dfrac{n^2}{(4n+1)^2+n)} = \lim_{n\to\infty}\dfrac{n^2}{(16n^2 +1+2\cdot 4n\cdot 1)+n} = \lim_{n\to\infty}\dfrac{1}{16}
\end{equation*}
Gracias a la Proposición~\ref{prop_comparacion}, tenemos que los algoritmos $A$ y $B$ son equivalentes.
\end{ejemplo}

\begin{ejemplo}
En esta ocasión, tenemos a dos algoritmos $A$ y $B$ con órdenes de eficiencia de $O(2^n)$ y $O(3^n)$, respectivamente.
\begin{equation*}
\lim_{n\to\infty}\dfrac{f(n)}{g(n)} = \lim_{n\to\infty}\dfrac{2^n}{3^n}=\lim_{n\to\infty}\left(\dfrac{2}{3}\right)^n = 0
\end{equation*}
Por la Proposición~\ref{prop_comparacion}, $A$ es más eficiente que $B$.
\end{ejemplo}

\begin{ejemplo}
El algoritmo $A$ tiene una eficiencia $O(n)$ y el algoritmo $B$ tiene una eficiencia de $O(n\log(n))$. Buscamos cuál es más eficiente.
\begin{equation*}
\lim_{n\to\infty}\dfrac{f(n)}{g(n)}=\lim_{n\to\infty}\dfrac{n}{n\log(n)}=\lim_{n\to\infty}\dfrac{1}{\log(n)} = 0
\end{equation*}
Por lo que $A$ es más eficiente que $B$, por la Proposción~\ref{prop_comparacion}.
\end{ejemplo}

\begin{ejemplo}
Disponemos de dos algoritmos, $A$ y $B$ con órdenes de eficiencia $O((n^2+29)^2)$ y $O(n^3)$ respectivamente. Intuimos que $B$ es más eficiente que $A$ pero queremos probarlo.
\begin{equation*}
\lim_{n\to\infty}\dfrac{f(n)}{g(n)} = \lim_{n\to\infty}\dfrac{(n^2+29)^2}{n^3} = \infty
\end{equation*}
Gracias a la Proposción~\ref{prop_comparacion}, hemos probado lo que esperábamos; $B$ es más eficiente que $A$.
\end{ejemplo}

\begin{ejemplo}
Se quiere probar que $O(\log(n))$ es más eficiente que $O(n)$.
\begin{equation*}
\lim_{n\to\infty}\dfrac{f(n)}{g(n)} = \lim_{n\to\infty}\dfrac{n}{\log(n)} = \lim_{n\to\infty}\dfrac{10^n}{n} = \infty
\end{equation*}
Por la Proposción~\ref{prop_comparacion}, lo acabamos de probar.
\end{ejemplo}

\begin{ejemplo}
Se quiere dar un ejemplo de que el órden de eficiencia de los logaritmos es equivalente sin importar la base de este. Podemos ver qué sucede con $O(\log_2(n))$ y con $O(\log_3(n))$:
\begin{equation*}
\lim_{n\to\infty}\dfrac{\log_3(n)}{\log_2(n)} = \lim_{n\to\infty}\dfrac{\ln(2)}{\ln(3)}
\end{equation*}
Por lo que ambos algoritmos tienen el mismo órden de eficiencia.
\end{ejemplo}

\begin{definicion}[Notación Omega]
Se dice que un algoritmo $A$ es de orden $\Omega(f(n))$, donde $f$ es una función $f:\mathbb{N}\rightarrow \mathbb{R}^{+}$, cuando existe una implementación del mismo tamaño cuyo tiempo de ejecución $T_A(n)$ es mayor igual que $K \cdot f(n)$, donde $K$ es una constante real positiva a partir de un tamaño grande $n_0$. Formalmente:
\begin{equation*}
A \text{\ es\ } \Omega(f(n)) \Leftrightarrow \exists K \in \mathbb{R}^{+}, \exists n_0 \in \mathbb{N} \mid T_A(n) \geq K \cdot f(n)\quad\forall n > n_0
\end{equation*}
La notación $\Omega$ nos permite conocer cómo se comportará el algoritmo en términos de eficiencia en instancias del caso mejor del problema.
Como poco, sabemos que el algoritmo no t ardará menos de $K\cdot f(n)$ en ejecutarse, en el mejor de los casos.
\end{definicion}

\begin{definicion}[Notación Theta]
Se dice que un algoritmo $A$ es de orden exacto $\theta(f(n))$, donde $f$ es una función $f:\mathbb{N}\rightarrow \mathbb{R}^{+}$, cuando existe una implementación del mismo tamaño cuyo tiempo de ejecución $T_A(n)$ es igual que $K \cdot f(n)$, donde $K$ es una constante real positiva a partir de un tamaño grande $n_0$. En este caso, el algoritmo es simultáneamente de orden $O(f(n))$ y $\Omega(g(n))$.
\begin{equation*}
A \text{\ es\ } \theta(f(n)) \Leftrightarrow \exists K \in \mathbb{R}^{+}, \exists n_0 \in \mathbb{N} \mid T_A(n) = K \cdot f(n)\quad\forall n > n_0
\end{equation*}
\end{definicion}

\subsubsection{Propiedades}
A continuación, vemos algunas propiedades de las notaciones anteriormente vistas:
\begin{description}
    \item Reflexiva.
\begin{equation*}
f(n) \in O(f(n))
\end{equation*}
También para las notaciones $\Omega$ y $\theta$.

\item Simétrica.
\begin{equation*}
f(n) \in \theta(g(n)) \Leftrightarrow g(n) \in \theta(f(n))
\end{equation*}

\item Suma.

Si $T_1(n) \in O(f(n))$ y $T_2(n) \in O(g(n))$. Entonces:
\begin{equation*}
T_1(n) + T_2(n) \in O(\max(f(n), g(n)))
\end{equation*}

\item Producto.

Si $T_1(n) \in O(f(n))$ y $T_2(n) \in O(g(n))$. Entonces:
\begin{equation*}
T_1(n) \cdot T_2(n) \in O(f(n) \cdot g(n))
\end{equation*}

\item Regla del máximo.
\begin{equation*}
O(f(n)+g(n)) = \max(O(f(n)), O(g(n)))
\end{equation*}

\item Regla de la suma.
\begin{equation*}
O(f(n)+g(n)) = O(f(n))+O(g(n))
\end{equation*}

\item Regla del producto.
\begin{equation*}
O(f(n)\cdot g(n)) = O(f(n))\cdot O(g(n))
\end{equation*}

\end{description}

Puede suceder que el tamaño del problema no depende de una única variable $n$, sino de varias.
En estos casos, se analiza de igual forma que en el caso de una variable, pero con una función de varias variables. Conocida una función $f:\mathbb{N}\times\mathbb{N}\rightarrow \mathbb{R}^{+}$:
\begin{equation*}
A \text{\ es\ } O(f(n,m)) \Leftrightarrow \exists K \in \mathbb{R}^{+} \mid T_A(n,m) \leq K \cdot f(n,m)\quad \forall n,m\in \mathbb{N}
\end{equation*}

\begin{ejemplo}
El órden de eficiencia del algoritmo canónico (el que todos conocemos) de suma de matrices $n\times m$ es de órden $O(n\cdot m)$.
\end{ejemplo}

\section{Análisis de algoritmos}
El primer paso a la hora de determinar la eficiencia de un algoritmo es identificar qué parámetro determina el tamaño del problema ($n$).
Posteriormente, tenemos que tener claro como se analiza cada estructura del código:
\begin{enumerate}
    \item Operaciones elementales.
    \item Secuencias de sentencias.
    \item Sentencias condicionales.
    \item Sentencias repetitivas.
    \item Llamadas a funciones no recursivas.
    \item Llamadas a funciones recursivas.
\end{enumerate}

\subsubsection{Sentencias simples u operaciones elementales}
Son aquellas instrucciones cuya ejecución no depende del tamaño del caso, como por ejemlo:
\begin{itemize}
\item Operaciones matemáticas básicas (sumas, multiplicaciones, $\ldots$).
\item Comparaciones.
\item Operaciones booleanas.
\end{itemize}

Su tiempo de ejecución está acotado superiormente por una constante. Su órden es $O(1)$.

\subsubsection{Secuencias de sentencias}
Constan de la ejecución de secuencias de bloques de sentencias:
\begin{listing}[H]
    \begin{minted}[xleftmargin=2cm]{c++}
Sentencia_1;
Sentencia_2;
// etc
Sentencia_r;
    \end{minted}
\end{listing}
Suponiendo que cada sentencia $i$ tiene eficiencia $O(f_i(n))$, la eficiencia de la secuencia se obtiene mediante las reglas de la suma y del máximo:
\begin{equation*}
O(f_1(n) + f_2(n) + \cdots + f_r(n)) = \max\left[O(f_1(n)), O(f_2(n)), \ldots, O(f_r(n))\right]
\end{equation*}

\begin{ejemplo}
Un ejemplo que puede parecer confuso es el siguiente:
\begin{listing}[H]
    \begin{minted}[xleftmargin=2cm]{c++}
if(n == 10){
    for(int i = 0; i < n; i++){
        cout << "Hola" << endl;
    }
}
    \end{minted}
\end{listing}
En este caso, se trata de un código de orden $O(1)$, ya que es para un valor fijo de $n$, $10$. 
\end{ejemplo}

\subsection{Sentencias condicionales}
Constan de la evaluación de una condición y la ejecución de un bloque de sentencias. Puede ejecutarse la \emph{Sentencia1} o la \emph{Sentencia2}, en función de la veracidad o falsedad de la condición:

\begin{listing}[H]
    \begin{minted}[xleftmargin=2cm]{c++}
if(condicion){
    Sentencia_1;
}else{
    Sentencia_2;
}
    \end{minted}
\end{listing}

\subsubsection{Peor caso}
El órden de eficiencia del peor caso (notación $O$) viene dado por:
\begin{equation*}
O(\mbox{estructura condicional}) = \max\left[O(\mbox{condicion}), O(\mbox{Sentencia1}), O(\mbox{Sentencia2})\right]
\end{equation*}
\begin{proof}
Como justificación para la fórmula, démonos cuenta de que la ejecución de la estructura condicional es igual a una de las siguientes secuencias de instrucciones (dependerá de la condición la ejecución de una o de otra):

\begin{listing}[H]
    \begin{minted}[xleftmargin=2cm]{c++}
bool a = condicion;
Sentencia_1;
    \end{minted}
\end{listing}
O:
\begin{listing}[H]
    \begin{minted}[xleftmargin=2cm]{c++}
bool a = condicion;
Sentencia_2;
    \end{minted}
\end{listing}

La notación $O$ trata de buscar el órden del mayor tiempo de ejecución, por lo que buscaremos la secuencia que más tarde de las dos:
\begin{equation*}
O(\mbox{estructura condicional}) = \max\left[O(\mbox{Secuencia1}), O(\mbox{Secuencia2})\right]
\end{equation*}
Usando la relga para secuencias de instrucciones vista anteriormente, podemos expresar cada órden como:
\begin{gather*}
O(\mbox{Secuencia1}) = \max\left[O(\mbox{condicion}), O(\mbox{Sentencia1})\right]\\
O(\mbox{Secuencia2}) = \max\left[O(\mbox{condicion}), O(\mbox{Sentencia2})\right]
\end{gather*}
Por lo que:
\begin{gather*}
O(\mbox{estructura condicional}) = \max\left[O(\mbox{Secuencia1}), O(\mbox{Secuencia2})\right] = \\
                                 = \max\left[\max\left[O(\mbox{condicion}), O(\mbox{Sentencia1})\right], \max\left[O(\mbox{condicion}), O(\mbox{Sentencia2})\right]\right] =\\
                                 = \max\left[O(\mbox{condicion}), O(\mbox{Sentencia1}), O(\mbox{Sentencia2})\right]
\end{gather*}
\end{proof}

\subsubsection{Mejor caso}
El órden de eficiencia del mejor caso (notación $\Omega$) viene dado por:
\begin{equation*}
\Omega(\mbox{estructura condicional}) = \Omega(\mbox{condicion}) + \min\left[\Omega(\mbox{Sentencia1}), \Omega(\mbox{Sentencia2})\right]
\end{equation*}
\begin{proof}
Buscamos expresar el órden de ejecución en el mejor caso. Al igual que en la justificación anterior, el bloque condicional es equivalante a seleccionar una de dos secuencias:
\begin{listing}[H]
    \begin{minted}[xleftmargin=2cm]{c++}
    bool a = condicion;
    Sentencia_1;
    \end{minted}
\end{listing}
\begin{listing}[H]
    \begin{minted}[xleftmargin=2cm]{c++}
    bool a = condicion;
    Sentencia_2;
    \end{minted}
\end{listing}
Nos damos cuenta de que la condición siempre se ejecuta y luego se ejecuta una sentencia. Como estamos en el mejor caso, seleccionamos la sentencia que tarde menos tiempo en ejecutarse. Por esto, tenemos que el órden de ejecución es la suma del órden de la condición más el mínimo del órden de las dos sentencias.
\end{proof}

\begin{ejemplo}
\ \\
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
if(n % 2 == 1){
    cout << "Es impar";
}else{
    // código de órden n
}
\end{minted}
\end{listing}
Aplicamos las fórmulas anteriormente vistas, sabiendo que la condición y la salida son de órden $O(1)$ al ser sentencias simples:
\begin{align*}
    O(\mbox{estructura condicional}) &= \max\left[O(\mbox{condicion}), O(\mbox{Sentencia1}), O(\mbox{Sentencia2})\right] \\
                                     &= \max\left[O(1), O(1), O(n)\right] = O(n)
\end{align*}
\begin{align*}
    \Omega(\mbox{estructura condicional}) &= \Omega(\mbox{condicion}) + \min\left[\Omega(\mbox{Sentencia1}), \Omega(\mbox{Sentencia2})\right] \\
                                          &= \Omega(1) + \min\left[\Omega(1), \Omega(n)\right] = \Omega(1)
\end{align*}

\end{ejemplo}

\subsection{Sentencias repetitivas}
Constan de la evaluación de uan condición y la ejecución de un bloque de sentencias, mientras que dicha condición se cumpla. Tienen la siguiente forma:
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
Mientras(condicion){
    BloqueSentencias;
}
\end{minted}
\end{listing}
Suponiendo que:
\begin{itemize}
\item El bloque de sentencias tiene eficiencia $f(n)$.
\item La evaluación de la condición tiene eficiencia $g(n)$.
\item El bucle se ejecuta $h(n)$ veces.
\end{itemize}
Entonces, la eficiencia será:
\begin{equation*}
O(\mbox{estructura repetitiva}) = O(g(n) + h(n) \cdot (g(n)+f(n)))
\end{equation*}

\begin{proof}
La condición se comprueba al menos una vez, por ello se suma.  
Cada iteración tiene un costo de $g(n) + f(n)$.  
El bucle realiza $h(n)$ iteraciones.
\end{proof}

\begin{ejemplo}
Dado el siguiente código, calcular su eficiencia:
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
while(n > 0){
    cout << n;
    n--;
}
\end{minted}
\end{listing}
\begin{itemize}
    \item Evaluación de la condición: $g(n) = 1$.  
    \item Bloque de sentencias: $f(n) = \max(O(1), O(1)) = 1$.  
    \item Repeticiones: $h(n) = n$.  
\end{itemize}
\begin{align*}
    O(\mbox{estructura repetitiva}) &= g(n) + h(n)\cdot (g(n)+f(n)) \\
                                    &= 1+n\cdot (1+1) = 2\cdot n+1 \Rightarrow O(2\cdot n+1)
\end{align*}
Aplicando la regla del máximo: $O(2\cdot n+1) = \max(O(2\cdot n), O(1)) = O(2\cdot n)$.  
Simplificando la constante: La secuencia repetitiva es $O(n)$.

\end{ejemplo}

\subsubsection{Bucles for}
Constan de la inicialización de una variable, comprobación de una condición y actualización de la variable. Se ejecutará un bloque de sentencias mientras que la condición se cumpla:
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
for(Inicialización; Condición; Actualización){
    BloqueSentencias;
}
\end{minted}
\end{listing}

Suponiendo que:
\begin{itemize}
\item El bloque de sentencias tiene eficiencia $f(n)$.
\item La evaluación de la condifición tiene eficiencia $g(n)$.
\item El bucle se ejecuta $h(n)$ veces.
\item La actualización tiene eficiencia $a(n)$.
\item La inicialización tiene eficiencia $i(n)$.
\end{itemize}

La eficiencia de una estructura de este tipo viene dada por:
\begin{equation*}
O(\mbox{for}) = O(i(n)+g(n) + h(n) \cdot (g(n)+f(n)+a(n)))
\end{equation*}

\begin{proof}
La inicialización tiene lugar una vez.
La condición se comprueba al menos una vez, por ello se suma.  
Cada iteración tiene un costo de $g(n) + f(n) + a(n)$.  
El bucle realiza $h(n)$ iteraciones.
\end{proof}

\begin{ejemplo}
Dado el siguiente código, calcular su eficiencia:
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
while(n > 0){
    for(int i = 1; i <= n; i*=2){
        cout << i;
    }
    n--;
}
\end{minted}
\end{listing}
Comenzamos analizando el bucle interno:
\begin{itemize}
\item Inicialización: $O(1)$.
\item Condición: $O(1)$.
\item Actualización: $O(1)$.
\item Bloque de sentencias: $O(1)$.
\item Veces que se ejecuta: $O(\log_2(n))$.
\end{itemize}
  
Eficiencia del bucle:
\begin{equation*}
O(1)+O(1)+O(\log_2(n))\cdot (O(1)+O(1)+O(1)) = O(\log_2(n))
\end{equation*}
  
Ahora, analizamos el bucle externo:
\begin{itemize}
\item Condición: $O(1)$.
\item Bloque de sentencias: $O(\log_2(n)+1)$.
\item Veces que se ejecuta: $O(n)$.
\end{itemize}
  
Eficiencia del bucle:
\begin{equation*}
O(1)+O(n)\cdot (O(1)+O(\log_2(n)+1)) = O(n\cdot \log_2(n))
\end{equation*}
Despreciando la base del logaritmo:
\begin{equation*}
O(n\log(n))
\end{equation*}
\end{ejemplo}

\subsection{Funciones no recursivas}
La eficiencia de la función persé se calcula como una secuencia de sentencias o bloques. Por otra parte, la eficiencia de la llamada a la función depende de si sus parámetros de entrada dependen o no del tamaño del problema. Esto se entenderá mejor en los siguientes ejemplos.

Dada la siguiente función (que usaremos en varios ejemplos):
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
bool esPrimo(int valor){
    double tope = sqrt(valor);
    for(int i = 2; i <=tope; i++){
        if(valor % i == 0)
            return false;
    }
    
    return true;
}
\end{minted}
\end{listing}
El cuerpo de la función persé tiene efieciencia $O(\sqrt{n})$.

\begin{ejemplo}
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
for(int i = 1; i<n; i++){
    if(esPrimo(1234567))
        cout << i;
}
\end{minted}
\end{listing}
El tiempo en calcular \verb|esPrimo(1234567)| es constante, luego es de eficiencia $O(1)$. Se repite $n$ veces, luego la eficiencia del bucle es de $O(n)$.
\end{ejemplo}

\begin{ejemplo}
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
for(int i = 1; i<n; i++){
    if(esPrimo(i))
        cout << i;
}
\end{minted}
\end{listing}
En este caso, es de eficiencia $O(n\sqrt{n})$:  
\begin{equation*}
\sum_{i=1}^n \sqrt{i} = \sqrt{1} + \sqrt{2} + \cdots + \sqrt{n} = \int_{1}^n \sqrt{n}\ dn = n\sqrt{n}
\end{equation*}
\end{ejemplo}

\begin{ejemplo}
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
for(int i = 1; i<2000; i++){
    if(esPrimo(i))
        cout << i;
}
\end{minted}
\end{listing}
El tiempo de ejecución no depende de $n$, siempre tarda lo mismo (es decir, es constante), luego es de orden $O(1)$.
\end{ejemplo}

\begin{ejemplo}
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
for(int i = n; i>0; i/=2){
    if(esPrimo(i))
        cout << i;
}
\end{minted}
\end{listing}
La eficiencia es de $O(\log(n)\sqrt{n})$, ya que repite $\log(n)$ veces una orde de eficiencia $O(\sqrt{n})$.
\end{ejemplo}

\begin{ejemplo}
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
int LlamarV(int *s, int N){
    for(int i = N-1; i > 0; i = i/2)
        V[i] = V[i]-1;
    return V[0];
}

void Ejemplo(int *v, int N){
    for(int i=0; i<N; i++){
        v[i] = (i*2+20-4*i)/N;
        v[i] = LlamarV(v, N-1)*LlamarV(v, N-2);
    }
}
\end{minted}
\end{listing}
La función \verb|LlamarV| tiene un tiempo de ejecución de $O(\log(n))$.  
El bucle de \verb|Ejemplo| se repite $n$ veces, luego es de orden $O(n\log(n))$.
\end{ejemplo}

\begin{ejemplo}
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
void ejemplo1(int n){
    int i, j, k;
    for(i = 0; i<n; i++){
        for(j = 0; j<n; j++){
            C[i][j] = 0;
            for(k = 0; k<n; k++){
                C[i][j] = A[i][k] - B[k][j];
            }
        }
    }
}
\end{minted}
\end{listing}
Tiene eficiencia $O(n^3)$.
\end{ejemplo}

\begin{ejemplo}
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
bool esPalindromo(char v[]){
    bool pal = true;
    int inicio = 0, fin = strlen(v)-1;
    
    while((pal) && (inicio<fin)){
        if(v[inicio] != v[fin])
            pal = true;
        inicio++;
        fin--;
    }
    
    return pal;
}
\end{minted}
\end{listing}
Se trata de un algoritmo de orden $O\left(\dfrac{n}{2}\right) = O(n)$.
\end{ejemplo}

\begin{ejemplo}
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
void F(int num1, int num2){
    for(int i = num1; i<= num2; i*=2){
        cout << i << endl;
    }
}
\end{minted}
\end{listing}
Tenemos un algoritmo de dos variables, con $n = num2- num1$. Se repite $\log(n)$ veces, luego eficiencia:
\begin{equation*}
O(\log(n)) = O(\log(num2-num1))
\end{equation*}
\end{ejemplo}

\begin{ejemplo}
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
void F(int* v, int num, int num2){
    int i = -1, j = num2;
    
    while(i <= j){
        do{
            i++; j--;
        }while(v[i]<v[j]);
    }
}
\end{minted}
\end{listing}
Es un algoritmo que no funciona correctamente en todos los cacos, en un caso puede no llegar a terminar.
\end{ejemplo}

\subsection{Funciones recursivas}
Se expresa como una ecuación en recurrencias y el orden de eficiencia es su solución.
Primero, suponemos que hay un caso base que se encarga de conocer la solución al problema en un caso menor.
Si la solución al caso base la podemos manipular para dar una solución a un caso mayor, tenemos el problema resuelto.

Para solucionar la ecuación en recurrencias, tratamos de buscar la expresión general de la ecuación y luego podremos resolverla, tal y como se elustra en los siguientes ejemplos. Sin emargo, como se apreciará en el Ejemplo~\ref{ej_fibonacci}, no resuelve todos los algoritmos recursivos. Para ello, deberemos avanzar en teoría.
\begin{ejemplo}[Factorial]
\ \\
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
unsigned long factorial(int n){
    if(n<=1) return 1;
    else return n*factorial(n-1);
}
\end{minted}
\end{listing}
La eficiencia de este algoritmo viene dada por la función $T(n)$:
\begin{equation*}
    \left\{ \begin{array}{l}
        T(n) = T(n-1)+1\\
        T(0) = T(1)=1
    \end{array}\right.
\end{equation*}
Tratamos de dar una expresión general a esta función:
\begin{align*}
    T(n) &= T(n-1)+1 = T(n-2)+2 = T(n-3)+3\\
    &= T(n-k)+k\quad \forall k \in \bb{N}
\end{align*}
Tomando $k = n-1$:
\begin{equation*}
    T(n) = T(n-(n-1))+(n-1) = T(1) + n-1 = \cancel{1}+n-\cancel{1} = n
\end{equation*}
Por lo que $T(n) \in O(n)$.
\end{ejemplo}

\begin{ejemplo}
\ \\
\begin{listing}[H]
\begin{minted}[xleftmargin=2cm]{c++}
int algoritmo(int n){
    if(n <= 1){
        int k = 0;
        for(int = 0; i < n; i++){
            k += k*i;
        }
        return k;
    }else{
        int r1, r2;
        r1 = algoritmo(n-1);
        r2 = algoritmo(n-1);
        return r1*r2;
    }
}
\end{minted}
\end{listing}
La eficiencia viene dada por $T(n)$:
\begin{equation*}
    \left\{ \begin{array}{l}
        T(n) = 2T(n-1) +1 \\
        T(1) = 1
    \end{array}\right.
\end{equation*}

Tratamos de resolver la ecuación:
\begin{align*}
    T(n) &= 2T(n-1) + 1\\
    T(n-1) &= 2T(n-2) + 1\\
    T(n-2) &= 2T(n-3) + 1
\end{align*}
\begin{align*}
    T(n) &= 2T(n-1) +1=2[2T(n-1)+1]+1 = 2^2T(n-2)+2+1 = \ldots\\
         &= 2^k T(n-k) + \sum_{i=0}^{k-1} 2^i \quad \forall k \in \bb{N}
\end{align*}
Para $k = n-1$:  
  
\begin{equation*}
2^{n-1}T(1) + \sum_{i=0}^{n-2} 2^i = \sum_{i=0}^{n-1} 2^i = 2^{n+1}-1\in O(2^n)
\end{equation*}
\end{ejemplo}

\begin{ejemplo}
Plantedada la siguiente función que define el tiempo de ejecución de un algoritmo, se pide determinar el orden de eficiencia del algoritmo:
\begin{equation*}
    \left\{ \begin{array}{l}
    T(n) = T(n-2) +1\\
    T(1) = 1\\
    T(0) = 1
    \end{array}\right.
\end{equation*}

Pasamos a resolver el ejercicio:

\begin{align*}
    T(n) &= T(n-2) +1\\
    T(n-2) &= T(n-4)+1 \\
    T(n-4) &= T(n-6) + 1 
\end{align*}
  
\begin{align*}
    T(n) &= T(n-4)+1+1 = T(n-6) + 3= T(n-8) + 4 =T(n-10)+5\\
         &=  T(n-2k) + k\quad \forall k \in \bb{N}
\end{align*}
  
\begin{description}
    \item Para $k = \dfrac{n}{2}$ si $n$ es par:
\begin{equation*}
T(n) = T\left(n-\dfrac{\cancel{2}n}{\cancel{2}}\right) + \dfrac{n}{2} = T(0) + \dfrac{n}{2} = 1+\dfrac{n}{2} \in O(n)
\end{equation*}

\item Para $k = \dfrac{n-1}{2}$ si $n$ es impar:
\begin{equation*}
T(n) = T\left(n-\dfrac{\cancel{2}(n-1)}{\cancel{2}}\right) + \dfrac{n-1}{2} = T(1) +\dfrac{n-1}{2} = 1+\dfrac{n-1}{2}\in O(n) 
\end{equation*}
\end{description}

\end{ejemplo}

\begin{ejemplo}[Fibonacci]\label{ej_fibonacci}
Dada la función del tiempo de ejecución de Fibonacci:
\begin{equation*}
    \left\{\begin{array}{l}
        T(n) = T(n-1)+T(n-2)+1 \\
        T(1) = T(0) = 1
    \end{array}\right.
\end{equation*}
Se pide calcular la expresión de la función para determinar su orden de eficiencia.
\begin{align*}
    T(n-1) &= T(n-2) + T(n-3) + 1\\
    T(n-2) &= T(n-3) +T(n-4)+1
\end{align*}
\begin{align*}
T(n) &= T(n-1)+T(n-2)+1 = T(n-2)+T(n-3)+1+T(n-3)+T(n-4)+1+1=\\
     &= T(n-3)+T(n-4)+1+2[T(n-4)+T(n-5)+1] + 1+T(n-5)+T(n-6)+1
\end{align*}
En resumen, este método no es útil para resolver este problema.
\end{ejemplo}

\section{Ecuación característica}
Se usa para resolver ecuaciones recurrentes que salen en análisis de eficiencia de algoritmos.
Vamos a ver dos etapas, que corresponden con cómo se solucionan:
\begin{itemize}
\item Ecuaciones lineales homogéneas de coeficientes constantes.
\item Ecuaciones lineales no homogéneas de coeficientes constantes.
\end{itemize}
  
En análisis de algoritmos nunca tendremos ecuaciones homogéneas, ya que es normal tener un $+1$ por sentencias constantes, usuales en código.
