\chapter{Introducción a la Programación Concurrente}
\section{Conceptos básicos}
Hasta ahora, nos hemos dedicado al estudio y desarrollo de programas secuenciales, que podemos entender de forma intuitiva como una ejecución lineal de instrucciones.\\

En programación concurrente, tendremos ahora múltiples unidades de ejecución independientes, a las que llamaremos procesos (sea un core o un procesador). La programación concurrente trata de coordinar los procesos para que cooperen entre sí con el fin de realizar un problema global de forma mucho más rápida de como lo haría un programa secuencial.\\

Podemos pensar que un proceso es una unidad de software abstracta conformada por con un conjunto de instrucciones a ejecutar y por el contexto del procesador (como los valores de los registros, el contador de programa, el puntero de pila, la memoria Heap, memoria para variables, el acceso a determinados recursos, \ldots), al que llamamos estado del proceso.\\

Cuando en esta asignatura aparezca ``flujo de control'', debemos pensar en una secuencia de ejecución de instrucciones. Es decir, como si fuera un proceso pero carante de un estado.\\

Nuestro trabajo en esta asignatura será gestionar la concurrencia, es decir, la ejecución independiente de dichos procesos con el fin de que no sea una sucesión de eventos incontrolados.

% // TODO: Cambiar este titulo
\subsection{Comparación de programas concurrentes con secuenciales}
Normalmente, en un programa concurrente tendremos más procesos que núcleos donde ejecutar dichos procesos, de donde aparece el concepto de concurrencia: en programación concurrente debe parecer que todos los procesos avanzan de forma simultánea, pese a haber más procesos que núcleos.\\

Si provocamos cambios de contexto dejando avanzar al resto de flujos de control, el programa no sufrirá las latencias provocadas por los procesos de E/S (por ejemplo), haciendo que el programa global sea más eficiente gracias a la concurrencia.\\

En sistemas que simulen el mundo real, podemos asociar un proceso con cada ente que intervenga en nuestro sistema (como una simulación del tráfico en una ciudad, o del movimiento de planetas), con lo que los sistemas de simulación pueden modelarse mejor con procesos concurrentes independientes, más que con programas secuenciales.

\subsection{Definición de concurrencia}
Podríamos definir la concurrencia como el paralelismo potencial que existe en los programa que puede aprovecharse independientemente de las limitaciones del hardware en el que se ejecuta el programa.\\

Como ya hemos mencionado, podremos tener un mayor número de procesos que de cores, y con este modelo cada uno de los procesos se ejecuta aparentemente al mismo tiempo que los demás.\\

El concepto de concurrencia es un concepto de programación a alto nivel que trata de representar el paralelismo potencial que existe en un programa. Con los compiladores adecuados, podemos programar en función de dichas características sin limitarnos por la arquitectura hardware del ordenador.\\

El objetivo fundamental de la concurrencia es simplificar toda la parte de la sincronización y comunicación entre los diferentes procesos de un programa, el cual suele ser un problema complejo sin solución fácil. Nos da un nivel algorítmico suficientemente independiente de los detalles del hardware para resolver dichos problemas, facilitando la portabilidad del código entre arquitecturas y lenguajes de programación.\\

\noindent
Como beneficios de este modelo abstracto (el de la concurrencia), podemos destacar:
\begin{itemize}
    \item Da herramientas, instrucciones y sentencias útiles para problemas de sincronización entre procesos.
    \item Las primitivas de programación en un lenguaje de alto nivel (como son los lenguajes concurrentes) son más fáciles de utilizar que con lenguajes de bajo nivel. Por ejemplo, los semáforos son más complejos que los monitores.
    \item Evita la dependencia con instrucciones de bajo nivel, haciendo que el programa pueda ejecutarse en otra computadora.
\end{itemize}

\subsection{Axiomas de la programación concurrente}
La programación concurrente es un modelo abstracto definido en base a 5 axiomas que nos dicen si un lenguaje es o no concurrente. En caso de no cumplirse, el código no va a poder ser transportable ni verificable.

Estos axiomas son:
\begin{description}
    \item [1. Atomicidad y entrelazamiento de instrucciones atómicas.]~\\
        Al menos ciertas instrucciones han de ser atómicas (esto es, instrucciones que no pueden ser interrumpidas, como por ejemplo las lecturas y escrituras en memoria).
    \item [2. Consistencia de los datos tras un acceso concurrente.]~\\
        Si tenemos muchos procesos actuando a la vez sobre un conjunto de datos compartidos, debemos estar seguros de que los accesos a los mismos no los estropeen.
    \item [3. Irrepetibilidad de las secuencias de instrucciones.]~\\
        Cuando se ejecuta un programa concurrente, se sucede un entrelazamiento de las instrucciones de los procesos que se ejecutan a la vez, con lo que la secuencia de instrucciones que obtenemos como resultado de volver a ejecutar el mismo programa con otros datos es muy probable que no sea la misma.

        Esto dificulta el debugging de un programa concurrente, ya que podemos tener un error en el programa que repercuta en el mal funcionamiento del mismo sólo cuando se suceda una secuencia de instrucciones específica en la ejecución del mismo.
    \item [4. Independencia de la velocidad de los procesos.]~\\
        No puede hacerse ninguna suposición en la velocidad de ejecución de un proceso, ya que este puede verse suspendido o ralentizado.

        La corrección en programas concurrentes no debe depender de la velocidad relativa de los procesos.
    \item [5. Hipótesis del progreso finito.]~\\
        \begin{itemize}
            \item Un proceso debe tratar de avanzar todo lo que pueda. Esto es, si un proceso se está ejecutando, debe tratar de ejecutar tantas instrucciones como sea posible.
            \item Una vez que un proceso comienza a ejecutar una sección de código, debe terminar dicha sección.
            \item Todo proceso debe seguir progresando durante la ejecución de un programa.
        \end{itemize}
\end{description}

\noindent
Cuando se interpreta la ejecución de un programa concurrente como un conjunto de trazas de las cuales elegimos una al ejecutar el programa, estamos ignorando ciertos detalles, como:
\begin{itemize}
    \item El estado de la memoria asignado a cada proceso.
    \item El valor de los registros de cada proceso.
    \item El coste computaciones de los cambios de contexto.
    \item La política de planificación que se emplea de los procesos.
    \item El desarrollo de los programas es independiente del hardware.
\end{itemize}

\section{Modelos para creación de procesos en un programa}
En relación al número de procesos que se ejecutan en un programa, podemos clasificarlos en:
\begin{itemize}
    \item Sistemas estáticos: El número de proesos en el programa es el mismo durante su ejecución. Dicho número se define al programarlo y en el momento de la compilación.
    \item Sistemas dinámicos: El número de procesos es variable, de forma que durante la ejecución del programa pueden crearse y destruirse procesos.
\end{itemize}

\subsection{Grafos de Sincronización}
Un Grafo de Sincronización es un Grafo Dirigido Acíclico (DAG) donde cada nodo representa una secuencia de sentencias del programa (o actividad). Nos sirven para definir situaciones de preferencia en la ejecución de un programa. Tenemos que tener instrucciones en el lenguaje concurrente que nos permitan representar el comienzo de las instrucciones con un DAG\@.\\

En un DAG, se succeden dependencias secuenciales, esto es, un proceso no empieza hasta que termina otro: dadas dos actividades $S_1$ y $S_2$, una arista desde la primera hacia la segunda ($S_1\rightarrow S_2$) significa que $S_2$ no puede comenzar su ejecución hasta que $S_1$ haya finalizado.\\

\begin{ejemplo}
    El DAG de la Figura~\ref{fig:primer_dag} nos indica que la primera actividad que tendrá lugar en nuestro programa será la actividad $S_1$. Tras el fin de esta, se sucederán de forma concurrente las actividades $S_2$ y $S_3$. Tras terminar $S_2$, comenzará $S_4$ y, tras esta, se ejecutarán de forma concurrente $S_5$ y $S_6$. Finalmente, tras el final de $S_5$, $S_6$ y $S_3$, el programa terminará con la actividad $S_7$.

    \begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \node[draw, circle, scale=0.7] (A) at (0,0) {$S_1$};
        \node[draw, circle, scale=0.7] (B) at (-1,-1) {$S_2$};
        \node[draw, circle, scale=0.7] (C) at (1,-1) {$S_3$};
        \node[draw, circle, scale=0.7] (D) at (-1,-2.5) {$S_4$};
        \node[draw, circle, scale=0.7] (E) at (-2,-3.5) {$S_5$};
        \node[draw, circle, scale=0.7] (F) at (0,-3.5) {$S_6$};
        \node[draw, circle, scale=0.7] (G) at (-1,-4.5) {$S_7$};
        
        \draw[-stealth] (A) -- (B);
        \draw[-stealth] (A) -- (C);
        \draw[-stealth] (B) -- (D);
        \draw[-stealth] (D) -- (E);
        \draw[-stealth] (D) -- (F);
        \draw[-stealth] (E) -- (G);
        \draw[-stealth] (F) -- (G);

        \draw[-stealth] (C) to[out=300, in=0] (G);
    \end{tikzpicture}
    \caption{Ejemplo de DAG}
    \label{fig:primer_dag}
    \end{figure}
\end{ejemplo}

\noindent
En relación a cómo podemos crear los procesos, destacamos dos formas que podemos encontrarnos en los lenguajes paralelos:

\subsection{Definición estructurada de procesos}
En programación estructurada, contaremos con dos palabras reservadas del lenguaje que nos permitirán recrear la siguiente funcionalidad a explicar. En pseudocódigo, nos referiremos a ellas como \verb|cobegin| y \verb|coend|.\\

Dados dos procesos $P_1$ y $P_2$ que queremos que se ejecuten de forma concurrente, bastará especificar en pseudocódigo:
\begin{minted}{pascal}
    cobegin
      P1; 
      P2; 
    coend
\end{minted}
Hasta llegar a la palabra \verb|cobegin| no comenzará ningún proceso. Tras esta, se sucederá un entrelazado de las instrucciones de $P_1$ y $P_2$, y no se saldrá de dicha región hasta que terminen ambos procesos.

\begin{ejemplo}
    Un programa utilizando la definición estructurada de procesos que cumpla el DAG de la Figura~\ref{fig:primer_dag} es el siguiente:
    \begin{minted}{pascal}
    begin
      S1;
      cobegin
        begin
          S2;
          S4;
          cobegin
            S5;
            S6;
          coend
        end
        S3;
      coend
      S7;
    end
    \end{minted}
\end{ejemplo}

\subsection{Definición no estructurada de procesos}
En lenguajes concurrentes que no cuenten con palabras reservadas que simulen \verb|cobegin| y \verb|coend|, contaremos con dos llamadas al sistema que nos permitirán replicar dicha funcionalidad para crear procesos:
\begin{description}
    \item [fork]~\\
        Duplica el proceso que actualmente se está ejecutando y lo lanza a ejecución. Si se el especifica una rutina, cambiará el código del clon por dicha rutina.
    \item [join]~\\
        Espera a que cierto proceso termine de ejecutarse antes de proseguir con la ejecución del resto de instrucciones.
\end{description}
La función \verb|fork| ya se vió en la asignatura de Sistemas Operativos, por lo que el estudiante debería estar familiarizado con ella.

\begin{ejemplo}
    Un programa con definición no estructurada de procesos para el DAG de la Figura~\ref{fig:primer_dag} es el siguiente:
    \begin{minted}{pascal}
    begin        
      S1;
      fork S3;
      S2;
      S4;
      fork S6;
      S5;
      join S3;
      join S6;
      S7;
    end
    \end{minted}
\end{ejemplo}

\section{Exclusión mutua y sincronización}
No todas las secuencias de entrelazamiento de un programa concurrente van a ser aceptables. Para impedir que sucedan ciertas secuencias (o trazas) tenemos condiciones de sincronización, relacionadas con instrucciones de lenguajes de programación de tal forma que dichas instrucciones no se ejecutan hasta que no es cierta una condición que depende de las variables del proceso.

De esta forma, una \textbf{condición de sincronización} es una restricción en el orden en que se pueden entremezclar las instrucciones que generan los procesos de un programa. Podemos utilizarlas para asegurarnos de que todas las trazas del programa son correctas.\\

La \textbf{exclusión mutua} es un caso particular de sincronización en el que se obliga a que un trozo de código de un proceso sea ejecutado de forma totalmente secuencial de manera que no se permita el entrelazamiento con otros procesos. Este trozo de código (en el que no se permite el entrelazamiento de instrucciones con otros procesos) recibe el nombre de \textbf{sección crítica}. Se dice que las secciones críticas se ejecutan en exclusión mutua.\\

La mayoría de instrucciones en un programa son instrucciones compuestas (esto es, formadas por varias instrucciones en lenguaje máquina). Si queremos establecer secciones críticas para la ejecución de cada una de dichas instrucciones, rodearemos la instrucción por \verb|<| y \verb|>|.

\begin{ejemplo}
    Por ejemplo, ante el siguiente código concurrente:
    \begin{minted}{pascal}
    begin
      x := 0;
      cobegin
        x := x+1;
        x := x-1;
      coend
    end
    \end{minted}
    El resultado obtenido en la variable \verb|x| es indeterminado, ya que puede ser 1, -1 o 0:
    \begin{itemize}
        \item El segundo proceso puede leer la variable \verb|x| antes de que el primero escriba en ella, leyendo 0; y podría escribir en ella después de que lo haga el primer proceso, escribiendo finalmente un -1.
        \item Podría ejecutarse el primer proceso antes que el segundo, dejando la variable \verb|x| a 1 y el segundo le cambiaría el valor a 0.
        \item El primer proceso puede leer la variable \verb|x| antes de que el segundo escriba en ella, leyendo 0; y podría escribir en ella después de que lo haga el segundo proceso, escribiendo finalmente un 1.
    \end{itemize}
    Notemos que esto sucede ya que la instrucción \verb|x := x OP a| es una instrucción compuesta de las instrucciones máquina: \verb|LOAD x|, \verb|OP a, x| y \verb|STORE x|.

    Sin embargo, ante el siguiente código concurrente:
    \begin{minted}{pascal}
    begin
      x := 0;
      cobegin
        < x := x+1 >;
        < x := x-1 >;
      coend
    end
    \end{minted}
    Obtenemos siempre 0 en \verb|x|, ya que las instrucciones de cada instrucción compeusta no se entrelazan, al ser secciones críticas. 
\end{ejemplo}

\subsubsection{Paradigma del Productor Consumidor}
El paradigma del productor/consumidor es una situación de dos procesos que cooperan, uno escribiendo datos en una variable, al que llamaremos productor; y otro que leerá dicha variable y realizará cálculos con ella, al que llamaremos consumidor.

Este paradigma nos sirve de ejemplo para justificar las condiciones de sincronización, así como para ponerlas en práctica.\\

Son necesarias condiciones de sincronización ya que no todas las trazas de ejecución de un programa con estructura productor/consumidor son correctas.

\begin{ejemplo}
    Si notamos por $L$ a las lecturas del consumidor y por $E$ a las escrituras del productor, las tres siguientes trazas de ejecución no son correctas:
    \begin{enumerate}
        \item \verb|L, E, L, E, ...|, porque leemos una lectura de la variable antes de que el productor escriba en ella, leyendo un valor indeterminado y puediendo provocar el fallo del programa.
        \item \verb|E, L, E, E, L, ...|, porque el consumidor se ha perdido una escritura del productor en la variable, que puede hacer que cambie la salida del programa a una errónea.
        \item \verb|E, L, L, E, L, ...|, porque el consumidor ha usado un mismo dato dos veces, que también puede resultar en un mal funcionamiento del programa.
    \end{enumerate}
\end{ejemplo}

Para que el paradigma del productor/consumidor funcione correctamente, han de cumplirse las dos condiciones de sincronización siguientes:
\begin{enumerate}
    \item El consumidor no puede leer la variable hasta que el productor no haya escrito en ella. Cuando el consumidor lee, debe esperar a que el productor proporcione un nuevo dato antes de volver a leer.
    \item El productor no puede escribir un nuevo valor hasta que el consumidor haya leido el último dato escrito (salvo en el primer valor a escribir).
\end{enumerate}
Para cumplir con las condiciones de sincronización, deberemos añadir instrucciones en el código para que:
\begin{itemize}
    \item El consumidor se detenga la primera vez hasta que el productor escriba en la variable.
    \item Se impida un segundo ciclo del consumidor hasta que se produzca el siguiente dato.
    \item Se impida un segundo ciclo del productor hasta que el dato anterior no haya sido leído por el consumidor.
\end{itemize}

\section{Propiedades de los sistemas concurrentes}
Una propiedad de un sistema concurrente es un atributo que se cumple en toda la ejecución del sistema, mientras que el conjunto de todas sus ejecuciones (de todas las posibles trazas generadas en la ejecución) nos dan el comportamiento del sistema.

Cualquier propiedad de un sistema concurrente puede ser formulada como combinación de dos tipos de propiedades fundamentales:

\begin{itemize}
    \item Propiedades de seguridad (\textit{safety}): Una propiedad de este tipo afirma que hay un estado del programa que es inalcanzable.
    \item Propiedades de vivacidad (\textit{liveness}): Propiedades que afirman que en algún momento se alcanzará un estado deseado.
\end{itemize}

\subsection{Propiedades de seguridad (\textit{safety})}
Estas propiedades expresan determinadas condiciones que han de cumplirse durante toda la ejecución del programa. Cualquier propiedad que pueda ser formulada por la existencia de un estado inalcanzable, es una propiedad de seguridad. En dicho caso, deberíamos poder definir qué estado es inalcanzable y demostrar que el programa concurrente nunca puede llegar a dicho estado.

Las propiedades de seguridad pueden ya comprobarse en tiempo de compilación, ya que se cumplen independientemente de la ejecución concreta que sigue el sistema en tiempo de ejecución. Es por esto que se trata de una propiedad estática.\\

Ejemplos de problemas donde vemos propiedades de seguridad son:
\begin{itemize}
    \item El problema de la exclusión mutua: La condición de que dos procesos del programa no puedan ejecutar simultáneamente las instrucciones de una sección crítica es de seguridad.
    \item Problema del productor/consumidor: Todos los estados que lleven a una traza distinta de \verb|E, L, E, L,| \ldots son estados prohibidos.
    \item La situación de interbloqueo: Es una de las situaciones más críticas que se dan tras quebrantar una propiedad de seguridad, ya que hay procesos ocupando recursos que no están usando y que no liberarán.
\end{itemize}

\subsection{Propiedades de vivacidad (\textit{liveness})}
Las propiedades de vivacidad expresan que el sistema llegará en un futuro a cumplir determinadas condiciones (en un tiempo no indeterminado). En determinados ejemplos, dichas propiedades pueden entenderse como que las condiciones dinámicas de ejecución no lleven a que determinados procesos sean sistemáticamente adelantados por otros, no pudiendo avanzar en la ejecución de instrucciones útiles, de forma que el proceso sufra inanición (\textit{starvation}).

Para demostrar que una propiedad es de vivacidad, debemos definir un ``buen estado'' del programa, y demostrar que es alcanzable para todos los procesos en un determinado tiempo.\\

Ejemplos de problemas donde vemos propiedades de vivacidad son:
\begin{itemize}
    \item El problema de la exclusión mutua: Las secciones críticas se ejecutan por un proceso a vez. El sistema debe garantizar que, en la espera por entrar a una región crítica, no ocurra que un proceso sea siempre adelantado por otros procesos, llevando a que dicho proceso nunca ejecute la región crítica (que es nuestro estado deseado).
    \item El problema del productor/consumidor: Un proceso que quiera escribir o leer de la variable compartida ha de poder hacerlo en un tiempo finito.
\end{itemize}

Debemos notar que el axioma de proceso finito expuesto en secciones anteriores no tiene nada que ver con la ausencia de inanición:
\begin{itemize}
    \item El axioma de proceso finito afirma que los procesos no pueden quedarse parados arbitrariamente, sino que estos deben intentar ejecutar instrucciones conforme les sea posible.
    \item Un proceso puede estar ejecutando instrucciones en un bucle indefinido pero no avanzar en la ejecución de las instrucciones de su código (es decir, puede estar realizando un trabajo inútil). En este caso, se cumpliría el axioma del proceso finito pero no se cumpliría la propiedad de vivacidad, ya que el proceso sufriría inanición.
\end{itemize}

Como ya venimos avisando, el no cumplimiento de la propiedad de vivacidad puede llevar a uno o más procesos a un estado de inanición (es indefinidamente pospuesto por otros, de forma que no pueda realizar aquello para lo que está programado). Aunque dicha situación es menos grave que una situación de interbloqueo (ya que hace que el programa no avance nada), tenemos procesos inoperantes (que no realizan su trabajo), por lo que consideraríamos que el programa concurrente no es correcto.\\

De esta forma, un programa concurrente solo podrá ser completamente correcto cuando se demuestre que los procesos que lo integran no sufren inanición en ninguna de sus posibles ejecuciones.

\begin{ejemplo}[Cena de filósofos]
    Disponemos de cinco filósofos, $F_0$, $F_1$, $F_2$, $F_3$ y $F_4$, que dedican su vida a pensar y en algún momento desean comer. Acceden a una mesa redonda en la que hay un plato del que todos pueden comer, siempre y cuando dispongan de dos palillos. Sólo hay 5 palillos, estos distribuidos de forma que entre dos filósofos hay un palillo.

    Los filósofos son cabezotas, por lo que una vez congen un palillo, no están dispuestos a soltarlo. Ademś, no pueden arrebatar un palillo a otro filósofo por ir en contra de sus ideales morales.\\

    Ante la situación descrita, podemos llegar a ver los dos ejemplos siguientes:
    \begin{itemize}
        \item Si todos los filósofos cogen a la vez el palillo de su derecha, cada filósofo dispondrá de un palillo y no habrá más palillos libres. 

            Estamos ante una situación de interbloqueo: ningún filósofo puede comer y no podrá hacerlo jamás. Como resultado, todos los filósofos se morirán de hambre.
        \item Si, por ejemplo, los filósofos $F_0$ y $F_2$ (que rodean al filósofo $F_1$ en la mesa) conspiran para dejar morir de hambre al filósofo $F_1$ de forma que cuando $F_0$ deje el palillo que hay entre $F_0$ y $F_1$, $F_2$ coja el palillo que hay entre él y $F_1$ (y viceversa), conseguirán que $F_1$ nunca consiga sus dos palillos, llevando al filósofo a un estado de inanición y, posteriormente, la muerte.
    \end{itemize}
\end{ejemplo}

Esta asignatura trata de crear protocolos que podamos demostrar que cumplen con las propiedades de seguridad y vivacidad, con el fin de no llevar nunca a situaciones de interbloqueos, inanición de algún (os) proceso (s), o alguna de las malas situaciones comentadas anteriormente.

\section{Lógica de programas de Hoare y verificación de programas concurrentes}
\subsection{Corrección de los programas concurrentes}
En los programas secuenciales, para comprobar la corrección total de los mismos, debemos probar que el programa \textbf{termina} dando \textbf{salidas esperadas} ante determinadas entradas.

Diremos que un programa secuencial es \textit{parcialmente correcto} si, supuesto que este termine, entonces los resultados que obtiene tras ejecutarse son esperados.

En un programa secuencial, hay un único conjunto de datos de entrada que provoca un único conjunto de datos de salida. Esto no sucede en programas concurrentes, ya que el indeterminismo en la ejecución provoca distintas trazas posibles del programa, y es bastante probable que todas las trazas posibles no provoquen los mismos resultados.\\

Para extender la definición de programa correcto a los programs concurrentes, notemos primero que muchos de ellos están pensado para no terminar nunca, de forma que su fin esté relacionado con alguna situación de error. Los sistemas operativos o los cajeros automáticos son programas concurrentes que están pensados para que nunca terminen, por lo que no podemos decir que una condición necesaria para que un programa concurrente sea correcto es que termine.\\

Para llevar a cabo la verificación de software, es decir, la demostración de que un programa es correcto, podemos emplear diferentes métodos:
\begin{description}
    \item [Depuración de código.] Explorar algunas ejecuciones de un código y comprobar que dichas ejecuciones son aceptables porque se cumplen las propiedades previamente fijadas.

        Este método sirve para programas secuenciales, pero no para programas paralelos, ya que nos es imposible depurar un código ante todas las posibles combinaciones de distintas trazas de ejecución.
    \item [Razonamiento operacional.] Realizar un análisis de casos exhaustivo para explorar todas las posibilidades de secuencias de ejecución de un código con el fin de garantizar que todas son correctas.

        Es un método inviable para programas concurrentes. Por ejemplo, en un programa que use dos procesos, cada uno con 3 instrucciones atómicas, el número de posibles trazas de ejecución es de 20.
    \item [Razonamiento asertivo.] Realizar un análisis abstracto basado en Lógica Matemática que permita representar de forma abstracta los estados\footnote{Un estado del programa viene definido por los valores que tienen las variables del programa en determinado instante durante su ejecución.} concretos que un programa alcanza.
\end{description}
De esta forma, el únifco enfoque posible es el razonamiento asertivo.

\subsection{Introducción a la Lógica de Hoare}
\subsubsection{Axiomática del lenguaje}
Construiremos ahora un sistema lógico formal (SLF) que facilite la elaboración de asertos o proposiciones lógicas ciertas con una base lógico-matemática precisa.\\

Nuestro SLF estará formado por:
\begin{itemize}
    \item Símbolos: Como sentencias del lenguaje de programación, variables proposicionales, operadores, \ldots
    \item Fórmulas: Secuencias de símbolos bien formadas\footnote{Entendemos por esto a sucesiones de símbolos con un significado fácilmente entendible.}.
    \item Axiomas: Proposiciones que mediante un consenso se consideran verdaderas.
    \item Reglas de inferencia o de derivación: Reglas que nos permiten derivar fórmulas ciertas a partir de axiomas o de fórmulas que ya conocemos que son ciertas.
\end{itemize}

Podemos pensar en las reglas de inferencia como teoremas matemáticos: tienen unas hipótesis y unas tesis de forma que, en cualquier situación que las hipótesis sean ciertas, las tesis lo serán.

\begin{notacion}
    Notaremos a las reglas de inferencia por:
    \begin{equation*}
        (\text{nombre de la regla})\dfrac{H_1, H_2, \ldots, H_n}{C}
    \end{equation*}
    De forma que disponemos de $n$ hipótesis ($H_1$, $H_2$, \ldots, $H_n$) en conjunción que nos llevan a la tesis $C$.
\end{notacion}

Para proseguir con el detallamiento del SLF, es necesario antes la definición de interpretación:
\begin{definicion}[Interpretación]
    Sea $A$ el conjunto de todos los asertos o fórmulas lógicas, una interpretación será una aplicación de dominio $A$ y codominio el conjunto $\{V, F\}$\footnote{Cuyos elementos interpretamos como verdadero y falso.}.
\end{definicion}
De esta forma, dada una interpretación y un aserto $v$, podemos ver la veracidad o falsedad de $v$ gracias a la interpretación.\\

Para que las demostraciones de nuestro SLF sean confiables, este sistema debe ser seguro y completo. Fijada una interpretación:
\begin{itemize}
    \item Decimos que un sistema es seguro si todos los asertos son hechos ciertos.
    \item Decimos que un sistema es completo si todos los hechos ciertos son asertos.
\end{itemize}
A partir de ahora, supondremos que no hay diferencia entre asertos y hechos ciertos. Es decir, que nuestro sistema es seguro y completo.

\subsubsection{Lógica proposicional}
Las fórmulas del SLF que estamos construyendo se llaman proposiciones, y están formadas por:
\begin{itemize}
    \item Constantes proposicionales $\{V,F\}$.
    \item Variables proposicionales $\{p,q,r,\ldots\}$.
    \item Operadores lógicos $\{\lnot,\land,\lor,\rightarrow,\leftarrow, \longleftrightarrow \}$.
    \item Expresiones formadas por constantes, variables y operadores.
\end{itemize}
Al igual que sucedía en la asignatura de Lógica y Métodos Discretos, podemos extender la definición de las interpretaciones y aplicarlas sobre las proposiciones del lenguaje, mediante unas reglas ya conocidas.\\

De esta forma, diremos que:
\begin{itemize}
    \item Una fórmula es satisfacible si existe alguna ínterpretación que la satisfaga.
    \item Una fórmula será válida si se satisface en cualquier estado del programa (es decir, si cualquier interpretación la satisface).
\end{itemize}
Llamaremos a las fórmulas válidas tautologías.\\

Dentro de la lógica proposicional de este SLF son tautologías algunas fórmulas ya conocidas, como la distributiva de $\land$ y de $\lor$, la conmutatividad de las mismas, \ldots

\begin{definicion}
    Dadas dos fórmulas $P$ y $Q$, diremos que son equivalentes siempre y cuando que $P$ se satisfaga para una cierta interpretación si y solo si $Q$ se satisface para la misma interpretación.
\end{definicion}
Por ejemplo, $p\rightarrow q$ y $\lnot q\rightarrow p$ son fórmulas equivalentes.

\subsubsection{Lógica de programas}
Este SLF trata de hacer afirmaciones sobre la ejecución de un programa. Incluimos por tanto a los triples, que tienen la forma
\begin{equation*}
    \{P\}S\{Q\}
\end{equation*}
donde $P$ y $Q$ son asertos (llamados precondición y poscondición, respectivamente) y $S$ es una sentencia simple o estructurada de un lenguaje de programación. En $P$ y $Q$ podrán aparecer tanto variables lógico-matemáticas como variables del propio programa. Para distinguirlas, notaremos a las primeras con letas mayúsculas y a las segundas con minúsculas.\\

Un triple $\{P\}S\{Q\}$ se interpreta como cierto si, ante cualquier estado del programa que satisfaga $P$ y después de la ejecución de cualquier entrelazamiento de instrucciones atómicas de $S$\footnote{Por tanto, $S$ ha de finalizar en algún momento.}, llegamos a un estado del programa que satisfaga $Q$.\\

\begin{notacion}
    A partir de la notación de los triples, y siendo $P$ una fórmula del lenguaje, notaremos por
    \begin{equation*}
        \{P\}
    \end{equation*}
    Al conjunto de los estados del programa que verifican $P$.
\end{notacion}~\\

De esta forma, $\{V\}$ es el conjunto de todos los estados de un programa, ya que todos los estados de dicho programa verifican $V$.

Análogamente, $\{F\}$ se corresponde con el conjunto vacío.\\

\begin{notacion}
    Dadas $P$ y $Q$ asertos equivalentes, entonces obtenemos el mismo conjunto de estados del programa que verifican dichos asertos:
    \begin{equation*}
        \{P\} = \{Q\}
    \end{equation*}
    Sin embargo, para evitar la confusión con el operador de asignación, notaremos las igualdades entre los conjuntos de estados de un programa con el operador $\equiv$.
\end{notacion}

\begin{definicion}[Sustitución textual]
    Dado un aserto $P$, que contiene al menos una aparición libre de la variable $x$ y una expresión $e$, definimos la sustitución textual de $x$ por $e$, notado por $P_e^x$ como la sustitución textual de todas las ocurrencias libres de $x$ en $P$ por $e$.
\end{definicion}

Enumeramos ahora los axiomas de nuestra Lógica de programas:
\begin{description}
    \item [Axioma de la sentencia nula $\{P\}\ null\ \{P\}$.]~\\
        Es decir, si el aserto $P$ es cierto antes de la ejecución de la sentencia nula (esta es, la que no cambia nada en el programa), $P$ seguirá siendo cierto tras la ejecución de la misma.
    \item [Axioma de asignación $\{P_e^x\}\ x=e\ \{P\}$.]~\\
        Es decir, la asignación de un determinado valor $e$ a una variable $x$ solo cambia en el programa el valor de dicha variable $x$.
\end{description}

\begin{ejemplo}
    Un ejemplo de uso del axioma de asignación es el siguiente:

    Tratamos de probar que el triple $\{V\}\ x=5\ \{x=5\}$ es cierto. Es decir, que desde cualquier estado del programa, si asignamos $5$ a $x$, acabaremos en cualquier estado del programa en el que $x$ valga $5$.
    \begin{proof}
        Sea $P$ la fórmula dada por $x=5$ y $e$ el valor numérico $5$, sabemos que el axioma de asignación es cierto, luego se cumplirá que:
        \begin{equation*}
            \{P_e^x\}\ x=e\ \{P\}
        \end{equation*}
        de donde:
        \begin{equation*}
            \{V\} \equiv \{5=5\}\ x=e\ \{x=5\}
        \end{equation*}
        
    \end{proof}
\end{ejemplo}

Seguidamente, para cada una de las sentencias que afectan al flujo de control en un programa secuencial, contamos con reglas de inferencia para poder formar triples correctos en las demostraciones; además de dos reglas básicas de consecuencia.

\begin{description}
    \item [Regla de la consecuencia (1).] 
        \begin{equation*}
            \dfrac{\{P\}S\{Q\}, \{Q\}\rightarrow\{R\}}{\{P\}S\{R\}}
        \end{equation*}
        Es decir, siempre podemos hacer más débil la poscondición de un triple, de forma que este siga siendo cierto.
    \item [Regla de la consecuencia (2).] 
        \begin{equation*}
            \dfrac{\{R\}\rightarrow\{P\},\{P\}S\{Q\}}{\{R\}S\{Q\}}
        \end{equation*}
        Es decir, siempre podemos hacer más fuerta la precondición de un triple, manteniendo su veracidad.
    \item [Regla de la composición.] 
        \begin{equation*}
            \dfrac{\{P\}S_1\{Q\}, \{Q\}S_2\{R\}}{\{P\}S_1;S_2\{R\}}
        \end{equation*}
        Es decir, podemos condensar dos triples en uno, siempre y cuando la poscondición de uno sea la precondición del otro.
    \item [Regla del \textit{if}.] 
        \begin{equation*}
            \dfrac{\{P\land B\}S_1\{Q\}, \{P\land \lnot B\}S_2\{Q\}}{\{P\}\ \texttt{if B then}\ S_1\ \texttt{else}\ S_2\ \{Q\}}
        \end{equation*}
        De esta forma, siempre que queramos probar que una tripleta de la forma
        \begin{equation*}
            \{P\}\ \texttt{if X then}\ S_1\ \texttt{else}\ S_2\ \{Q\}
        \end{equation*}
        es cierta, tendremos que probar que las tripletas
        \begin{equation*}
            \{P\land X\}S_1\{Q\}\qquad \{P\land \lnot X\}S_2\{Q\}
        \end{equation*}
        son ciertas.
    \item [Regla de la iteración.] 
        Suponiendo que una sentencia \verb|while| puede iterar un número arbitrario de veces (incluso 0), tenemos que:
        \begin{equation*}
            \dfrac{\{I\land B\}S\{I\}}{\{I\}\ \texttt{while B do}\ S\ \texttt{end do}\ \{I\land \lnot B\}}
        \end{equation*}
        Donde a la proposición $I$ la llameremos invariante.
\end{description}

\subsection{Verificación de sentencias concurrentes}
Sabemos ya hacer demostraciones para verificar la corrección de programas secuenciales. Será de nuestro interés ahora permitir la ejecución concurrente de dichos flujos de ejecución secuenciales, con el objetivo de probar la corrección de un programa concurrente.\\

Si entendemos la ejecución de un programa concurrente como un entrelazamiento de las instrucciones atómicas ejecutadas por los procesos del programa, entonces hemos de tener en cuenta para la demostración de corrección que no todas las secuencias de entrelazamiento resultan ser aceptables. Para poder programar correctamente, usamos sentencias de sincronización, tales como:
\begin{itemize}
    \item Secciones críticas en el código para evitar condiciones de carrera.
    \item Sincronización con una condición (hacer que un proceso espere hasta que se dé una determinada condición en el estado del programa).
\end{itemize}~\\

Dados varios triples de Hoare que representan secciones de programas secuenciales de los que hemos probado su corrección parcial, tratamos ahora de introducir estas secciones de programa en un programa concurrente.

Sin embargo, puede suceder que un proceso ejecute una instrucción atómica de su región de código que haga falsa la precondición o la poscondición de una sentencia que esté siendo simultáneamente ejecutada por otro proceso, invalidando su corrección.

\begin{ejemplo}
    Por ejemplo, tenemos los dos siguientes triples:
    \begin{gather*}
        \{x=0\}\ x=x+2\ \{x=2\} \\
        \{V\}\ x=0\ \{x=0\}
    \end{gather*}
    cuya corrección puede comprobarse fácilmente usando el axioma de asignación.\\

    Ahora, nos disponemos a ejecutar el siguiente código concurrente:
    \begin{minted}{pascal}
        x = 0;
        cobegin
          <x = x + 2;> || <x = 0;>
        coend
    \end{minted}
    donde podemos ver que la ejecución de una instrucción \textit{interfiere} con la poscondición de la otra instrucción:
    \begin{itemize}
        \item La poscondición $\{x=0\}$ de la instrucción de la derecha no se cumple tras la ejecución de \verb|<x = x + 2;>| (en caso de que esta se ejecute después).
        \item La poscondición $\{x=2\}$ de la instrucción de la izquierda no se cumple tras la ejecución de \verb|<x = 0;>| (en caso de que esta se ejecute después).
        \item Notemos que la ejecución de una instrucción no \textit{interfiere} con la precondición de la otra.
    \end{itemize}
    La ejecución de una instrucción hace falsa la poscondición de la otra, invalidando la demostración del programa concurrente.\\

    Sin embargo, podemos hacer un cambio en los triples iniciales para no tener este problema: usando la regla de la consecuencia (1), podemos relajar las poscondiciones de ambos triples:
    \begin{gather*}
        \{x=0\}\ x=x+2\ \{x=0 \lor x=2\} \\
        \{V\}\ x=0\ \{x=0 \lor x=2\}
    \end{gather*}
    haciendo que estos sigan siendo correctos. Si ahora tratamos de ejecutar de forma concurrente estas dos instrucciones, observamos que independientemente de la traza de ejecución, la ejecución de una instrucción no hace falsas las pre o poscondición de la otra.
\end{ejemplo}
Antes de proceder a la formalización del concepto de interferencia, hemos de comentar ciertos detalles:\\

\begin{itemize}
    \item Una acción atómica elemental o sección crítica realiza una transformación indivisible del estado del programa, de forma que cualquier estado intermedio que pudiera existir no sería visible para el resto de los procesos.

        En los programas concurrentes, puede suceder que las asignaciones no sean atomicas, por estar típicamente implementadas por varias instrucciones máquina:

        \begin{minted}{pascal}
            y = 0; z = 0;
            cobegin
              x = y + z; || y = 1; z = 2
            coend
        \end{minted}
        El programa superior puede dar como resultados esperados de \verb|x| 0, 1, 3 (como el lector habrá podido adivinar) y 2. Este último valor sería resultado de haber leído la instrucción de la izquierda \verb|y|, la ejecución de las dos instrucciones de la derecha, y finalmente añadir \verb|z| al valor leído, guardándolo en \verb|x|.

        Se trata de un ejemplo curioso, ya que $z+y=2$ no se correponde con ningún estado del programa. Esta situación puede resolverse con el uso de secciones críticas.
        
    \item Una expresión que no hace referencia a ninguna variable modificada por otro proceso será evaludad de forma atómica, ya que ninguno de los valores de la variable puede ser modfiicada mientras la expresión resulta evaludada.

        De esta forma, si consideramos el siguiente programa concurrente en el que los procesos se encuentran usando variables disjuntas:
        \begin{minted}{pascal}
            x = 0; y = 0;
            cobegin x = x + 1; || y = y + 1; coend
        \end{minted}
        La ejecución de una instrucción no tiene nada que ver con la ejecución de la otra.
\end{itemize}

\subsubsection{Interferencia}
Dado un triple $\{P\}S\{Q\}$, este puede contener varios asertos: $\{P\}$, $\{Q\}$ y cualquiera que sirve como pre y poscondición entre las acciones atómicas incluidas en la sección de código $S$.
\begin{ejemplo}
    De esta forma, el triple $\{V\}x=0;x=x+1\{x=1\}$ contiene los asertos $\{V\}$, $\{x=0\}$ y $\{x=1\}$.
\end{ejemplo}
\begin{definicion}[Interferencia]
    Dado un triple $\{P\}S\{Q\}$ y una sentencia $R$ con precondición $pre(R)$, decimos que $R$ no interfiere con $\{P\}S\{Q\}$ si para todo aserto $a$ del triple se cumple el triple
    \begin{equation*}
        NI(a, R) \equiv \{a \land pre(R)\} R \{a\}
    \end{equation*}
    Es decir, si el aserto $a$ es invariante para la sentencia $R$.
\end{definicion}
\begin{definicion}
    Dados $n$ triples $\{P_i\}S_i\{Q_i\}$, decimos que están libres de interferencia si $S_i$ no interfiere con $\{P_j\}S_j\{Q_j\}$, para cada par $(i,j)$ con $i\neq j$.
\end{definicion}
La falta de interferencia significa que la ejecución atómica de pasos de un proceso del programa nunca falsifica los asertos usados en las demostraciones de los otros procesos.

\begin{ejemplo}
    Recuperando el ejemplo del inicio de la sección, nos disponemos a demostrar que los triples $\{x=0\}\ x=x+2\ \{x=0 \lor x=2\}$, $\{V\}\ x=0\ \{x=0 \lor x=2\}$ están libres de interferencia:
    \begin{itemize}
        \item $x=x+2$ no interfiere con $\{V\}\ x=0\ \{x=0 \lor x=2\}$:
            \begin{enumerate}
                \item Para la precondición:
                    \begin{equation*}
                        \{x=0\} \equiv \{V \land x = 0\}\ x=x+2\ \{V\}
                    \end{equation*}
                    se cumple por la regla de la consecuencia (1), ya que se verifican $\{x=0\}\ x=x+2\ \{x=2\}$, $\{x=2\} \rightarrow \{V\}$.
                \item Para la poscondición:
                    \begin{equation*}
                        \{x=0\} \equiv \{(x=0 \lor x=2)\land x=0\}\ x=x+2\ \{x=0 \lor x=2\}
                    \end{equation*}
                    se cumple por la regla de la consecuencia (1), ya que se verifican $\{x=0\}\ x=x+2\ \{x=2\}$, $\{x=2\} \rightarrow\{x=0 \lor x=2\}$.
            \end{enumerate}
        \item $x=0$ no interfiere con $\{x=0\}\ x=x+2\ \{x=0 \lor x=2\}$:
            \begin{enumerate}
                \item Para la precondición:
                    \begin{equation*}
                        \{x=0\} \equiv \{x=0 \land V\}\ x=0\ \{x=0 \lor x=2\}
                    \end{equation*}
                    se cumple lor la regla de la consecuencia (1), ya que se verifican $\{x=0\}\ x=0\ \{x=0\}$ y $\{x=0\}\rightarrow \{x=0 \lor x=2\}$.
                \item Para la poscondición:
                    \begin{equation*}
                        \{x=0 \lor x = 2\} \equiv \{(x=0 \lor x=2)\land V\}\ x=0\ \{x=0 \lor x=2\}
                    \end{equation*}
                    se cumple por la regla de la consecuencia (1), ya que se verifican $\{x=0 \lor x=2\}\ x=0\ \{x=0\}$ y $\{x=0\}\rightarrow \{x=0 \lor x=2\}$.
            \end{enumerate}
    \end{itemize}
\end{ejemplo}

Finalmente, llegamos a una regla de inferencia que nos permitirá demostrar la corrección de un programa concurrente:

\begin{description}
    \item [Regla de la composición concurrente segura de procesos.] 
        \begin{equation*}
            \dfrac{\{P_i\}\ S_i\ \{Q_i\} \text{ son triples libres de interferencia } 1\leq i \leq n}{\{P_1 \land P_2 \land \ldots \land P_n\} \texttt{ cobegin } S_1\ ||\ S_2\ ||\ \ldots\ ||\ S_n \texttt{ coend } \{Q_1 \land Q_2 \land \ldots \land Q_n\}}
        \end{equation*}
\end{description}

\begin{ejemplo}
    Aplicando la regla de la composición concurrente al ejemplo que venimos manejando, como los triples $\{x=0\}\ x=x+2\ \{x=0 \lor x=2\}$, $\{V\}\ x=0\ \{x=0 \lor x=2\}$ están libres de interferencia, podemos aplicar la regla de la composición concurrente, llegando a que el triple

    \begin{gather*}
        \{x=0\} \\
        \texttt{cobegin } x=x+2;\ ||\ x=0; \texttt{ coend} \\
        \{x=0 \lor x=2\}
    \end{gather*}
    es cierto.
\end{ejemplo}

\subsection{Verificación usando invariantes globales}
La demostración de verificación de programas concurrntes usando la regla de la composición concurrentes es tediosa, ya que es necesario comprobar la veracidad de muchos triples antes de poder aplicarla. Es por tanto que buscamos otra forma predominante de probar los programas concurrentes.\\

Los invariantes globales (IG) de un programa pueden entenderse como expresiones definidas a partir de las variables globales de un programa. Estas suelen definirse como un predicado de la Lógica de Programas que captura la relación que existe entre las variables compartidas por los procesos de un programa concurrente.\\

Si cualquier aserto crítico $\{C\}$ (esto es, un aserto que aparece en un triple) de las demostraciones individuales de los programas concurrntes $\{P_i\}\ S_i\ \{Q_i\}$ se puede escribir como una conjunción del tipo $\{IG \land L\}$ donde $IG$ es un invariante global del programa y $\{L\}$ es un predicado en el que intervienen solo variables de un proceso o parámetros de una función, entonces las demostraciones de los procesos secuenciales estarán libres de interferencias.\\

Para que un predicado $\{I\}$ definido a partir de las variables compartidas entre los procesos pueda ser considerado un IG válido, se han de cumplir:
\begin{enumerate}
    \item Es cierto para los valores iniciales de las variables que aparecen en $\{I\}$.
    \item Se ha de poder demostrar la no interferencia de dicho aserto con cualquier sentencia elemental de $S_i$, es decir, se ha de poder probar el triple
        \begin{equation*}
            NI(I, R) \equiv \{I \land pre(R)\}\ R\ \{I\}
        \end{equation*}
        para toda $R$ de $S_i$.
\end{enumerate}
