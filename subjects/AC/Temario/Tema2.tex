\chapter{Programación Paralela}

En el capítulo anterior, nos dedicamos a introducir los conceptos de paralelismo dentro de una aplicación, tanto a nivel implícito como explícito; así como formas de llevarlo a cabo y de evaluación de las mejoras relacionadas con implementar paralelismo. A continuación, nos toca conocer a un menor nivel de abstracción cómo se programan todas estas estrategias relacionadas con el paralelismo que ya hemos desarrollado y que el lector debería poseer.

\section{Estructuras en programación paralela}
En esta sección, planteamos los aspectos particulares de las herramientas de programación paralela (aquellas que nos ayudan a desarrollar código paralelo, para poder crear aplicaciones paralelas). Haremos incapié en el trabajo extra que supone hacer una aplicación paralela frente a una secuencial, así como aprender formas de comunicación (o sincronización) que se ofertan al programador.

\subsection{Objetivos}
En esta sección, tratamos de:
\begin{itemize}
    \item Distinguir entre los diferentes tipos de herramientas de programación paralela, como compiladores paralelos, lenguajes paralelos, APIs de directivas y APIs de funciones.
    \item Distinguir entre los diferenties tipos de comunicaciones colectivas.
    \item Diferenciar el paradigma de programación de paso de mensajes con respecto al paradigma de variables compartidas.
    \item Diferenciar entre OpenMP y MPI, en cuanto a su estilo de programación y tipo de herramienta.
    \item Distinguir entre las esctructuras de tareas junto a procesos y hebras, master-slave, cliente-servidor, descomposición de dominio, flujo de datos (o segmentación) y divide y vencerás.
\end{itemize}

\subsection{Problemas que plantea la programación paralela}
La programación paralela requiere de algún ente (ya sea la herramienta que usemos o el propio programador) que realice el trabajo necesario para llevar a cabo el paralelismo, a diferencia de un código secuencial. Los mayores trabajos (y los más comunes) que nos encontramos a la hora de pasar de una aplicación secuencial a una paralela los desarrollaremos a continuación.

\begin{description}
    \item [Localicación o detección de paralelismo]~\\
        Para poder implementar paralelismo dentro de una aplicación, esto es, dividir las aplicaciones en unidades de cómputo independientes que recibirán el nombre de \emph{tareas}, es necesario primero localizar de dónde podemos extraer este paralelismo. Lo más cómodo y usual será, a partir del código secuencial que resuelve nuestra aplicación (o a partir de la definición de la aplicación), analizarlo para ver de dónde podemos extraer paralelismo (y en qué parte del código es donde debemos intentar introducir paralelismo). Los grafos pueden ser una gran herramienta en esta tarea, ya que nos permiten ver las tareas que pueden ejecutarse en paralelo (estos serán los nodos a misma altura, si la altura representa el tiempo de ejecución); así como las dependencias que hay entre ellas (los datos que una tarea requiere de otra para su ejecución). Finalmente, también nos permite ver cuál es el número máximo de tareas que se ejecutarán en paralelo. A este número máximo se le suele llamar \emph{grado de paralelismo} de una aplicación.
    \item [Asignación de carga de trabajo]~\\
        Tenemos que decidir qué tareas corresponderán a qué ente del sistema operativo (como procesos o threads), así como la asignación de flujos de instrucciones a los procesadores disponibles. Cabe destacar que no suele ser rentable usar más flujos de instrucciones que procesadores ni que los flujos cambien de procesador en tiempo de ejecución. Así, las asignaciones de flujos a procesadores puede hacerse estática o dinámicamente (en tiempo de ejecución); y explícita o implícita (lo hace la herramienta de forma automática). Notemos que la asignación dinámica requiere un costo extra, lo que introduce un retardo adicional. La asignación dinámica es la única posible cuando no puede conocerse el número de tareas a ejecutar. 
    \item [Comunicación o sincronización]~\\
        Muchas veces necesitaremos mecanismos de comunicación entre los distintos flujos de instrucciones, ya que todos estos están colaborando en la ejecución del programa (uno puede generar una variable que otro necesite). 
\end{description}

Un ejemplo de necesidad de todas estas tareas la vemos reflejada en el siguiente código:
    \begin{minted}[xleftmargin=1cm]{c++}
main(int argc, char** argv){
    double ancho, sum = 0;
    int intervalos, i;
    
    intervalos = atoi(argv[1]);
    ancho = 1.0/(double) intervalos;

    for(int i = 0; i < intervalos; i++){
        x = (i+0.5) * ancho;
        sum += 4.0/(1.0 + x * x);
    }

    sum *= ancho;
    // ...
}
    \end{minted}
Tenemos un código secuencial que se encarga de realizar una tarea determinada. Ahora, queremos hacer uso del paralelismo para disminuir el tiempo de ejecución de la tarea. Vemos cómo hacerlo en la siguiente figura, donde hemos hecho uso de la herramienta OpenMP.
    \begin{minted}[xleftmargin=1cm]{c++}
#include <omp.h>
#define NUM_THREADS 4

main(int argc, char** argv){
    double ancho, sum = 0;
    int intervalos, i;
    
    intervalos = atoi(argv[1]);
    ancho = 1.0/(double) intervalos;
    omp_set_num_threads(NUM_THREADS);

    #pragma omp parallel for reduction(+:sum) private(x)
    for(int i = 0; i < intervalos; i++){
        x = (i+0.5) * ancho;
        sum += 4.0/(1.0 + x * x);
    }

    sum *= ancho;
    // ...
}
    \end{minted}
Donde primero, hemos detectado qué parte podríamos mejorar del código. En este caso, repartir las iteraciones del bucle entre distintas hebras, ya que las iteraciones no están relacionadas entre sí. A continuación, hemos decidido que vamos a usar 4 hebras, y que vamos a repartir las \verb|intervalos| iteraciones de forma equitativa entre las 4 hebras. Finalmente, comunicamos las hebras entre sí gracias a dos detalles:
\begin{itemize}
    \item La cláusula \verb|reduction(+:sum)| nos permite sumar en \verb|sum| los distintos valores de la variable \verb|sum| de cada hebra (cabe destacar que lo podríamos haber hecho con un proceso menos automático y más personalizado, como usando directivas \verb|critical|, aunque en este caso la mejor elección es \verb|atomic|).
    \item La directiva \verb|for| tiene una barrera implícita al final que hace que todas las hebras se esperen entre sí (tarea de sincronización).
\end{itemize}

\subsubsection{Modos de programación}
A la hora de programar una aplicación paralela, podemos distinguir dos modos de programación:
\begin{description}
    \item [SPMD (Single-Programa Multiple Data)]~\\
        También denominado a veces paralelismo de datos, todos los códigos que se ejecutan en paralelo se obtienen compilando el mismo programa. Cada copia trabaja con un conjunto de datos distintos y se ejecuta en un procesador diferente.
    \item [MPMD (Multiple-Program Multiple Data)]~\\
        También llamado a veces paralelismo de tareas o funciones, los códigos que se ejecutan en paralelo se obtienen compilando programas independientes. En este caso, la aplicación a ejecutar (o el código secuencial inicial) se divide en unidades independientes. Cada unidad trabaja con un conjunto de datos distintos y se ejecutan en un procesador diferente.
\end{description}

SPMD es recomendable en sistemas masivamente paralelos. Es más fácil resolver la aplicación escribiendo un único programa. Usado en sistemas con memoria distribuida, evita la necesidad de tener que distribuir el código entre los nodos, sólo habría que distribuir datos. En la práctica, se aplica en mayor medida SPMD antes que MPMD.

En los programas paralelos se pueden utilizar combinaciones de MPMD y SPMD. La programación dueño-esclavo se puede considerar una variante del modo MPMD (se verá a lo largo de esta sección). Si todos los esclavos tienen el mismo código, sería una mezcla de MPMD y SPMD. Los programas que conforman una solución dueño-esclavo con MPMD se pueden juntar en un único programa SPMD con el uso de estructuras condicionales.

\subsection{Herramientas para obtener código paralelo}
Las herramientas de programación paralela debería permitirnos de forma explícita o implícita:
\begin{enumerate}
    \item Localizar paralelismo: descomponer la aplicación en tareas independientes.
    \item Asignar las tareas: repartir la carga de trabajo entre procesos.
    \item Crear (enrolar) y terminar (desenrolar) procesos.
    \item Comunicar y sincronizar procesos.
    \item Asignar procesos a procesadores.
\end{enumerate}

Donde este último es el SO o el hardware quien realiza esta tarea (usualmente).\\

Cuanto mayor sea la abstracción que desarrolle la herramienta paralela, menor serán las labores que debe desarrollar el programador de aplicaciones paralelas. La labor más difícil para la herramienta es la primera, la detección del paralelismo. Podemos realizar una clasificación de las herramientas de programación paralela en función de la abstracción en la que sitúan al programador. Las enumeramos desde el mayor al menor nivel de abstracción:
\begin{description}
    \item [Compiladores paralelos]~\\
        Un compilador paralelo pretende ser aquella automatización capaz de extraer paralelismo a nivel de bucle (paralelismo de datos) y de función (paralelismo de tareas) a partir de un código secuencial. Para ello, realizan análisis de dependencias entre bloques de código. No generan código eficiente para cualquier programa y todavía se investiga en este campo.
    \item [Lenguajes paralelos y APIs de funciones y directivas]~\\
        Generalmente, los lengaujes paralelos y directivas sitúan al programador en un nivel de abstracción superior a sólo bibliotecas de funciones. Encontramos lenguajes como Occam, Ada o Java, los cuales tienen construcciones particulares y bibliotecas de funciones que requieren un compilador exclusivo. Por otra parte, las APIs mencionadas (formadas tanto por directivas para el lenguaje como por bibliotecas de funciones) nos permiten trabajar en cualquier lenguaje para el que fueron diseñadas, como \verb|C++| o \verb|Fortran|, en el caso de OpenMP. En este nivel, el programador es el encargado de detectar el paralelísmo implícito en la aplicación. Sin embargo, el programador no hace el reparto (la asignación directa) de este paralelismo, así de eximir al programador las tareas de creación y terminación de flujos o de detalles para comunicación. Como ventaja, es más sencillo desarrollar aplicaciones paralelas, obteniendo códigos más cortos.
    \item [APIs de funciones]~\\
        Como por ejemplo Pthreads o MPI, las cuales sólo consisten en una biblioteca de funciones que se añaden a un compilador de un lenguaje sencuencial. El cuerpo de procesos y hebras es escrito en lenguaje secuencial y es el programador quien se encarga de distribuir las tareas entre los procesos, crear o gestionar procesos, e impmlementar la comunicación y sincronización usando funciones de la biblioteca. Como ventajas a destacar:
        \begin{itemize}
            \item Los programadores están familiarizados con los lenguajes secuenciales.
            \item Las bibliotecas están disponibles para todos los sistemas paralelos.
            \item Las bibliotecas están más cercanas al hardware y permiten dar al programador un control a más bajo nivel.
            \item Se pueden utilizar a la vez bibliotecas para programar con hebras y con procesos.
        \end{itemize}
    \item [Lenguajes paralelos para arquitecturas de propósito específico]~\\
        Como por ejemplo CUDA (de NVIDIA). Consisten en construcciones del lenguaje y bibliotecas de funciones que requieren un compilador exclusivo. El programador debe participar en todas las labores salvo quizás en la asignación de instrucciones a unidades de procesamiento. Debe tener un gran conocimiento de las arquitecturas para poder escribr el código paralelo.
\end{description}

Comentamos ahora que, mientras OpenMP es el estándar industrial en programación paralela (gracias al alto nivel de abstracción que provee al programador), MPI es el estándar industrial para la programación de multicomputadores. OpenMP realiza de forma automática el reparto de trabajo, mientras que en MPI es el programador quien debe llevarlo a cabo (esto es lógico, debido al estar orientado a multicomputadores, donde el reparto de la carga de trabajo es más difícil de realizar, como ya vimos en el capítulo anterior).

\subsection{Comunicaciones y sincronizaciones}
Las herramientas para la programación paralela también pueden ofrecer al programador, además de la comunicación entre dos procesos, comunicaciones en las que intervienen múltiples procesos. Estas comunicaciones se implementan para comunicar a todos los procesos que forman parte del grupo que colabora en la ejecución de un código. En muchos casos estas comunicaciones no tienen la finalidad de transmitir datos, sino de sincronizar procesos. Es común ver en varias aplicaciones las funcionalidades de:
\begin{itemize}
    \item Reordenar datos entre procesos.
    \item Difusión de datos.
    \item Reducir un conjunto de datos a uno solo.
    \item Múltiples reducciones en paralelo con el mismo conjunto de datos.
    \item Sincronizar múltiples procesos en un punto.
\end{itemize}

Por tanto, se intenta que las herramientas de programación paralela nos permitan implementar dichas funcionalidades mediante comunicaciones entre procesos (flujos de instrucciones). A lo largo de esta sección, cada vez que aparezca ``mensaje'', estaremos haciendo referencia a un dato o estructura de datos. A continuación, enumeramos los distintos tipos de comunicaciones entre procesos que podemos encontrarnos:
\begin{description} % // TODO: terminar de comentar bien
    \item [Comunicación múltiple uno-a-uno.] Hay componentes del grupo que envían un único mensaje y componentes que reciben un único mensaje. Si todos los componentes del grupo envían y reciben, diremos que se trata de una \emph{permutación}. Nos podemos encontrar \emph{rotaciones} (el proceso $P_i$ envía al proceso $P_{i+1}$ y el $P_n$ al $P_0$), \emph{intercambios}, \emph{barajes}, \emph{desplazamientos}, \ldots 
        % // TODO: foto

    \item [Comunicación uno-a-todos.] Un proceso envía y todos los procesos del grupo reciben el mensaje. Destacamos aquí:
        \begin{description}
            \item [Difusión (broadcast).] Todos los procesos reciben el mismo mensaje.
            \item [Dispersión (scatter).] Cada proceso recibe un mensaje diferente.
        \end{description}
        % // TODO: foto

    \item [Comunicación todos-a-uno.] Todos los procesos en el grupo envían un mensaje a un único proceso. Destacamos:
        \begin{description}
            \item [Reducción.] Los mensajes enviados por los procesos se combinan en un sólo mensaje mediante algún operador (como por ejemplo, el proceso recibe una suma de distintas variables de cada proceso).
            \item [Acumulación (gather).] Los mensajes se reciben de forma concatenada en el receptor.
        \end{description}
        % // TODO: foto

    \item [Comunicación todos-a-todos.] Todos los procesos del grupo ejecutan una comunicación uno-a-todos. Puede ser:
        \begin{description}
            \item [Todos difunden (all-broadcast).] Todos los procesos realizan una difusión.
            \item [Todos dispersan (all-scatter).] Todos los procesos realizan una dispersión.
        \end{description}
        % // TODO: foto

    \item [Comunicaciones colectivas compuestas.] Hay servicios que resultan de la combinación de algunos anteriores, como:
        \begin{description}
            \item [Todos combinan, o reducción y extensión.] El resultado de aplicar una reducción se obtiene en todos los procesos.
            \item [Barrera.] Es un punto de sincronización que todos los procesos de un grupo deben alcanzar para que cualquier proceso del grupo pueda continuar con su ejecución.
            \item [Recorrido (scan).] Todos los procesos envían un mensaje, recibiendo cada uno de ellos el resultado de reducir un conjunto de estos mensajes.
                \begin{itemize}
                    \item Recorrido prefijo: El proceso $P_i$ recibe el resultado de reducir los mensajes de $P_0, P_1, \ldots, P_i$.
                    \item Recorrido sufijo: El proceso $P_i$ recibe el resultado de reducir los mensajes de $P_i, P_{i+1}, \ldots, P_n$.
                \end{itemize}
        \end{description}
\end{description}

Comunicaciones como ``dispersión'' o ``todos dispersan'' son usadas para reparto de datos. ``Acumulación'' es usada para fusionar datos intermedios. Los ``desplazamientos'' son tramos intermedios para realizar nuevos repartos de datos. 

\subsubsection{Implementación en OpenMP} 
Varias comunicaciones de las anteriormente descritas podemos llevarlas a cabo usando la herramienta OpenMP:
\begin{description}
    \item [Uno-a-todos] Podemos llevar a cabo la difusión (lo haremos en la Sesión II de prácticas) con:
        \begin{itemize}
            \item La cláusula \verb|firstprivate| desde el thread 0.
            \item La directiva \verb|single| con la cláusula \verb|copyprivate|. 
            \item La directiva \verb|threadprivate| y uso de la cláusula \verb|copyin| en directiva \verb|parallel| desde thread 0.
        \end{itemize}
    \item [Todos-a-uno] Podemos implementar reducción (lo haremos en la Sesión II de prácticas) con la cláusula \verb|reduction|. 
    \item [Servicios compuestos] En la Sesión I de prácticas hemos usado la directiva \verb|barrier|, que implementa una barrera.
\end{description}
Observemos el trabajo a alto nivel que nos facilitan las diversas directivas propias de una API de directivas y funciones.

\subsubsection{Implementación en MPI} 
Por otra parte, podemos implementar las comunicaciones de una forma más cercana al hardware (a más bajo nivel) con una API de funciones tal y como lo es MPI:
\begin{description}
    \item [Uno-a-uno] De forma asíncrona, con las funciones \verb|MPI_Send()| y \verb|MPI_Receive()|.
    \item [Uno-a-todos] Podemos implementar:
        \begin{itemize}
            \item Difusión, con la función \verb|MPI_Bcast()|.
            \item Dispersión, con la función \verb|MPI_Scatter()|.
        \end{itemize} 
    \item [Todos-a-uno] Como:
        \begin{itemize}
            \item Reducción, con la función \verb|MPI_Reduce()|.
            \item Acumulación, con la función \verb|MPI_Gather()|.
        \end{itemize}
    \item [Todos-a-todos] Podemos implementar ``todos acumulan'' con la función \verb|MPI_Allgather()|.
    \item [Servicios compuestos] Como:
        \begin{itemize}
            \item Todos combinan, con la función \verb|MPI_Allreduce()|.
            \item Barreras, con la función \verb|MPI_Barrier()|.
            \item Scan, con la función \verb|MPI_Scan()|.
        \end{itemize}
\end{description}

\subsection{Paradigmas de programación paralela}
Cada tipo de arquitectura paralela (según la taxonomía de Flynn anteriormente estudiada) presenta distintas implementaciones en cuanto a su diseño se refiere. Es por esto que para cada tipo de arquitectura buscaremos un tipo de código que mejor se adapte a su diseño. Destacamos tres principales paradigmas en cuanto a programación paralela, cada uno asociado a un tipo de arquitectura:
\begin{itemize}
    \item Paso de mensajes (para multicomputadores).
    \item Variables compartidas (para multiprocesadores). 
    \item Paralelismo de datos (para computadores SIMD).
\end{itemize}
Con \emph{paso de mensajes} se supone que cada procesador del sistema tiene su propio espacio de direcciones. Los mensajes llevan datos de un espacio de direcciones a otro y pueden aprovecharse para sincronizar procesos. Los datos transferidos estarán duplicados en el sistema de memoria. Con \emph{variables compartidas}, se supone que los procesasdores comparten espacio de direcciones. Se realiza la transferencia de forma implícita usando instrucciones de lectura y escritura en memoria. Con \emph{paralelismo de datos} la misma instrucción se ejecuta en paralelo en múltiples cores de forma que cada uno actúa sobre un conjunto de datos distinto. Este paradigma es apropiado para aquellas arquitecturas que sólo soportan paralelismo a nivel de bucle. La sincronización se encuentra implícita. 

Asimismo, también podemos encontrar herramientas que permiten programar multiprocesadores mediante paso de mensajes, un software que se apoya en el hardware para variables compartidas. De igual forma, hay herramientas que permiten variables compartidas en multicomputadores. Hay lenguajes que soportan paralelismo de datos, tanto en multiprocesadores como en multicomputadores.

\begin{description}
    \item [Paso de mensajes.]~\\
        Se dispone de diversas herramientas software, como lenguajes de programación (Ada, Occam, \ldots) o bibliotecas de funciones (como MPI). Los fabricantes de supercomputadores suelen proporcionar este tipo de software, capaz de extraer un gran rendimiento de sus máquinas. Las funciones básicas de comunicación suelen ser \verb|send()| y \verb|receive()|. Generalmente, en la función \verb|send| se especifica el proceso destino y el mensaje a enviar, mientras que en \verb|receive| se especifica la fuente y la estructura de datos en la que se almacenará el mensaje. Podemos encontrar implementaciones \emph{síncronas} (el proceso que ejecuta un \verb|send| se bloquea hasta que el destinatario hace uso de \verb|receive| y viceversa) o \emph{asíncronas} (\verb|send| no tiene por qué bloquear el proceso). Para esta última, es necesario usar un buffer en su implementación. 
    \item [Variables compartidas.]~\\
        Encontramos software como lenguajes de programación (Ada 95 o Java), bibliotecas de funciones y APIs de directivas y funciones (como OpenMP). Los propios fabricantes ofrecen compiladores secuenciales con estas extensiones para programar sus máquinas. Para la comunicación se suelen desarrollar instrucciones de \emph{lectura} y \emph{escritura} en memoria, las cuales serán usadas por distintos procesos (en el SO, las hebras comparten memoria entre sí, por lo que no es necesario que usen estas instrucciones). El software ofrece mecanismos para implementar sincronización (para que un proceso no lea antes de que el otro escriba), como cerrojos, semáforos, variables condicionales, \ldots 
        OpenMP dispone además de directivas para llevar a cabo paralelismo de datos (directiva \verb|for|), de tareas (directiva \verb|sections|), y muchas más funcionalidades. 
    \item [Paralelismo de datos.]~\\
        En este paradigma se aprovecha el paralelismo de datos inherente a aplicaciones en la que los datos se organizan en estructuras como vectores o matrices. El programador escribe un programa con construcciones que permiten aprovechar paralelismo de datos (como paralelización de bucles, instrucciones vectoriales, \ldots), así como construcciones para distribuir los datos (la carga de trabajo) entre los núcleos de procesamiento. El programador no lleva a cabo las sincronizaciones (se encuentran implícitas). Ejemplos software son Fortran 95, HPF (High Performance Fortran), NVIDIA CUDA, \ldots
\end{description}

\subsection{Estructuras típicas de códigos paralelos}
Analizando las estructuras (los grafos) de las tareas y de los procesos (junto con las conmunicaciones y sincronizaciones entre estos) que componen distintos programas paralelos, se puede encontrar que hay ciertos patrones que se repiten en distintos programas y dentro de un programa. Entre estras estructuras encontramos:
\begin{itemize}
    \item Dueño-esclavo (\emph{master-slave}), o ``granja de tareas'' (\emph{task-farming}).
    \item Paralelismo de datos, descomposición de datos, o descomposición de dominio. 
    \item Divide y vencerás (\emph{divide and conquer}), o descomposición recursiva. 
    \item Estructura segmentada (\emph{pipeline}), o flujo de datos.
\end{itemize}
Hay estructuras que quizás no se puedan clasificar en un sólo tipo de los anteriores. Por otra parte, nos podemos encontrar dentro de un mismo programa paralelo varias de estas estructuras, en distintos niveles.

\subsubsection{Dueño-esclavo} 
En este caso, contamos con un proceso denominado dueño (o master) y con varios procesos denominados esclavos (o slaves). El dueño se encarga de distribuir las tareas de un conjunto entre el grupo de esclavos, y de ir recolectando los resultados parciales que van calculando los esvlavos, con los que el dueño obtiene el resultado final. Usualmente, no hay comunicación entre los esclavos.
Puede implementarse con un único programa si el código de los esclavos no es muy complicado (SPMD), con dos programas, si el código de los esclavos es igual (modo mixto MPMD-SPMD), o con múltiples programas (MPMD).
La repartición de carga puede hacerse de forma estática o dinámica.
Un ejemplo de esto es un programa que tiene que calcular los primos hasta un número $n$, y que hace uso de $r$ esclavos para que cada uno calcule los primos que se encuentran en un vector de tamaño $ \nicefrac{n}{r} $. Combinando el resultado de cada esclavo, obtendremos todos los primos.

\subsubsection{Cliente-Servidor} 
También llamado \emph{client-server}, se trata de una estructura similar a dueño-esclavo, pero con los roles invertidos: contamos con múltiples procesos denominados clientes y con un ente (puede ser un proceso, o tener a su vez una estructura paralela distinta en su interior) denominado servidor. En esta caso, los clientes son procesos que en un momento determinado solicitan cierta información al servidor. Este, se encarga de procesar la petición y de dar al cliente correspondiente la respuesta esperada. 
Notemos que el servidor puede estar formado a su vez de otras estructuras, como de un sistema dueño-esclavo. Por tanto, podemos tener por un lado a los clientes, que necesitan del servidor información. Este es el encargado de gestionar los mensajes y de repartir las tareas entre sus esclavos, quien realizan el cómputo, devolviendo el trabajo al maestro y este a los clientes. 

\subsubsection{Descomposición de datos} 
Alternativa muy utilizada para obtener tareas paralelas en programas con grandes estructuras de datos. La estructura de datos de entrada (o la de salida, o ambas) es dividida en varias partes. A partir de esta división se derivan las tareas paralelas. Estas generalmente realizan operaciones similares. Los algoritmos con imágenes, por ejemplo, admiten una descomposición de datos.
En este esquema, cada proceso puede englobar una o varias tareas. Los diferentes procesos ejecutan normalmente el mismo código (SPMD), que se ejecuta sobre distintos conjuntos de datos. Puede haber comunicaciones entre los distintos procesos.
Un ejemplo de descomposición de datos es el siguiente código, donde se invierten los colores de una imagen de tamaño $N\times N$:
    \begin{minted}[xleftmargin=1cm]{c++}
for(int i = 0; i < N; i++)
    #pragma omp parallel for private(i)
    for(int j = 0; j < N; j++){
        imagen[i][j] = 255 - imagen[i][j];
    }
    \end{minted}

\subsubsection{Estructura segmentada o flujo de datos} 
Esta estructura aparece en problemas en los que se aplica a un flujo de datos en secuencia distintas funciones (paralelismo de tareas). La estructura de los procesos y de las tareas es la de un cauce segmentado: cada proceso ejecuta por tanto distinto código. Es un caso típico de un programa MPMD puro. Para que resulte apropiada la estructura segmentada, se ebe aplicar el proceso a una secuencia de datos de entrada.
Por ejemplo, podemos tener un decodificador de imágenes JPEG, que aplica a una secuencia de $N\times N$ píxeles (la imagen de entrada) las siguientes funciones: decodificación de entropía, cuantificación inversa, transformada del coseno inversa y conversión RGB. Este programa podría implementarse con cuatro procesos en una estructura segmentada.
En este tipo de estructura, la comunicación entre procesos es necesaria, y suele ser unidireccional.

\subsubsection{Divide y vencerás} 
Se utiliza cuando un problema puede dividirse en dos o más subproblemas de menor tamaño de forma que cada uno pueda resolverse independiéntemente y combinar al final las distintas soluciones. Si los subproblemas son a su vez instancias más pequeñas del problema original, entonces se podrán volver a subdividir de forma recursiva. Las tareas presentan, por tanto, una estructura de árbol, de forma que no habrá interacciones (comunicaciones o sincronizaciones) entre las tareas que cuelgan del mismo padre.
Un ejemplo de uso de esta estructura podemos verlo cuando nos disponemos a sumar todas las componentes de un vector: podemos dividir el vector en 2, obteniendo dos subproblemas de tamaño $ \nicefrac{n}{2} $, siendo $n$ el número de componentes del vector. Podemos seguir recursivamente dividiendo el vector hasta obtener vectores de tamaño relativamente pequeños, donde haríamos su suma iterativamente. Obtenemos así al final la suma de todas las componentes del vector.

\newpage
\section{Proceso de paralelización}
Cuando queremos obtener una versión paralela de una aplicación con una biblioteca de funciones con directivas o con un lenguaje de programación, pueden seguirse los siguientes pasos:
\begin{itemize}
    \item Descomposición de la aplicación en tareas independientes.
    \item Asignación de las tareas a procesos o hebras. 
    \item Redactar el código paralelo. 
    \item Evaluación de los tres pasos anteriores.
\end{itemize}
Estudiaremos a lo largo de esta sección todos estos pasos a fondo, junto con un ejemplo práctico.

\subsection{Objetivos}
Esta sección está dedicada a:
\begin{itemize}
    \item Programar en paralelo una aplicación sencilla.
    \item Distinguir entre asignación estática y dinámica, destacando sus ventajas e inconvenientes.
\end{itemize}

\subsection{Descomposición en tareas} 
En esta etapa, tenemos que buscar unidades de trabajo en la aplicación que puedan ejecutarse en paralelo (es decir, que sean independientes). Estas unidades, junto con los datos que usan, forman las \emph{tareas}. Es conveniente en este paso representar las tareas y las relaciones entre ellas mediante un grafo, para observar las dependencias entre estas y el nivel de paralelización de la aplicación. Podemos situarnos en dos niveles de abstracción:
\begin{description}
    \item [Nivel de función.] Analizando las dependencias entre las funciones del código podemos encontrar aquellas que son independientes (o aquellas que pueden hacerse independientes), que serán las que se ejecutarán en paralelo. Estamos extrayendo paralelismo de tareas en nuestra aplicación.
    \item [Nivel de bucle.] Analizando las iteraciones de los bucles dentro de una función, podemos encontrar si son (o se pueden hacer) independientes. Podemos así detectar paralelismo de datos. Además, si una función consta de varios bucles, puede verse la relación entre estos, para así ver si pueden ejecutarse en paralelo.
\end{description}

\subsubsection{Ejemplo} 
Para paralelizar el cálculo de $\pi$, puede partirse de una versión secuencial disponible o de un planteamiento para el problema que se preste a paralelización. Procedemos pues a realizar este según el segundo método:

Podemos definir el cálculo de $\pi$ como un problema de integración, susceptible de ser paralelizado. Podemos calcular la integral definida en el intervalo $[0,1]$ de la derivada de la arcotangente de $x$:
\begin{equation*}
    \left.\begin{array}{rcl}
        \text{arctg}'(x) & = & \dfrac{1}{1+x^2} \\
                  & & \\
        \text{arctg}(1) & = & \nicefrac{\pi}{4} \\
        \text{arctg}(0) & = & 0 
    \end{array}\right\} \Longrightarrow \int_{0}^{1} \dfrac{dx}{1+x^2} = [\text{arctg}(x)]_0^1 = \dfrac{\pi}{4} - 0
\end{equation*}
Y así multiplicar por 4 para obtener $\pi$.

Esta integral puede obtenerse mediante métodos de integración numérica (rectángulo izquierdo, rectángulo derecho, punto medio, trapcio, fórmula de Simpson, \ldots). El área de la derivada del arcotangente en $[0,1]$ en subintervalos, calculando el área de la función en cada subintervalo y sumando. Cuantos más subintervalos tengamos, más exacta será la aproximción.
Notemos que el cálculo del área de los diferentes intervalos es independientes, por lo que podemos repartir estos cálculos en un conjunto de procesadores. Si se divide el intervalo en 100 subintervalos y tenemos 10 procesadores, podemos asignar a cada procesador el cálculo de 10 subintervalos. Sobre código, tenemos lo siguiente:
    \begin{minted}[xleftmargin=1cm]{c++}
main(int argc, char** argv){
    double ancho, sum = 0;
    int intervalos, i;

    intervalos = atoi(argv[1]);
    ancho = 1.0 / (double) intervalos;

    for(int i = 0; i < intervalos; i++){
        x = (i + 0.5) * ancho;
        sum += 4.0 / (1.0 + x*x);
    }

    sum *= ancho;
}
    \end{minted}
Para su paralelización, tratamos de determinar si las iteraciones del bucle son independientes. Como podemos ver, entre las iteraciones existe dependencias, ya que se escribe y se lee en la misma variable \verb|sum|. Sin embargo, estas dependencias pueden eliminarse fácilmente: basta suponer que cada iteración distinta escribe en una variable que depende de \verb|i|. La suma de todas estas variables daría la aproximación de $\pi$. 

% \subsection{Asignación de tareas} 
Nota: Estos apuntes no están terminados.

\newpage
\section{Evaluación de prestaciones}
\subsection{Objetivos}
Siguiendo con la evaluación en prestaciones de los computadores con aplicaciones paralelas, en esta sección aprenderemos a:
\begin{itemize}
    \item Obtener ganancia y escalabilidad en el contexto de procesamiento paralelo.
    \item Aplicar la Ley de Amdahl en el contexto de procesamiento paralelo.
    \item Comparar la Ley de Amdahl y la ganancia escalable.
\end{itemize}
