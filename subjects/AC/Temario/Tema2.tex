\chapter{Programación Paralela}

\section{Estructuras en programación paralela}
\subsection{Objetivos}
En esta sección, tratamos de:
\begin{itemize}
    \item Distinguir entre los diferentes tipos de herramientas de programación paralela, como compiladores paralelos, lenguajes paralelos, APIs de directivas y APIs de funciones.
    \item Distinguir entre los diferenties tipos de comunicaciones colectivas.
    \item Diferenciar el paradigma de programación de paso de mensajes con respecto al paradigma de variables compartidas.
    \item Diferenciar entre OpenMP y MPI, en cuanto a su estilo de programación y tipo de herramiento.
    \item Distinguir entre las esctructuras de tareas junto a procesos y hebras, master-slave, cliente-servidor, descomposición de dominio, flujo de datos (o segmentación) y divide y vencerás.
\end{itemize}

\subsection{Problemas que plantea la programación paralela}
% // TODO: Todo esto son apuntes, arreglar
El código secuencial nos permite analizar la aplicación: viendo qué porcentaje (f de amdahl) de tiempo de ejecución corresponde a cada parte.
Comentar SPMD, MPMD, para explotar MIMD.


\subsection{Herramientas para obtener código paralelo}
Poner lo que nos permite las herramientas de código paralelo. Lo más complicado es detectar paralelismo. OpenMP no nos ayuda en nada. Los compiladores sí.

Si OpenMP es el estandar industrial en programacion paralela, MPI es el estandar instrustrial para la programacion de multicomputadores. OpenMP facilita la tarea de reparto de trabajo. MPI se basa en usar funciones, API de funciones.
CUDA: herramienta relacionada con NVIDIA.

dynamic = repartir en tiempo de ejecución (se verá próximamente)
Es mejor físicos a lógicos porque los lógicos compiten por recursos, como unidad de cálculo

En la página 13 tenemos una condición de carrera en sum: tenemos que hacerla privada
Además, tenemos que sumar todas las sum privadas de todos los hilos (esto se haría mediante un critical, aunque es mejor atomic).
Esto podemos hacerlo automáticamente con reduction(+:sum)

Comunicaciones colectivas
dispersión y todos dispersan usado para reparto
Acumulación usada para fusionar datos intermedios
Desplazamientos: tramos intermedios para nuevos repartos de datos, para seguir procesando en paralelo.
Compuestos son compmosiciones de las anteriores.

\subsection{Paradigmas de programación paralela}
Se pueden agrupar en:
- paso de mensajes
- Variables compartidas
- Paralelismo de datos

de forma que cada uno es una abstracción software de una arquitectura paralela.

\subsection{Estructuras típicas de códigos paralelos}
\section{Proceso de paralelización}
\subsection{Objetivos}
Esta sección está dedicada a:
\begin{itemize}
    \item Programar en paralelo una aplicación sencilla.
    \item Distinguir entre asignación estática y dinámica, destacando sus ventajas e inconvenientes.
\end{itemize}

\section{Evaluación de prestaciones}
\subsection{Objetivos}
Siguiendo con la evaluación en prestaciones de los computadores con aplicaciones paralelas, en esta sección aprenderemos a:
\begin{itemize}
    \item Obtener ganancia y escalabilidad en el contexto de procesamiento paralelo.
    \item Aplicar la Ley de Amdahl en el contexto de procesamiento paralelo.
    \item Comparar la Ley de Amdahl y la ganancia escalable.
\end{itemize}
