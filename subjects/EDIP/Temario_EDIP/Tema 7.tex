\chapter{Modelos de distribuciones discretas}
\section{Distribución degenerada}

\begin{definicion}
    Sea $X$ una variable aleatoria. Decimos que es degenerada si $X=c$, es decir, toma tan solo un valor constante.
\end{definicion}

Su función masa de probabilidad es:
\begin{equation*}
    f(x) = P[X=x] = \left\{\begin{array}{cc}
        1 & x=c \\
        0 & x\neq c
    \end{array}\right.
\end{equation*}

Su función de distribución es:
\begin{equation*}
    F_X(x) = P[X\leq x] \left\{\begin{array}{cc}
        0 & x<c \\
        1 & c\leq x
    \end{array}\right.
\end{equation*}

Sus momentos no centrados son:
\begin{equation*}
    m_k = E[X^k] = c^kP[X=c] = c^k \qquad \forall k\in \mathbb{N}
\end{equation*}

Sus momentos centrados son:
\begin{equation*}
    \mu_k = E[(X-E[X])^k]
    = E[(X-c)^k] = (c-c)^kP[X=c] = 0    
    \qquad \forall k\in \mathbb{N}
\end{equation*}

Su función generatriz de momentos es:
\begin{equation*}
    M_X(t) = E[e^{tX}] = e^{tc}P[X=c] = e^{tc}    
    \qquad \forall t\in \mathbb{R}
\end{equation*}


Como se vio en la Proposición \ref{prop:6.13}, tenemos que $\Var[X]=0$ caracteriza a las variables degeneradas.

\section{Distribución uniforme discreta}

\begin{definicion}
    Sea $X$ una variable aleatoria. Se dice que la variable aleatoria $X$ se distribuye uniformemente alrededor de los puntos $x_1,\dots, x_n$ si dichos valores son equiprobables.

    Se notará como sigue:
    \begin{equation*}
        X \leadsto U(x_1,\dots, x_n)
    \end{equation*}
\end{definicion}

\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\leadsto U(x_1,\dots, x_n)$. Entonces, su función masa de probabilidad es:
    \begin{equation*}
        f(x) = P[X=x] = \left\{\begin{array}{cc}
            \nicefrac{1}{n} & x=x_i, i=1,\dots, n \\
            0 & \text{en caso contrario}
        \end{array}\right.
    \end{equation*}
\end{prop}
\begin{proof}
    Como son equiprobables, tenemos que:
    \begin{equation*}
        P[X=x_i] = k\qquad \forall i=1,\dots, n
    \end{equation*}

    Para determinar el valor de $k$, tenemos que:
    \begin{equation*}
        1 = \sum_{i=1}^n P[X=x_i] = \sum_{i=1}^n k = nk \Longrightarrow k = \frac{1}{n}
    \end{equation*}
\end{proof}

Su función de distribución es:
\begin{equation*}
    F_X(x) = P[X\leq x] \left\{\begin{array}{cl}
        0 & x<x_1 \\
        \nicefrac{k}{n} & x\in \left[x_k, x_{k+1}\right[ \qquad \forall k=1,\dots, n-1\\
        1 & x_n\leq x
    \end{array}\right.
\end{equation*}

Sus momentos no centrados son:
\begin{equation*}
    m_k = E[X^k]
    = \sum_{x_i\in Re_X}x_i^kf(x_i)
    = \frac{1}{n} \sum_{x_i\in Re_X}x_i^k
    \qquad \forall k\in \mathbb{N}
\end{equation*}

En particular, $\displaystyle E[X]=m_1 = \frac{1}{n} \sum_{x_i\in Re_X}x_i = \bar{x}$.

Sus momentos centrados son:
\begin{equation*}
    \mu_k = E[(X-E[X])^k]
    = E[(X-\bar{x})^k]
    = \frac{1}{n} \sum_{x_i\in Re_X}(x_i-\bar{x})^k   
    \qquad \forall k\in \mathbb{N}
\end{equation*}

En particular, $\displaystyle \Var[X]=\mu_2 = \frac{1}{n} \sum_{x_i\in Re_X}(x_i-\overline{x})^2 = \sigma_{x}^2$.

Su función generatriz de momentos es:
\begin{equation*}
    M_X(t) = E[e^{tX}] = \frac{1}{n} \sum_{x_i\in Re_X}e^{tx_i}     
    \qquad \forall t\in \mathbb{R}
\end{equation*}

Cuando $Re_X = \{1, 2, \ldots, n\}$, veamos algunos casos particulares de series\footnote{Se demuestran fácilmente mediante inducción, pero se deja como ejercicio por no ser materia de la asignatura de EDIP, sino de Cálculo I.}:
\begin{equation*}
    \begin{split}
        E[X] &= \dfrac{1}{n}\sum_{i=1}^n i = \dfrac{1}{n}\dfrac{n(n+1)}{2} = \dfrac{n+1}{2}\\
        E[X^2] &= \dfrac{1}{n}\sum_{i=1}^n i^2 = \dfrac{1}{n}\dfrac{n(n+1)(2n+1)}{6} = \dfrac{(n+1)(2n+1)}{6}\\
        Var(X) &= \dfrac{(n+1)(2n+1)}{6} - \dfrac{(n+1)^2}{4} = \dfrac{n^2-1}{12}
    \end{split}
\end{equation*}



\section{Distribución de Bernouilli}

\begin{definicion}[Experimentos de Bernouilli]
    Un experimento de Bernouilli es un experimento aleatorio que da lugar a dos posibles resultados mutuamente excluyentes y exhaustivos. Estos son denominados como éxito $(E)$ y fracaso $(F=\bar{E})$.
    \begin{equation*}
        \Omega = \{E,F\}
    \end{equation*}
\end{definicion}

\begin{definicion}[Distribución de Bernouilli]
    Se define la variable aleatoria con distribución de Bernouilli como:
    \begin{equation*}
        X = \left\{\begin{array}{cc}
            1 & \text{ocurre el suceso } E \\
            0 & \text{ocurre el suceso } F \\
        \end{array}\right.
    \end{equation*}

    Denotando $p$ como $P(E)\in [0,1]$, tenemos que la variable aleatoria en cuestión tiene distribución de Bernouilli, y se notará como sigue:
        \begin{equation*}
            X \leadsto B(1,p)
        \end{equation*}
\end{definicion}

Su función masa de probabilidad es:
\begin{equation*}
    f(x) = P[X=x] = \left\{\begin{array}{cc}
        p & x=1 \\
        1-p & x=0 \\
        0 & x\neq 0,1
    \end{array}\right.
\end{equation*}

Análogamente, es común escribirlo como:
$$f(x)=p^x (1-p)^{1-x} \qquad x=0,1$$

Su función de distribución es:
\begin{equation*}
    F_X(x) = P[X\leq x] \left\{\begin{array}{cl}
        0 & x<0 \\
        1-p & 0 \leq x < 1 \\
        1 & 1 \leq x
    \end{array}\right.
\end{equation*}

Sus momentos no centrados son:
\begin{equation*}
    m_k = E[X^k]
    = \sum_{x_i\in Re_X}x_i^kf(x_i)
    = 0^k\cdot (1-p) + 1^k\cdot p = p
    \qquad \forall k\in \mathbb{N}
\end{equation*}

Sus momentos centrados son:
\begin{multline*}
    \mu_k = E[(X-E[X])^k]
    = E[(X-p)^k]
    = \sum_{x_i\in Re_X}(x_i-p)^k   f(x_i)
    = (1-p)^k\cdot p + (0-p)^k(1-p) =\\=
    p(1-p)^k + (-p)^k(1-p)    
    \qquad \forall k\in \mathbb{N}
\end{multline*}

En particular,
\begin{equation*}
    \Var[X] = E[X^2] - E[X]^2 = p-p^2 = p(1-p)
\end{equation*}

Su función generatriz de momentos es:
\begin{equation*}
    M_X(t) = E[e^{tX}] = \sum_{x_i\in Re_X}e^{tx_i}f(x_i) = e^t\cdot p + e^0\cdot (1-p) = 1-p +e^tp = 1+p(e^t-1)     
    \qquad \forall t\in \mathbb{R}
\end{equation*}

\section{Distribución binomial}
\begin{definicion}[Distribución binomial]
    Se dice que una variable aleatoria $X$ sigue una distribución binomial de parámetros $n$ y $p$, $n\in \mathbb{N}$, $p\in ]0,1[$ si modela el número de éxitos en $n$ repeticiones independientes de un ensayo de Bernouilli con probabilidad $p$ de éxito, manteniéndose esta constante en las $n$ repeticiones. Se notará como $$X\leadsto B(n,p)$$
\end{definicion}

\begin{observacion}
    Es fácil ver que la distribución de Bernouilli es un caso particular de una distribución binomial, considerando $n=1$.
\end{observacion}

Razonemos la función masa de probabilidad. Como la variable $X$ modela el número de éxitos, tenemos que en total se producen $x$ éxitos y $n-x$ fracasos. Como la probabilidad se mantiene constante, tenemos que la probabilidad para cierta ordenación de los éxitos es $p^x(1-p)^{n-x}$. No obstante, las distintas formas de reorganizar los sucesos son combinaciones de $x$ éxitos entre un total de $n$ sucesos; es decir, combinaciones sin repetición. Por tanto, tenemos que en total hay $\binom{n}{x}$ formas distintas. Por tanto, tenemos el siguiente resultado:
\begin{prop}
    Sea $X\leadsto B(n,p)$. Entonces, la función masa de probabilidad de la distribución binomial es:
    \begin{equation*}
        f(x) = \binom{n}{x}p^x (1-p)^{n-x} \qquad x\in \{0,\dots,n\}
    \end{equation*}
\end{prop}
\begin{proof}
    Comprobemos que es una función masa de probabilidad. En primer lugar, es fácil ver que $f(x)\geq 0\;\forall x\in \{0,\dots,n\}$. Veamos ahora que $\sum\limits_{x=0}^n f(x)=1$:
    \begin{equation*}
        \sum_{x=0}^n f(x)
        = \sum_{x=0}^n \binom{n}{x}p^x (1-p)^{n-x}
        = [p + (1-p)]^n = 1^n = 1
    \end{equation*}
    donde he aplicado que $f(x)$ es el término $x-$ésimo del binomio de Newton.
\end{proof}

\begin{prop} Sea $X\leadsto B(n,p)$.
La función generatriz de momentos es:
    $$M_X(t) = [1 + p(e^t - 1)]^n$$
\end{prop}
\begin{proof}
    Buscamos emplear el binomio de Newton:
    \begin{equation*}
        (a+b)^n = \sum_{k=0}^n \binom{n}{k} a^kb^{n-k} \qquad \forall a,b\in \mathbb{R}, \;\forall n\in \mathbb{N}
    \end{equation*}

    Tenemos que:
    \begin{align*}
        E[e^{tX}] &= \sum_{x=0}^n e^{tx}\binom{n}{x}p^x(1-p)^{n-x}
        = \sum_{x=0}^n \binom{n}{x}(e^tp)^x(1-p)^{n-x}
        =\\&= (e^tp +1-p)^n = [1+p(e^t-1)]^n
    \end{align*}
\end{proof}

Una vez calculada la función generatriz de momentos, podemos calcular la esperanza y la varianza de forma sencilla.
\begin{prop}
    Sea $X\leadsto B(n,p)$. Entonces, tenemos que:
    \begin{equation*}
        E[X] = np
    \end{equation*}
\end{prop}
\begin{proof}
    Usando la función generatriz de momentos de la binomial:
    \begin{equation*}
        E[X] = \dfrac{d}{dt}M_X(t)\Big|_{t=0}
        = n[1+p(e^t-1)]^{n-1}pe^t\Big|_{t=0}
        = np
    \end{equation*}
\end{proof}

\begin{prop}
    Sea $X\leadsto B(n,p)$. Entonces, tenemos que:
    \begin{equation*}
        \Var[X] = np(1-p)
    \end{equation*}
\end{prop}
\begin{proof}
    Usando la función generatriz de momentos de la binomial:
    \begin{align*}
        E[X^2] &= \dfrac{d^2}{d^2t}M_X(t)\Big|_{t=0}
        = \dfrac{d}{dt}(n[1+p(e^t-1)]^{n-1}pe^t)\Big|_{t=0}
        =\\&= \left[n(n-1)[1+p(e^t-1)]^{n-2}(pe^t)^2+ n[1+p(e^t-1)]^{n-1}pe^t\right]\Big|_{t=0}
        = n(n-1)p^2+ np
    \end{align*}

    Por tanto:
    \begin{align*}
        \Var[X] &= E[X^2] - E[X]^2 = n(n-1)p^2 +np -n^2p^2 =np[p(n-1)+1-np] =\\&= np[pn-p+1-np] = np(1-p)
    \end{align*}
\end{proof}


\begin{prop}[Simetría]
    Si $X\leadsto B(n,p)$, entonces la variable que contabiliza el número de fracasos, $Y=n-X \leadsto B(n, 1-p)$ y, además,
    \begin{equation*}
        P[X=x] = P[Y=n-x]\qquad \forall x\in \{0,\dots,n\}
    \end{equation*}
\end{prop}
\begin{proof}
    Sabemos que se trata de nuevo de una distribución binomial con las mismas repeticiones. No obstante, por tener solo dos sucesos elementales tenemos que $P(f)=1-p$, siendo $f$ el fallo.
    La segunda expresión es también trivialmente cierta, ya que el número de fallos ha de ser $n$ menos el número de éxitos.\\

    Formalmente, consideramos la siguiente aplicación:
    \Func{g}{\bb{R}}{\bb{R}}{X}{Y=n-X}

    Por el Teorema de Cambio de Variable, tenemos que $Y$ es discreta y toma valores en $g(Re_X)$:
    \begin{equation*}
        g\left(\{0,\dots,n\}\right)= \{0,\dots,n\} = Re_X
    \end{equation*}

    Además, tenemos la segunda igualdad.
    \begin{equation*}
        P[Y=y]=P[X=n-y]
        \Longrightarrow
        P[Y=n-x] = P[X=n-n+x] = P[X=x]\qquad \forall x\in \{0,\dots,n\}
    \end{equation*}

    Para probar que $Y\leadsto B(n,1-p)$ tenemos que:
    \begin{equation*}
        P[Y=y]=P[X=n-y] = \binom{n}{n-y}p^{n-y}(1-p)^y
        \AstIg \binom{n}{y}p^{n-y}(1-p)^y \qquad \forall y\in \{0,\dots,n\}
    \end{equation*}
    donde en $(\ast)$ hemos empleado que:
    \begin{equation*}
        \binom{n}{n-y} = \dfrac{n!}{(n-y)!y!} = \binom{n}{y}
    \end{equation*}
\end{proof}

\begin{ejemplo}
    Tenemos una muestra de tornillos defectuosos, y sabemos que la probabilidad de que un tornillo sea defectuoso es $p=0.05$. Calcular la probabilidad de que en una muestra de 30 tornillos haya exactamente 5 defectuosos.

    Tenemos que el espacio muestral es $\Omega = \{T.Op, T.Def\}$
    
    Sea la variable aleatoria siguiente:
    \begin{center}
        $X$=``Número de tornillos defectuosos en una muestra de 30''
    \end{center}
    
    Tenemos que $X$ sigue una distribución binomial de la forma:
    \begin{equation*}
        X\leadsto B(30;0.05)
    \end{equation*}
    
    Por tanto,
    \begin{equation*}
        P[X=5] = \binom{30}{5}\cdot 0.05^5 \cdot 0.95^{25}
    \end{equation*}
\end{ejemplo}

\section{Distribución Geométrica}

\begin{definicion}[Distribución geométrica]
    Se dice que una variable aleatoria $X$ sigue una distribución geométrica de parámetro $p$, $p\in ]0,1[$ si modela el número de fracasos antes de llegar al primer éxito en un ensayo de Bernouilli con probabilidad $p$ de éxito, manteniéndose esta constante en todas las repeticiones. Se notará como $$X\leadsto \cc{G}(p)$$
\end{definicion}

Veamos cuál es su función masa de probabilidad. Como la variable $X$ modela el número de fracasos antes de llegar al primer éxito, tenemos que en total se producen $x$ fracasos y un éxito. Como la probabilidad se mantiene constante, tenemos que la probabilidad para cierta ordenación de los sucesos es $(1-p)^{x}p$. Por tanto:
\begin{equation*}
    f(x) = (1-p)^{x}p \qquad \forall x\in \mathbb{N}\cup \{0\}
\end{equation*}

Veamos que, efectivamente, se trata de una función masa de probabilidad.
\begin{prop}
    Sea $X\leadsto \cc{G}(p)$. Entonces, la función masa de probabilidad de la distribución geométrica es:
    \begin{equation*}
        f(x) = (1-p)^{x}p \qquad x\in \mathbb{N}\cup \{0\}
    \end{equation*}
\end{prop}
\begin{proof}
    Comprobemos que es una función masa de probabilidad. En primer lugar, es fácil ver que $f(x)\geq 0\;\forall x\in \mathbb{N}\cup \{0\}$. Veamos ahora que $\sum\limits_{x=0}^\infty f(x)=1$:
    \begin{equation*}
        \sum_{x=0}^\infty f(x)
        = \sum_{x=0}^\infty (1-p)^{x}p
        = p\sum_{x=0}^\infty (1-p)^{x}
        \AstIg p\cdot \frac{1}{1-(1-p)} = 1
    \end{equation*}
    donde en $(\ast)$ he aplicado la fórmula de la suma de una serie geométrica de razón $(1-p)$.
\end{proof}


\begin{prop}
    Sea $X\leadsto \cc{G}(p)$. Entonces, su función de distribución es:
    \begin{equation*}
        F_X(x) = P[X\leq x] = \begin{cases}
            1-(1-p)^{x+1} & x\geq 0 \\
            0 & x<0
        \end{cases}
    \end{equation*}
\end{prop}
\begin{proof}
    Tenemos que:
    \begin{equation*}
        F_X(x) = P[X\leq x] = \sum_{k=0}^x f(k) = \sum_{k=0}^x (1-p)^{k}p = p\sum_{k=0}^x (1-p)^{k}
    \end{equation*}

    Por tanto, y usando la fórmula de la suma parcial de una serie geométrica de razón $(1-p)$, tenemos que:
    \begin{equation*}
        F_X(x) = p\cdot \frac{1-(1-p)^{x+1}}{1-(1-p)} = 1-(1-p)^{x+1}
    \end{equation*}
\end{proof}

\begin{prop}
    Sea $X\leadsto \cc{G}(p)$. Entonces, su función generatriz de momentos es:
    \begin{equation*}
        M_X(t) = \frac{p}{1-(1-p)e^t} \qquad t<-\ln(1-p)
    \end{equation*}
\end{prop}
\begin{proof}
    Tenemos que:
    \begin{equation*}
        M_X(t) = E[e^{tX}] = \sum_{x=0}^\infty e^{tx}f(x) = \sum_{x=0}^\infty e^{tx}(1-p)^{x}p = p\sum_{x=0}^\infty [e^t(1-p)]^x
    \end{equation*}

    Veamos ahora qué ha de cumplir $t$ para que la serie converga. Como
    sabemos que $e^t(1-p)>0$, tan solo comprobamos que $e^t(1-p)<1$:
    \begin{equation*}
        e^t(1-p)<1 \sii e^t<\frac{1}{1-p} \sii t<\ln\left(\frac{1}{1-p}\right) = -\ln(1-p)
    \end{equation*}

    Por tanto, para $t<-\ln(1-p)$, la serie converge y, por tanto, la función generatriz de momentos es:
    \begin{equation*}
        M_X(t) = \frac{p}{1-(1-p)e^t}
    \end{equation*}
\end{proof}


\begin{coro}
    Sea $X\leadsto \cc{G}(p)$. Entonces, tenemos que:
    \begin{equation*}
        E[X] = \frac{1-p}{p}
    \end{equation*}
\end{coro}
\begin{proof}
    Tenemos que:
    \begin{equation*}
        E[X] = M_X'(t)\Big|_{t=0} = \frac{p}{(1-(1-p)e^t)^2}\cdot (1-p)e^t\Big|_{t=0} = \frac{p(1-p)}{p^2} = \frac{1-p}{p}
    \end{equation*}
\end{proof}

\begin{coro}
    Sea $X\leadsto \cc{G}(p)$. Entonces, tenemos que:
    \begin{equation*}
        \Var[X] = \frac{1-p}{p^2}
    \end{equation*}
\end{coro}
\begin{proof}
    Tenemos que:
    \begin{align*}
        E[X^2] &= M_X''(t)\Big|_{t=0} = p(1-p)\cdot \dfrac{e^t[1-(1-p)e^t]^{\cancel{2}} + e^t\cdot 2(1-p)e^t\cancel{(1-(1-p)e^t)}}{(1-(1-p)e^t)^{\cancelto{3}{4}}}\Big|_{t=0} =\\
        &= p(1-p)e^t\cdot \dfrac{1-(1-p)e^t + 2(1-p)e^t}{(1-(1-p)e^t)^3}\Big|_{t=0}
        = p(1-p)e^t\cdot \dfrac{1+(1-p)e^t}{(1-(1-p)e^t)^3}\Big|_{t=0} =\\
        &= p(1-p)\cdot \dfrac{1+(1-p)}{p^3}
        = (1-p)\cdot \dfrac{2-p}{p^2}
    \end{align*}

    Por tanto, tenemos que:
    \begin{equation*}
        \Var[X] = E[X^2] - E[X]^2
        = \dfrac{(1-p)(2-p)}{p^2} - \dfrac{(1-p)^2}{p^2}
        = \dfrac{(1-p)(2-p+1-p)}{p^2} = \dfrac{1-p}{p^2}
    \end{equation*}
\end{proof}

\begin{prop}[Falta de memoria]
    Sea $X\leadsto \cc{G}(p)$. Entonces, se cumple que:
    \begin{equation*}
        P(X\geq h + k \mid X\geq h) = P(X\geq k) \qquad \forall h,k \in \mathbb{N}\cup \{0\}
    \end{equation*}
    \begin{proof}
        Tenemos que:
        \begin{align*}
            P(X\geq h + k \mid X\geq h) &= \frac{P(X\geq h+k, X\geq h)}{P(X\geq h)} = \frac{P(X\geq h+k)}{P(X\geq h)} \AstIg\\
            \AstIg & \dfrac{(1-p)^{h+k}}{(1-p)^h} = (1-p)^k \AstIg P(X\geq k)
        \end{align*}
        donde en $(\ast)$ hemos usado que:
        \begin{equation*}
            P(X\geq x) = 1-P(X<x) = 1-P(X\leq x-1) = 1-(1-(1-p)^x) = (1-p)^x
        \end{equation*}
    \end{proof}
    % // TODO: Interpretación falta de memoria de Geométrica? No es la misma que la exponencial
\end{prop}

\section{Distribución binomial negativa}
\begin{definicion}[Distribución binomial negativa]
    Se dice que una variable aleatoria $X$ sigue una distribución binomial negativa de parámetros $r$ y $p$, $r\in \mathbb{N}$, $p\in ]0,1[$ si modela el número de fracasos antes de llegar al $r-$ésimo éxito en un ensayo de Bernouilli con probabilidad $p$ de éxito, manteniéndose esta constante en todas las repeticiones. Se notará como $$X\leadsto BN(r,p)$$
\end{definicion}
\begin{observacion}
    Es fácil ver que la Distribución Geométrica es un caso particular de una distribución binomial negativa, considerando $r=1$.
\end{observacion}

Razonemos la función masa de probabilidad. Como la variable $X$ modela el número de fracasos antes de llegar al $r-$ésimo éxito, tenemos que en total se producen $x$ fracasos y $r$ fracasos. Como la probabilidad se mantiene constante, tenemos que la probabilidad para cierta ordenación de los sucesos es $p^r(1-p)^{x}$. No obstante, las distintas formas de reorganizar los sucesos son combinaciones de $x$ fracasos y $r-1$ éxitos\footnote{El éxito $r-$ésimo no se añade porque su posición está fijada, ya que debe ser el último suceso.}; es decir, combinaciones sin repetición. Por tanto, tenemos que en total hay $\binom{x+r-1}{x}$ formas distintas. Por tanto, a priori deducimos que la función masa de probabilidad de la distribución binomial negativa es:
\begin{equation*}
    f(x) = \binom{x+r-1}{x}(1-p)^{x}p^r \qquad x\in \mathbb{N}\cup \{0\}
\end{equation*}

No obstante, hay una expresión alternativa de la función masa de probabilidad muy útil para las demostraciones. Para ello, se introduce la siguiente definición:
\begin{definicion}
    Dado $r\in \mathbb{R}, x\in \mathbb{N}$, se define\footnote{El número combinatorio está ya definido, pero con más restricciones. Esta definición concuerda con la anterior en los supuestos anteriores.} el siguiente número combinatorio:
    \begin{equation*}
        \binom{-r}{x} = \frac{(-r)(-r-1)\cdots (-r-x+1)}{x!};~\binom{-\alpha}{0}=1, \qquad \forall r\in \mathbb{R}, x\in \mathbb{N}
    \end{equation*}
\end{definicion}

Habiendo definido dicho número combinatorio, podemos introducir la expresión alternativa de la función masa de probabilidad:
\begin{prop}[Expresión alternativa] Se cumple la siguiente igualdad:
    \begin{equation*}
        f(x)=\binom{x+r-1}{x}(1-p)^{x}p^r=\binom{-r}{x}(p-1)^xp^r \qquad \forall x\in \mathbb{N}\cup \{0\}
    \end{equation*}
\end{prop}
\begin{proof}
    En primer lugar, veamos el siguiente resultado:
    \begin{multline*}
        \binom{x+r-1}{x}
        = \frac{(x+r-1)!}{x!(r-1)!}
        = \frac{(x+r-1)(x+r-2)\cdots(\cancel{x}+r-\cancel{x})\cancel{(r-1)!}}{x!\cancel{(r-1)!}}
        =\\= \frac{(x+r-1)(x+r-2)\cdots(r)}{x!}
        \stackrel{(\ast)}{=} (-1)^x\cdot \frac{(-r)(-r-1)\cdots (-r-x+1)}{x!}
        = (-1)^x\binom{-r}{x}
    \end{multline*}
    donde en $(\ast)$ he incluido en el numerador $x$ signos negativos y, por tanto, se incluye también el factor $(-1)^x$.
    
    Por tanto, se tiene que:
    \begin{equation*}
        \binom{x+r-1}{x}(1-p)^{x}p^r
        = (-1)^x\binom{-r}{x}(1-p)^xp^r
        = \binom{-r}{x}(p-1)^xp^r
    \end{equation*}
\end{proof}


Recordemos que tenemos lo que hemos deducido que es la función masa de probabilidad y su expresión alternativa. No obstante, no hemos demostrado que, efectivamente, se trata de una función masa de probabilidad.
\begin{prop}
    Sea $X\leadsto BN(r,p)$. Entonces, la función masa de probabilidad de la distribución binomial negativa es:
    \begin{equation*}
        f(x)
        = \binom{x+r-1}{x}(1-p)^{x}p^r
        = \binom{-r}{x}(p-1)^xp^r
        \qquad x\in \mathbb{N}\cup \{0\}
    \end{equation*}
\end{prop}
\begin{proof}
    Comprobemos que es una función masa de probabilidad. En primer lugar, es fácil ver que $f(x)\geq 0\;\forall x\in \mathbb{N}\cup \{0\}$ a partir de la primera expresión. Veamos ahora que $\sum\limits_{x=0}^\infty f(x)=1$. Para ello, usaremos el desarrollo en serie de potencias de la función $(1 + t)^\alpha$:
    \begin{equation}\label{eq:1+t}
        (1+t)^\alpha = \sum_{x=0}^\infty \binom{\alpha}{x}t^x,\qquad |t|<1, \alpha\in \mathbb{R}
    \end{equation}
    Usando la expresión alternativa de la función masa de probabilidad obtenida anteriormente y aplicando este desarrollo tenemos:
    \begin{equation*}
        \sum_{x=0}^\infty f(x)
        = \sum_{x=0}^\infty \binom{-r}{x}(p-1)^xp^r
        = p^r \sum_{x=0}^\infty \binom{-r}{x}(p-1)^x
        \stackrel{Ec.\;\ref{eq:1+t}}{=} p^r [1+(p-1)]^{-r} = p^rp^{-r}=1
    \end{equation*}
    donde he aplicado que $f(x)$ es el término $x-$ésimo del binomio de Newton.
\end{proof}

\begin{prop}
    Sea $X\leadsto BN(r,p)$. Entonces, tenemos que:
    \begin{equation*}
        E[X]=\frac{r(1-p)}{p}
    \end{equation*}
\end{prop}
\begin{proof}
    Demostramos haciendo uso de la expresión alternativa y de la Ecuación \ref{eq:1+t}:
    \begin{equation*}\begin{split}
        E[X]
        &=\sum_{x=0}^\infty xf(x)
        \stackrel{(\ast)}{=}\sum_{x=1}^\infty xf(x)
        =\sum_{x=1}^\infty x\binom{-r}{x}(p-1)^xp^r
        =p^r\sum_{x=1}^\infty x\binom{-r}{x}(p-1)^x
        =\\&= p^r\sum_{x=1}^\infty x\frac{(-r)(-r-1)\cdots (-r-x+1)}{x!}(p-1)^x
        =\\&= (-r)(p-1)p^r\sum_{x=1}^\infty \frac{(-r-1)\cdots (-r-x+1)}{(x-1)!}(p-1)^{x-1}
        =\\&= r(1-p)p^r\sum_{x=1}^\infty \binom{-r-1}{x-1}(p-1)^{x-1}
        \stackrel{[y=x-1]}{=} r(1-p)p^r\sum_{y=0}^\infty \binom{-r-1}{y}(p-1)^{y}
        = \\ &\stackrel{Ec.\;\ref{eq:1+t}}{=} r(1-p)p^r[1+(p-1)]^{-r-1}
        = r(1-p)p^r p^{-r-1}
        = r(1-p)p^{-1} = \frac{r(1-p)}{p}
    \end{split}\end{equation*}
    donde en $(\ast)$ he aplicado que la primera iteración es nula por ser $x=0$.
\end{proof}

\begin{lema}
    Sea $X\leadsto BN(r,p)$. Entonces, tenemos que:
    \begin{equation*}
        E[X^2]=\frac{r(r+1)(1-p)^2}{p^2} + \frac{r(1-p)}{p}
    \end{equation*}
\end{lema}
\begin{proof}
    Calculamos en primer lugar lo siguiente:
    \begin{equation*}\begin{split}
        E[X(X-1)]
        &= \sum_{x=0}^\infty x(x-1)f(x)
        = \sum_{x=2}^\infty x(x-1)\binom{-r}{x}(p-1)^xp^r \\
        &= p^r\sum_{x=2}^\infty x(x-1)\frac{(-r)(-r-1)\cdots (-r-x+1)}{x!}(p-1)^x \\
        &= (-r)(-r-1)p^r(p-1)^2\sum_{x=2}^\infty \frac{(-r-2)\cdots (-r-x+1)}{(x-2)!}(p-1)^{x-2} \\
        &= r(r+1)p^r(p-1)^2\sum_{x=2}^\infty \binom{-r-2}{x-2}(p-1)^{x-2} = \\
        &\stackrel{y=x-2}{=} r(r+1)p^r(p-1)^2\sum_{y=0}^\infty \binom{-r-2}{y}(p-1)^{y} \\
        &\stackrel{Ec.\;\ref{eq:1+t}}{=} r(r+1)p^r(p-1)^2[1+(p-1)]^{-r-2}
        = r(r+1)p^r(p-1)^2p^{-r-2} \\
        &= r(r+1)p^{-2}(p-1)^2
        = \frac{r(r+1)(1-p)^2}{p^2}
    \end{split}\end{equation*}

    Por tanto, tenemos que:
    \begin{multline*}
        E[X(X-1)] = E[X^2-X] = E[X^2] - E[X]
        \Longrightarrow \\ \Longrightarrow
        E[X^2] = E[X(X-1)] + E[X] = \frac{r(r+1)(1-p)^2}{p^2} + \frac{r(1-p)}{p}
    \end{multline*}
\end{proof}


\begin{coro}
    Sea $X\leadsto BN(r,p)$. Entonces, tenemos que:
    \begin{equation*}
        \Var[X]=\frac{r(1-p)}{p^2}
    \end{equation*}
\end{coro}
\begin{proof}
    Por la linealidad de la esperanza, tenemos que:
    \begin{equation*}\begin{split}
        \Var[X] &= E[X^2] - E[X]^2 = \frac{r(r+1)(1-p)^2}{p^2} + \frac{r(1-p)}{p} - \frac{r^2(1-p)^2}{p^2} =\\
        &=  \frac{\cancel{r^2(1-p)^2} +r(1-p)^2 - \cancel{r^2(1-p)^2}}{p^2} + \frac{r(1-p)}{p}
        = \frac{r(1-p)^2 + pr(1-p)}{p^2} =\\
        &= \frac{[r(1-p)][(1-p)+p]}{p^2}
        = \frac{r(1-p)}{p^2}
    \end{split}\end{equation*}
\end{proof}

\begin{prop}
    Sea $X\leadsto BN(r,p)$. Entonces, tenemos que:
    \begin{equation*}
        M_X(t)=\left(\frac{p}{[1-(1-p)e^t]}\right)^r \qquad \forall t<-\ln(1-p)
    \end{equation*}
\end{prop}
\begin{proof}
    Usando la expresión alternativa de la función masa de probabilidad:
    \begin{equation*}\begin{split}
        M_X(t)
        &=E[e^{tX}]
        = \sum_{x=0}^\infty e^tx \binom{-r}{x}(p-1)^xp^r
        = p^r\sum_{x=0}^\infty \binom{-r}{x}[e^t(p-1)]^x =\\
        & \stackrel{Ec.\;\ref{eq:1+t}}{=} p^r [1+[e^t(p-1)]]^{-r}
        = \frac{p^r}{[1-(1-p)e^t]^{r}}
    \end{split}\end{equation*}
    donde hemos empleado la Ecuación \ref{eq:1+t}. No obstante, esto solo es válido si:
    \begin{equation*}
        |e^t(p-1)|<1
        \Longleftrightarrow e^t(1-p)<1
        \Longleftrightarrow e^t < \frac{1}{1-p}
        \Longleftrightarrow t < -\ln(1-p)
    \end{equation*}
\end{proof}

\begin{ejemplo}
    Sabemos que la probabilidad de que te multen al conducir es $P(M)=~0.01$. Además, a las 3 te quitan el carnet. Calcular la probabilidad de que te quiten el carnet la décima vez que te subes al coche.

    Sea $X=$``Número de veces que te subes al coche antes de la tercera multa.''
    
    Tenemos que $X\leadsto BN(3,0.01)$. El número de fracasos antes del tercer éxito son 7. Por tanto,
    \begin{equation*}
        P(X=7) = \binom{9}{7} 0.01^3 0.99^7
    \end{equation*}
\end{ejemplo}


\section{Distribución Hipergeométrica}

\begin{definicion}[Distribución hipergeométrica]
    Supongamos una población de $N$ individuos en dos categorías de $N_1$ y $N_2 =N-N_1$ individuos cada una. Se elige una muestra de $n$ individuos de la población (sin reemplazamiento o simultáneamente). La variable aleatoria $X$ que contabiliza el número de individuos de la primera categoría en la muestra se dice sigue una distribución hipergeométrica de parámetros $N$, $N_1$ y $n$ y se denota:
    \begin{equation*}
        X\leadsto H(N,N_1,n) \hspace{1cm} N,N_1,n\in \mathbb{N},\quad N_1,n\leq N
    \end{equation*}
\end{definicion}


Tenemos que su función masa de probabilidad viene determinada por la Ley de Laplace:
\begin{equation*}
    f(x) = \frac{\binom{N_1}{x}\binom{N-N_1}{n-x}}{\binom{N}{n}}
    \hspace{1cm}
    x\in \left[\max\{0, n-(N-N_1)\}, \min\{n, N_1\}\right]
\end{equation*}
El denominador son las combinaciones totales de $n$ individuos de un total de $N$. En el denominador, tenemos que son las combinaciones de los $x$ individuos de la población $N_1$ por las combinaciones de los $n-x$ de la población $N_2$.\\

Comprobemos que se trata de una función masa de probabilidad:
\begin{prop}
    Sea $X\leadsto H(N,N_1,n)$. Entonces, su función masa de probabilidad es:
    \begin{equation*}
        f(x) = \frac{\binom{N_1}{x}\binom{N-N_1}{n-x}}{\binom{N}{n}}
        \hspace{1cm}
        x\in \left[\max\{0, n-(N-N_1)\}, \min\{n, N_1\}\right]
    \end{equation*}
\end{prop}
\begin{proof}
    Como todos los términos son positivos, es fácil ver que $f(x)\geq 0\quad \forall x$. Veamos ahora que su suma es igual a $1$. Para ello, empleamos el siguiente resultado, que no se demuestra al no entrar dentro del temario de EDIP sino de Cálculo.
    \begin{equation*}
        \sum_{x=0}^n \binom{a}{x}\binom{b}{n-x} = \binom{a+b}{n}, \qquad \forall a,b\in \mathbb{R},~n\in \mathbb{N}
    \end{equation*}

    Notemos que en la suma del primer miembro algunos sumandos pueden ser nulos. En efecto, si $x > a$, entonces $\binom{a}{x}=0$ (puesto que habrá un término nulo en $a(a-1)\cdots(a-x+1)$). De igual forma, si $n - x > b$ se verifica que $\binom{b}{n-x}=0$. En consecuencia, los únicos sumandos no nulos son aquellos comprendidos entre $\max\{0, n - b\}$ y $\min\{n, a\}$ y, por tanto:
    \begin{equation*}
        \sum_{x=0}^n \binom{a}{x}\binom{b}{n-x}
        = \sum_{\mathclap{x=\max\{0, n - b\}}}^{\min\{n, a\}} \binom{a}{x}\binom{b}{n-x}
        = \binom{a+b}{n}, \qquad \forall a,b\in \mathbb{R},~n\in \mathbb{N}
    \end{equation*}

    Por tanto,
    \begin{equation*}
        \sum_{x=0}^\infty f(x)
        = \sum_{x=0}^\infty \frac{\binom{N_1}{x}\binom{N-N_1}{n-x}}{\binom{N}{n}}
        = \sum_{\mathclap{\max\{0, n-(N-N_1)\}}}^{\min\{n, N_1\}} \frac{\binom{N_1}{x}\binom{N-N_1}{n-x}}{\binom{N}{n}}
        = \frac{\binom{N_1+N-N_1}{x+n-x}}{\binom{N}{n}}
        = \frac{\binom{N}{n}}{\binom{N}{n}} = 1
    \end{equation*}
    
\end{proof}

\begin{prop}
    Sea $X\leadsto H(N,N_1,n)$. Entonces, tenemos que:
    \begin{equation*}
        E[X]= n\frac{N_1}{N}
    \end{equation*}
\end{prop}
\begin{prop}
    Sea $X\leadsto H(N,N_1,n)$. Entonces, tenemos que:
    \begin{equation*}
        \Var[X]=\frac{n(N-n)N_1(N-N_1)}{N^2(N-1)}
    \end{equation*}
\end{prop}
% // TODO: Esperanza y varianza de la hipergeométrica
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T05_MediaVarianzaHipergeometrica.pdf


\begin{prop}[Aproximación de Hipergeométrica a Binomial]
    Sea $X\leadsto H(N,N_1,n)$. Entonces, tenemos que, definiendo $p=\frac{N_1}{N}$, $X$ se aproxima a una distribución binomial $B(n,p)$.

    De forma empírica, se ha demostrado que esta aproximación es adecuada si $N_1\leq~0.1N$ o, equivalentemente, $p\leq 0.1$.
    
\end{prop}
\begin{proof}
    Tomando $N_1=Np$, consideremos la función masa de probabilidad de la variable $X$ y la desarrollamos de la siguiente forma:
    \begin{align*}
        P(X=x)
        &= \frac{\binom{Np}{x}\binom{N(1-p)}{n-x}}{\binom{N}{n}}
        = \frac{(Np)!}{x!(Np-x)!}\frac{(N(1-p))!}{(n-x)!(N(1-p)-n+x)!}\frac{n!(N-n)!}{N!} =\\
        &= \frac{n!}{x!(n-x)!}\frac{(Np)!}{(Np-x)!}\frac{(N(1-p))!}{(N(1-p)-n+x)!}\frac{(N-n)!}{N!}=\\
        &= \binom{n}{x} \cdot \dfrac{Np(Np-1)\cdots (Np-x+1)}{N(N-1)\cdots (N-n+1)}
        \cdot \\&\qquad \cdot \dfrac{N(1-p)(N(1-p)-1)\cdots (N(1-p)-n+x+1)}{N(N-1)\cdots (N-n+1)}
    \end{align*}
    Si observamos la última expresión, tanto el numerador como el denominador son polinomios
    en $N$ de grado $n$, cuyos términos de mayor grado son, respectivamente, $(Np)^x(N(1-p))^{n-x}$ y $N^n$. Por tanto, tomando límites cuando $N$ tiende a infinito, se tiene:
    \begin{align*}
        \lim_{N\to \infty}f(x)
        &= \lim_{N\to \infty} \binom{n}{x} \cdot \dfrac{Np(Np-1)\cdots (Np-x+1)}{N(N-1)\cdots (N-n+1)}
        \cdot \\&\qquad \cdot \dfrac{N(1-p)(N(1-p)-1)\cdots (N(1-p)-n+x+1)}{N(N-1)\cdots (N-n+1)}
        =\\&= \binom{n}{x} \cdot \lim_{N\to \infty} \dfrac{N^x(Np)^x(N(1-p))^{n-x}}{N^n}
        = \binom{n}{x} \lim_{N\to \infty} \dfrac{N^np^x(1-p)^{n-x}}{N^n}
        =\\&= \binom{n}{x} p^x(1-p)^{n-x}
    \end{align*}
\end{proof}

\begin{ejemplo}
    Tenemos un grupo de 20 empleados en el que hay 15 hombres y 5 mujeres y elegimos 6 personas para formar un grupo de forma completamente aleatoria.
    Calcular la probabilidad de que en el grupo haya 2 mujeres.\\
    
    Definimos nuestra variable aleatoria $X$ como el número de mujeres en el grupo que cogemos. Notemos que:
    \begin{equation*}
        H\leadsto (20, 5, 6)
    \end{equation*}
    
    Calculamos la probabilidad de que haya dos mujeres en el grupo:
    \begin{equation*}
        P[X=2]=\frac{\binom{5}{2}\binom{15}{4}}{\binom{20}{6}} = 0.3521
    \end{equation*}
\end{ejemplo}


\section{Distribución de Poisson}

\begin{definicion}[Distribución de Poisson]
    Sea $X$ una variable aleatoria. Se dice que $X$ sigue una distribución de Poisson si representa el número de de ocurrencias de un determinado suceso durante un periodo de tiempo fijo o una región fija del espacio.

    Ha de cumplir las siguientes condiciones:
    \begin{enumerate}
        \item El número de ocurrencias en un intervalo o región específicas ha de ser independiente de las ocurrencias en otras zonas o intervalos de tiempo.

        \item Si se considera un intervalo de tiempo muy pequeño (o una región muy pequeña):
        \begin{itemize}
            \item La probabilidad de una ocurrencia es proporcional a la longitud del intervalo (el volumen de la región).
            \item La probabilidad de dos o más ocurrencias es despreciable.
        \end{itemize}
    \end{enumerate}

    Dicho de otra manera, si de media se tiene que en determinado intervalo se producen $\lambda$ ocurrencias, tenemos que sigue una distribución de Poisson de parámetro $\lambda$, notado por:
    \begin{equation*}
        X\leadsto \mathcal{P}(\lambda)
    \end{equation*}
\end{definicion}

El razonamiento de su función masa de probabilidad no se explica en el presente documento, ya que se basa en la teoría de ecuaciones diferenciales. No obstante, demostramos que, efectivamente, es una función masa de probabilidad:
% // TODO: Razonamiento f.m.p. Poisson
\begin{prop} Sea $X\leadsto \mathcal{P}(\lambda)$. Entonces, su función masa de probabilidad es:
    \begin{equation*}
        f(x)=e^{-\lambda}\frac{\lambda^x}{x!}
    \end{equation*}
\end{prop}
\begin{proof}
    Como todos los términos son positivos, tenemos que $f(x)\geq 0$ para todo $x\in \mathbb{N}\cup \{0\}$. Comprobemos ahora que la suma es la unidad:
    \begin{equation*}
        \sum_{x=0}^\infty f(x)
        = \sum_{x=0}^\infty e^{-\lambda}\frac{\lambda^x}{x!}
        = e^{-\lambda}\sum_{x=0}^\infty \frac{\lambda^x}{x!}
        \stackrel{(\ast)}{=} e^{-\lambda}e^\lambda = 1
    \end{equation*}
    donde en $(\ast)$ he aplicado el desarrollo en serie de Taylor de la exponencial.
\end{proof}


\begin{prop}
    Sea $X\leadsto \mathcal{P}(\lambda)$. Entonces, tenemos que:
    \begin{equation*}
        M_X(t)= e^{\lambda(e^t-1)}
    \end{equation*}
\end{prop}
\begin{proof} Tenemos que la función generatriz de momentos es:
\begin{equation*}
    M_X(t)=E[e^{tX}] = \sum_{x=0}^\infty e^{tx}f_X(x) = \sum_{x=0}^\infty e^{tx}e^{-\lambda}\frac{\lambda^x}{x!}
    =e^{-\lambda}\sum_{x=0}^\infty \frac{(e^t\lambda)^x}{x!} = e^{-\lambda}e^{e^t\lambda} = e^{\lambda(e^t-1)}
\end{equation*}
\end{proof}


\begin{prop}
    Sea $X\leadsto \mathcal{P}(\lambda)$. Entonces, tenemos que:
    \begin{equation*}
        E[X]= \lambda
    \end{equation*}
\end{prop}
\begin{proof} 
    Tenemos dos opciones:
    \begin{description}
        \item[Opción 1]  Partiendo de la definición de esperanza matemática:
        \begin{equation*}
            E[X]=\sum_{x=0}^\infty xe^{-\lambda}\frac{\lambda^x}{x!}
            = e^{-\lambda}\cdot \lambda \sum_{x=1}^\infty \frac{\lambda^{x-1}}{(x-1)!}
            = e^{-\lambda}\cdot \lambda \sum_{k=0}^\infty \frac{\lambda^k}{k!}
            \stackrel{(\ast)}{=} e^{-\lambda}\lambda e^{\lambda}=\lambda
        \end{equation*}
        donde en $(\ast)$ he aplicado el desarrollo en serie de Taylor de la exponencial.

        \item[Opción 2] Usando la función generatriz de momentos:
        \begin{equation*}
            E[X]=M_X'(0)=\left. \frac{d}{dt} e^{\lambda(e^t-1)}\right|_{t=0} = \left. \lambda e^t \cdot e^{\lambda(e^t-1)}\right|_{t=0} = \lambda
        \end{equation*}
    \end{description}
\end{proof}

\begin{lema}
    Sea $X\leadsto \mathcal{P}(\lambda)$. Entonces, tenemos que:
    \begin{equation*}
        E[X^2]= \lambda^2+\lambda
    \end{equation*}
\end{lema}
\begin{proof} 
    Igualmente, tenemos dos opciones:
    \begin{description}
        \item[Opción 1]  Partiendo de la definición de esperanza matemática:
        \begin{equation*}
            E[X(X-1)]=\sum_{x=0}^\infty x(x-1)e^{-\lambda}\frac{\lambda^x}{x!}
            = e^{-\lambda}\cdot \lambda^2 \sum_{x=2}^\infty \frac{\lambda^{x-2}}{(x-2)!}
            = e^{-\lambda}\cdot \lambda^2 \sum_{k=0}^\infty \frac{\lambda^k}{k!} = e^{-\lambda}\lambda^2 e^{\lambda}=\lambda^2
        \end{equation*}
        donde he usado la expresión del polinomio de Taylor de la exponencial. Por tanto,
        \begin{equation*}
            E[X(X-1)]=E[X^2]-E[X]=\lambda^2 \Longrightarrow E[X^2]=\lambda^2 +\lambda
        \end{equation*}

        \item [Opción 2] Usando la función generatriz de momentos:
        \begin{align*}
            E[X^2]&=M_X''(0)=\left. \frac{d^2}{dt^2} e^{\lambda(e^t-1)}\right|_{t=0} = \left. \dfrac{d}{dt} \lm \cdot e^{\lm (e^t-1) +t}\right|_{t=0} =\\&= \left.\lm e^{\lm(e^t-1)+t}(\lm e^t+1)\right|_{t=0} = \lambda^2+\lambda
        \end{align*}
    \end{description}
\end{proof}

\begin{coro}
    Sea $X\leadsto \mathcal{P}(\lambda)$. Entonces, tenemos que:
    \begin{equation*}
        \Var[X]= \lambda
    \end{equation*}
\end{coro}
\begin{proof} Tenemos que la varianza es:
    \begin{equation*}
        \Var[X]=E[X^2]-E[X]^2 = \lambda^2+\lambda-\lambda^2=\lambda
    \end{equation*}
\end{proof}


\begin{prop}[Aproximación de Binomial a la Poisson]
    Sea $X\leadsto B(n,p)$. Entonces, tenemos que, si $n$ es muy grande y $p\approx 0$, podemos aproximarla mediante una distribución de Poisson.
    
    Definiendo $\lambda=np$, $X$ se aproxima a una distribución de Poisson $P(\lambda)$.

    De forma empírica, se ha demostrado que esta aproximación es adecuada si $np\leq~5$.
\end{prop}
\begin{proof}
    Reescribimos la función de masa de probabilidad de la binomial como sigue:
    \begin{align*}
        P[X=x] &= \binom{n}{x}p^x(1-p)^{n-x} = \frac{n!}{x!(n-x)!}\dfrac{(np)^x}{n^x}(1-p)^{n-x}
        =\\&= \dfrac{(np)^x}{x!}\dfrac{n(n-1)\cdots (n-x+1)}{n^x}(1-p)^{n-x}
    \end{align*}

    Consideramos ahora $n\to \infty$ y $np\to\lm$. Entonces, tenemos que:
    \begin{itemize}
        \item $\displaystyle \lim_{n\to \infty}\dfrac{n(n-1)\cdots (n-x+1)}{n^x} = 1$
        \item $\displaystyle \lim_{np\to \lm}\dfrac{(np)^x}{x!} = \dfrac{\lm^x}{x!}$
        \item Como $n\to \infty$ y $np\to \lm$, tenemos que $p\to 0$, de donde:
        \begin{equation*}
            \lim_{\substack{n\to \infty\\np\to \lm}}(1-p)^{n-x}
            \AstIg e^{\lim\limits_{\substack{n\to \infty\\np\to \lm}}(n-x)(-p)} = e^{-\lm}
        \end{equation*}
        donde en $(\ast)$ hemos empleado la Fórmula de Moivre\footnote{Concepto visto en Cálculo I para las indeterminaciones del tipo $1^\infty$.}.
    \end{itemize}

    Por tanto, tenemos que:
    \begin{equation*}
        \lim_{\substack{n\to \infty\\np\to \lm}}P[X=x] = \frac{\lm^x}{x!}e^{-\lm}
    \end{equation*}

    Por tanto, hemos demostrado que, si $n$ es muy grande y $p\approx 0$, la distribución binomial se aproxima a una distribución de Poisson.
\end{proof}
% // TODO: Aproximacion de Binomial a Poisson
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T05_PoissonBinomial.pdf


\begin{observacion}[Cambio de Intervalo de Tiempo]
    Podremos cambiar el intervalo de tiempo al que está relacionada nuestra variable aleatoria de distribución de Poisson, alterando el valor de $\lambda$ y pasando a un nuevo valor $\lambda t$, donde $t$ es la cantidad de veces que se repite el nuevo intervalo en el antiguo.
\end{observacion}
\begin{ejemplo}
    Un dispositivo falla $1.2$ veces en 6 meses. Se pide calcular la probabilidad de que dicho dispositivo falle 2 veces en 6 meses y calcular la probabilidad de que dicho dispositivo falle menos de 5 veces en 2 años y medio.\\

    Sea $X$ la variable aleatoria que determina el número de veces que falla en 6 meses. Tenemos que $X\leadsto \mathcal{P}(1.2)$.

    Entonces, tenemos que la probabilidad de que dicho dispositivo falle 2 veces en 6 meses es:
    \begin{equation*}
        P[X=2]=0.2169
    \end{equation*}

    Por tanto, si $X'$ es una variable aleatoria que determina el número de veces que un dispositivo falla en $2.5$ años, tenemos que $X\leadsto \mathcal{P}(5\cdot 1.2)=\mathcal{P}(6)$. Por tanto,
    \begin{equation*}
        P[X'< 5] =P[X'\leq 4]=\sum_{x_i=0}^4f(x_i) = 0.0025 + 0.0149 + 0.0446 +0.0892+0.1339=0.2851
    \end{equation*}
\end{ejemplo}