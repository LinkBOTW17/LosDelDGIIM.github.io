\chapter{Probablidad condicionada e Independencia}
\section{Definiciones}
\begin{definicion}
    Sea $(\Omega, \mathcal{A}, P)$ un espacio probabilístico arbitrario y $A \in \mathcal{A} \mid P(A)>0$, definimos $\forall B \in \mathcal{A}$ la \textbf{probabilidad condicionada de $B$ sobre $A$} como:
    $$P(B\mid A) = \dfrac{P(B \cap A)}{P(A)}$$
\end{definicion}

Es decir, acabamos de definir la función $P(\cdot \mid A):\mathcal{A} \rightarrow [0,1]$
que nos lleva a cada $B \in \mathcal{A}$ en $P(B\mid A)$.\\


Veamos que esta nueva función cumple la axiomática de Kolmogorov y que, por tanto,
puede definir una nueva probabilidad a partir de la ya definida.
\begin{prop}
    Sea $(\Omega, \mathcal{A}, P)$ un espacio probabilístico arbitrario y $A \in \mathcal{A} \mid P(A)>0$. Para todo $B\in \cc{A}$, la probabilidad dondicionada de $B$ sobre $A$ cumple las tres condiciones de la Axiomática de Kolmogorov.
\end{prop}
\begin{proof}
    Demostramos cada una de las 3 condiciones:
    \begin{enumerate}
        \item \underline{Axioma de la no-negatividad}
        $$P(B\mid A) = \dfrac{P(B \cap A)}{P(A)} \geq 0 ~~~~\forall B \in \mathcal{A}$$
        
        \item \underline{Axioma del suceso seguro}
        $$P(\Omega \mid A) = \dfrac{P(\Omega \cap A)}{P(A)} = \dfrac{P(A)}{P(A)} = 1$$
        
        \item \underline{Axioma de $\sigma$-aditividad o aditividad numerable}

        Sea $A_1, A_2, \ldots \in \mathcal{A}$ una sucesión de sucesos incompatibles, entonces:
        \begin{equation*}\begin{split}
            P\left(\bigcup_{i=1}^\infty B_i \mid A\right)&
            = \dfrac{P\left[ \left(\bigcup\limits_{i=1}^\infty B_i\right) \cap A \right]}{P(A)}
            = \dfrac{P\left(\bigcup\limits_{i=1}^\infty B_i \cap A \right)}{P(A)}
            = \\&
            = \dfrac{\sum\limits_{i=1}^\infty P(B_i \cap A)}{P(A)}
            = \sum_{i=1}^\infty \dfrac{P(B_i \cap A)}{P(A)}
            = \sum_{i=1}^\infty P(B_i \mid A) \qedhere
        \end{split}\end{equation*}
    \end{enumerate}
\end{proof}

Por tanto, $P(\cdot \mid A)$ es también una probabilidad y por tanto, cumple todas las consecuencias vistas en la sección anterior.

\section{Teoremas}

Observemos que de la propia definición de la probabilidad condicionada se tiene que:
\begin{gather*}
    P(A \cap B) = P(A) P(B \mid A) \quad \text{ Si } \quad P(A) > 0 \\
    P(A \cap B) = P(B) P(A \mid B) \quad \text{ Si } \quad P(B) > 0
\end{gather*}

Ambas ecuaciones se suman a la lista ya disponible de cálculo de probabilidades de intersecciones:
$$P(A \cap B) = P(A) + P(B) - P(A \cup B)$$

\begin{teo}[de la probabilidad compuesta]
    Sea $(\Omega, \mathcal{A}, P)$ un espacio de probabilidad y $A_1, A_2, \ldots, A_n \in \mathcal{A}$ con 
    $P\left( \bigcap\limits_{i=1}^{n-1} A_i \right) > 0$, entonces:
    $$
    P\left( \bigcap_{i=1}^n A_i \right)
    = P(A_1) P(A_2 \mid A_1) P(A_3 \mid A_1 \cap A_2) \cdot \ldots \cdot
    P\left( A_n \mid \bigcap_{i=1}^{n-1} A_i \right)
    $$
\end{teo}
\begin{proof}
  Realizamos inducción sobre $n$:
  \begin{itemize}
      \item \underline{Para $n=2$}: Lo tenemos visto.

      \item \underline{Supongámoslo cierto para $n-1$, veamos el caso $n$}:
      \begin{multline*}
          P\left( \bigcap_{i=1}^n A_i \right) 
          = P\left( \bigcap_{i=1}^{n-1} A_i \cap A_n \right)
          = P\left( A_n \mid \bigcap_{i=1}^{n-1} A_i \right) P\left( \bigcap_{i=1}^{n-1} A_i \right) =\\
          \mathop{=}^{HI} P(A_1) P(A_2 \mid A_1) P(A_3 \mid A_1 \cap A_2) \cdot \ldots \cdot P\left( A_{n-1} \mid \bigcap_{i=1}^{n-2} A_i \right) P\left( A_n \mid \bigcap_{i=1}^{n-1} A_i \right)
      \end{multline*}
  \end{itemize}
\end{proof}

\begin{teo}[de la Probabilidad Total]
    Sea $(\Omega, \mathcal{A}, P)$ un espacio de probabilidad y sea $\{A_n\}_{n \in \N} \subset \mathcal{A}$ una partición de $\Omega$ con $P(A_n)>0~~\forall n \in \N$. Entonces, $\forall B \in \mathcal{A}$:
    $$P(B) = \sum_{i=1}^\infty P(A_i)P(B \mid A_i) $$
\end{teo}
\begin{proof}
    \begin{multline*}
        P(B)
        = P(B \cap \Omega)
        = P\left(B \cap \left( \bigcup_{i=1}^\infty A_i \right)\right)
        = P\left(\bigcup_{i=1}^\infty (B \cap  A_i) \right) = \\
        = \sum_{i=1}^\infty P(A_i \cap B)
        = \sum_{i=1}^\infty P(A_i) P(B \mid A_i)
    \end{multline*}
\end{proof}

\begin{ejemplo}
    Tenemos dos cajas: A, que contiene 3 bolas rojas y 1 negra; y B, que contiene 5 bolas rojas y 8 negras.
    
    La probabilidad de que cojamos una bola de la caja A es de $1/3$ mientras que la probabilidad de coger una bola de la caja B es de $2/3$. Calcular la probabilidad de que salga una bola negra.
    $$P(A) = 1/3 \qquad P(B) = 2/3$$
    $$P(N) = P(A)P(N \mid A) + P(B)P(N \mid B) = \frac{1}{3} \frac{1}{4} + \frac{2}{3} \frac{8}{13} = \frac{77}{156} \approx 0.49$$
\end{ejemplo}

\begin{teo}[Regla de Bayes]
  Sea $(\Omega, \mathcal{A}, P)$ un espacio de probabilidad y sea $\{A_n\}_{n \in \N} \subset \mathcal{A}$ una partición de $\Omega$ con $P(A_n)>0~~\forall n \in \N$. Entonces, $\forall B \in \mathcal{A}$:
  $$P(A_j \mid B) = \dfrac{P(B \mid A_j)P(A_j)}{\sum\limits_{i=1}^\infty P(B \mid A_i)P(A_i)}\qquad j \in \N$$
\end{teo}
\begin{proof}
  Sea $j \in \N$:
  $$P(A_j \mid B) = \dfrac{P(A_j \cap B)}{P(B)} =
    \dfrac{P(B \mid A_j)P(A_j)}{\sum\limits_{i=1}^\infty P(A_i) P(B \mid A_i)}$$
\end{proof}

\begin{ejemplo}
    Continuando con el ejemplo anteriormente propuesto, calculamos la probabilidad de que hayamos obtenido la bola en la caja A en el caso de que hayamos obtenido una bola negra:
    $$P(A \mid N) = \dfrac{P(N \mid A)P(A)}{P(A)P(N \mid A)+P(B)P(N \mid B)} =
      \dfrac{\frac{1}{4}\frac{1}{3}}{\frac{1}{3}\frac{1}{4}+\frac{2}{3}\frac{8}{13}}=\frac{13}{77}
      \approx 0.1688$$
\end{ejemplo}

\section{Independencia de sucesos}
Sea $(\Omega, \mathcal{A}, P)$ un espacio de probabilidad y $A \in \mathcal{A}$ con $P(A)>0$. La ocurrencia del suceso $A$ puede alterar la probabilidad de ocurrencia de cualquier otro suceso $B \in \mathcal{A}$.
Al estudiar dichas probabilidades, puede darse que:
\begin{itemize}
  \item $P(B \mid A) \neq P(B)$, es decir, la ocurrencia de $A$ modifica la probabilidad de $B$. Diremos
        entonces que $B$ \textbf{depende} de $A$.
        \begin{itemize}
          \item Si $P(B \mid A)>P(B)$, se dice que el suceso $A$ favorece al $B$.
          \item Si $P(B \mid A)<P(B)$, se dice que el suceso $A$ desfavorece al $B$.
        \end{itemize}
  \item $P(B \mid A) = P(B)$, es decir, la ocurrencia de $A$ no tiene ningún efecto sobre el suceso
        $B$, se dice que el suceso $B$ es \textbf{independiente} del suceso $A$.
\end{itemize}

\begin{teo} [Caracterización de la independencia] Sea $A,B\in \mathcal{A}\mid P(B)>0$. Entonces:
\begin{equation*}
    \text{$A$ es independiente a $B$} \Longleftrightarrow P(A\cap B) = P(A)P(B)
\end{equation*}
\end{teo}
\begin{proof} Procedemos mediante doble implicación:
\begin{description}
    \item [$\Longrightarrow $)] Suponemos $A$ y $B$ independientes, es decir, $P(A|B)=P(A)$.
    \begin{equation*}
        P(A\cap B) = P(A|B)P(B) = P(A)P(B)
    \end{equation*}

    \item [$\Longleftarrow $)] Suponemos $P(A\cap B)=P(A)P(B)$
    \begin{equation*}
        P(A)=\frac{P(A)P(B)}{P(B)} = \frac{P(A\cap B)}{P(B)} = P(A|B)
    \end{equation*}
\end{description}
\end{proof}

\begin{coro}[Simetría de la independencia] Sean $A,B\in \mathcal{A}$. Entonces:
\begin{equation*}
    \text{$A$ es independiente a $B$} \Longleftrightarrow
    \text{$B$ es independiente a $A$}
\end{equation*}
\end{coro}
\begin{proof}
    \begin{equation*}
        P(A\cap B) = P(A)P(B) = P(B)P(A) = P(B\cap A)
    \end{equation*}
\end{proof}

\begin{prop}
  Si $A,B\in \cc{A}$ son independientes. Entonces, son equivalentes:
  \begin{enumerate}
    \item $A$ y $\overline{B}$ son independientes.
    \item $\overline{A}$ y $B$ son independientes.
    \item $\overline{A}$ y $\overline{B}$ son independientes.
  \end{enumerate}
\end{prop}
\begin{proof}
    Demostramos cada una de las partes, partiendo de que: $$P(A\cap B)=P(A)P(B)$$
    \begin{enumerate}
        \item Probemos que $A$ y $\overline{B}$ son independientes:
        \begin{multline*}
            P(A)P(\overline{B}) = P(A)(1-P(B)) = P(A)-P(A)P(B) =\\= P(A) - P(A \cap B) = P(A - B) = P(A \cap \overline{B})
        \end{multline*}

        \item $A$ y $B$ son independientes $\Leftrightarrow B$ y $A$ son independientes $\Leftrightarrow B$ y $\overline{A}$ son independientes $\Leftrightarrow \overline{A}$ y $B$ son independientes.

        \item $A$ y $B$ son independientes $\Leftrightarrow \overline{A}$ y $B$ son independientes.
        
        Sea $C = \overline{A}$. Sabemos que $C$ y $B$ son independientes $\Leftrightarrow C$ y $\overline{B}$ son independientes $\Leftrightarrow
    \overline{A}$ y $\overline{B}$ son independientes.
    \end{enumerate}
\end{proof}

\begin{prop}
    Sean $A,B \in \cc{A}$ con $A,B \neq \emptyset$, entonces:
    \begin{center}
        Si $A$ y $B$ son independientes $\Rightarrow$ No son incompatibles $(A \cap B \neq \emptyset)$
    \end{center}
\end{prop}
\begin{proof}
  $P(A) > 0 \ \land \ P(B)>0$ Luego:
  $$P(A \cap B) = P(A) P(B) > 0 \Rightarrow P(A \cap B) \neq 0 \Rightarrow A \cap B \neq \emptyset$$
\end{proof}

Por último, destacamos dos últimas definiciones en cuanto a sucesos independientes:
\begin{definicion} [Independencia dos a dos]
    Dado un espacio probabilístico $(\Omega, \mathcal{A}, P)$ y una clase de sucesos $\mathcal{U} \subset \mathcal{A}$ no vacía, diremos que sus sucesos son \textbf{independientes dos a dos} si
    $$\forall A, B \in \mathcal{U} \mid A \neq B \Rightarrow A \mbox{ y } B \mbox{ son independientes}$$
\end{definicion}

\begin{definicion} [Independencia mutua]
    Dado un espacio probabilístico $(\Omega, \mathcal{A}, P)$ y una clase de sucesos $\mathcal{U} \subset \mathcal{A}$ no vacía, diremos que sus sucesos son \textbf{mutuamente independientes} si para cada subcolección finita $\{A_{i1}, A_{i2}, \ldots, A_{ik}\}$ de sucesos distintos de $\mathcal{U}$ se verifica:
    $$P(A_{i1} \cap A_{i2} \cap \ldots \cap A_{ik}) = \prod_{j=1}^k P(A_{ij})$$
\end{definicion}