\chapter{Estadística descriptiva bidimensional}

\section{Distribución conjunta de dos caracteres estadísticos}

Sea una población formada por $n$ individuos en la que se desea estudiar simultáneamente dos caracteres, $X$ e $Y$.
Dichos caracteres podrán ser ambos cualitativos, uno cualitativo y otro cuantitativo o ambos cuantitativos (los dos
discretos, los dos continuos o uno discreto y otro continuo).\\


Si designamos por $x_1, x_2, \ldots, x_k$ las $k$ modalidades posibles del carácter $X$ y por $y_1, y_2, \ldots, y_p$
las $p$ modalidades posibles del carácter $Y$, las observaciones correspondientes a cada individuo serán de la forma
$(x_i, y_j)$, par ordenado que representa las modalidades tomadas por dicho individuo en los caracteres $X$ e $Y$.

\begin{itemize}
    \item $n_{ij}$: Número total de individuos en la población que presentan simultáneamente la modalidad $x_i$ del carácter $X$
          y la modalidad $y_j$ del carácter $Y$. Le llamamos frecuencia absoluta del par $(x_i, y_j)$.
    \item $f_{ij}$: Proporción de individuos en la población que presentan simultáneamente la modalidad $x_i$ del
          carácter $X$ y la modalidad $y_j$ del carácter $Y$. Le llamamos frecuencia relativa del par $(x_i, y_j)$.
          Por la definición de proporción sobre el total, tenemos que:
          $$f_{ij} = \dfrac{n_{ij}}{n} \qquad i \in \{1, 2, \ldots, k\} ~ j \in \{1, 2, \ldots, p\}$$
\end{itemize}

Gracias al principio de incompatibilidad y exhaustividad de las modalidades, tenemos que:
$$\sum_{i=1}^k\sum_{j=1}^p n_{ij} = n ~~ \sum_{i=1}^k\sum_{j=1}^p f_{ij}=1$$

La distribución $\left\{ (x_i,y_j), n_{ij}\right\}_{\substack{i=1,\dots,k\\j=1,\dots,p}}$ recibe el nombre de distribución conjunta de los caracteres $X$ e $Y$.

\begin{itemize}
    \item $n_{i.}$: Número total de individuos que presentan la modalidad $x_i$ del carácter $X$ sin tener en cuenta
          las modalidades que puedan tomar para el carácter $Y$:
          $$n_{i.} = \sum_{j=1}^p n_{ij} \qquad i\in \{1, \ldots, k\}$$
    \item $f_{i.}$: Proporción total de individuos que presentan la modalidad $x_i$ del carácter $X$ sin tener
          en cuenta el carácter $Y$:
          $$f_{i.} = \sum_{j=1}^p f_{ij} = \dfrac{n_{i.}}{n} \qquad i \in \{1, \ldots, k\}$$
    \item $n_{.j}$: Número total de individuos que presentan la modalidad $y_j$ del carácter $Y$ sin tener en cuenta
          las modalidades que puedan tomar para el carácter $X$:
          $$n_{.j} = \sum_{i=1}^k n_{ij} \qquad j\in \{1, \ldots, p\}$$
    \item $f_{.j}$: Proporción total de individuos que presentan la modalidad $y_j$ del carácter $Y$ sin tener
          en cuenta el carácter $X$:
          $$f_{.j} = \sum_{i=1}^k f_{ij} = \dfrac{n_{.j}}{n} \qquad j \in \{1, \ldots, p\}$$
\end{itemize}

Se tiene que:
$$\sum_{i=1}^k n_{i.} = \sum_{j=1}^p n_{.j} = n \hspace{2cm} \sum_{i=1}^k f_{i.} = \sum_{j=1}^p f_{.j} = 1$$

\section{Tablas estadísticas bidimensionales}

Para agrupar nuestros datos estadísticos, usaremos una tabla de doble entrada como la siguiente:

\begin{center}
    \begin{tabular}{c|c|c|c|c|c|c|c}
        $X \backslash Y$ & $y_1$    & $y_2$    & $\ldots$ & $y_j$    & $\ldots$ & $y_p$    & $n_{i.}$ \\
        \hline
        $x_1$            & $n_{11}$ & $n_{12}$ & $\ldots$ & $n_{1j}$ & $\ldots$ & $n_{1p}$ & $n_{1.}$ \\
        \hline
        $x_2$            & $n_{21}$ & $n_{22}$ & $\ldots$ & $n_{2j}$ & $\ldots$ & $n_{2p}$ & $n_{2.}$ \\
        \hline
        $\vdots$         & $\vdots$ & $\vdots$ & $\ldots$ & $\vdots$ & $\ldots$ & $\vdots$ & $\vdots$ \\
        \hline
        $x_i$            & $n_{i1}$ & $n_{i1}$ & $\ldots$ & $n_{ij}$ & $\ldots$ & $n_{ip}$ & $n_{i.}$ \\
        \hline
        $\vdots$         & $\vdots$ & $\vdots$ & $\ldots$ & $\vdots$ & $\ldots$ & $\vdots$ & $\vdots$ \\
        \hline
        $x_k$            & $n_{k1}$ & $n_{k2}$ & $\ldots$ & $n_{kj}$ & $\ldots$ & $n_{kp}$ & $n_{k.}$ \\
        \hline
        $n_{.j}$         & $n_{\text{.}1}$ & $n_{\text{.}2}$ & $\ldots$ & $n_{.j}$ & $\ldots$ & $n_{.p}$ & $n$
    \end{tabular}
\end{center}

En el caso de que uno de los carácteres sea cualitativo, esta tabla recibirá el nombre de tabla de contingencia.

\section{Representaciones gráficas}
\subsection{Diagrama de dispersión o nube de puntos}

Consiste en representar cada par de observaciones $(x_i, y_j)$ por un punto en un plano bidimensional.
Para representar la frecuecia absoluta de cada punto, se suele incluir un número pequeño al lado de cada punto.
En caso de representar variables cuantitativas continuas, usaremos las marcas de clase.

\subsection{Estereogramas}

Los estereogramas son gráficos tridimensionales formados por barras colocadas en cada punto $(x_i, y_j)$ con altura
$n_{ij}$.

En el caso de las variables cuantitativas continuas, las barras pasarán a ser prismas, donde la base del prisma
tiene dimensiones $e_i - e_{i-1} x e_j - e_{j-1}$. La altura de cada prisma vendrá dada por la fórmula:
$$h_{ij} = \dfrac{n_{ij}}{(L_i - L_{i-1})(L_j - L_{j-1})}$$
De tal forma que el volumen de cada prisma es igual a la frecuencia absoluta de cada pareja de intervalos de clase.

\section{Distribuciones marginales}

Las modalidades del carácter $X$ junto con las frecuencias $n_{i.}$ forman la distribución marginal del carácter $X$:
$$\{x_i, n_{i.}\}_{i=1,\ldots,k}$$

Mientras que las modalidades del carácter $Y$ junto con las frecuencias $n_{.j}$
forman la distribución marginal del carácter $Y$: $$\{y_j, n_{.j}\}_{j=1,\ldots,p}$$

Ambas son distribuciones unidimensionales, a las que es posible dar el tratamiento visto en el tema anterior.

Como ejemplo, la distribución marginal del carácter $X$ es la siguiente:
\begin{center}
    \begin{tabular}{c|c|c}
        $X$      & $n_{i.}$ & $f_{i.}$ \\
        \hline
        $x_1$    & $n_{1.}$ & $f_{1.}$ \\
        \hline
        $x_2$    & $n_{2.}$ & $f_{2.}$ \\
        \hline
        $\vdots$ & $\vdots$ & $\vdots$ \\
        \hline
        $x_i$    & $n_{i.}$ & $f_{i.}$ \\
        \hline
        $\vdots$ & $\vdots$ & $\vdots$ \\
        \hline
        $x_k$    & $n_{k.}$ & $f_{k.}$ \\
        \hline
                 & $n$        & $1$
    \end{tabular}
\end{center}

\section{Distribuciones condicionadas}

En ocasiones es interesante el estudio de un carácter sólo sobre los individuos que presentan una modalidad (o varias)
del otro carácter. por ejemplo, podría ser interesante el estudio del carácter $X$ en la subpoblación formada por
los individuos que presentan la modalidad $y_j$ del carácter $Y$, una subpoblación de $n_{.j}$ individuos.\\


Decimos que la frecuencia relativa de la modalidad $x_i$ del carácter $X$ en aquellos individuos que presentan la
modalidad $y_j$ del carácter $Y$ es:
$$f_{i/j} \equiv f_i^j = \dfrac{n_{ij}}{n_{.j}} \qquad i \in \{1, \ldots, k\}$$


Análogamente, podemos considerar la frecuencia relativa de la modalidad $y_j$ del carácter $Y$ en aquellos individuos
que presentan la modalidad $x_i$ del carácter $X$:
$$f_{j/i} \equiv f_j^i = \dfrac{n_{ij}}{n_{i.}} \qquad j \in \{1, \ldots, p\}$$

De esta forma, existen $p$ distribuciones condicionadas para el carácter $X$ según una única modalidad del carácter
$Y$ y $k$ distribuciones condicionadas para el carácter $Y$ según una única modalidad del carácter $X$.\\


Ejemplo de la distribución condicionada del carácter $X$ respecto a la modalidad $y_j$ del carácter $Y$, que
denotaremos por $X/Y=y_j$:

\begin{center}
    \begin{tabular}{c|c|c}
        $X$      & $n_{ij}$ & $f_i^j$  \\
        \hline
        $x_1$    & $n_{1j}$ & $f_1^j$  \\
        \hline
        $x_2$    & $n_{2j}$ & $f_2^j$  \\
        \hline
        $\vdots$ & $\vdots$ & $\vdots$ \\
        \hline
        $x_i$    & $n_{ij}$ & $f_i^j$  \\
        \hline
        $\vdots$ & $\vdots$ & $\vdots$ \\
        \hline
        $x_k$    & $n_{kj}$ & $f_k^j$  \\
        \hline
                 & $n_{.j}$ & 1
    \end{tabular}
\end{center}

De las definiciones anteriores, tenemos que:
$$f_{ij} = \dfrac{n_{ij}}{n} = \dfrac{n_{i.}}{n}\dfrac{n_{ij}}{n_{i.}} = \dfrac{n_{.j}}{n}\dfrac{n_{ij}}{n_{.j}} $$
$$ f_{ij} = f_{i.} f_j^i = f_{.j}f_i^j $$

\section{Dependencia e Independencia estadística}

Dos caracteres $X$ e $Y$ serán \underline{estadísticamente dependientes} cuando la variación en uno de ellos influya en la distribución del otro.\\


Por otra parte, se dice que el carácter $X$ es \underline{estadísticamente independiente} del carácter $Y$ si las distribuciones
de $X$ condicionadas a cada valor $y_j$ de $Y$ ($X/Y=y_j$) son idénticas para cualquier valor de $j$. En este caso,
cada distribución condicionada es idéntica a la distribución marginal de $X$:
$$\dfrac{n_{i1}}{n_{\text{.}1}} = \dfrac{n_{i2}}{n_{\text{.}2}} = \ldots = \dfrac{n_{ij}}{n_{.j}} = \ldots = \dfrac{n_{ip}}{n_{.p}} \qquad \forall i = 1, \ldots, k$$

De donde tenemos que:
$$f_{i/j} \equiv f_i^j = \dfrac{n_{ij}}{n_{.j}} = \dfrac{n_{i1} + n_{i2} + \ldots + n_{ip}}{n_{.1} + n_{.2} + \ldots + n_{.p}}
    = \dfrac{n_{i.}}{n} = f_{i.}$$

Por lo que si $f_i^j = f_{i.} \ \forall i \in \{1, \ldots, k\}$, entonces el carácter $X$ será independiente del
carácter $Y$. (Análogamente, se define la independencia del carácter $Y$ con el carácter $X$).

\begin{prop}
Si $X$ es una variable independiente de $Y$ $\Longrightarrow f_{i\cdot} = f_{i/j}$
\end{prop}
\begin{proof}
Suponemos $X$ e $Y$ variables independientes.
$$f_{i\cdot} = \sum_{j=1}^p f_{ij} = \sum_{j=1}^p f_{i/j}f_{\cdot j} \stackrel{(\ast)}{=} f_{i/j}\sum_{j=1}^pf_{\cdot j} = f_{i/j} f_{\cdot \cdot} = f_{i/j}$$

donde en $(\ast)$ he usado que las variables son independientes, por lo que la frecuencia condicionada a $Y$ no depende de $j$.
\end{proof}

\begin{teo}[Teorema de Caracterización de la Independencia]
Sean $X$ e $Y$ dos variables estadísticas.

\centering
$X$ e $Y$ son independientes $\Longleftrightarrow$ $f_{ij} = f_{i\cdot} f_{\cdot j} \Longleftrightarrow n_{ij} = \frac{n_{i\cdot} n_{\cdot j}}{n} \quad \forall i,j$
\end{teo}
\begin{proof} Demostramos mediante la doble implicación:
    \begin{description}
    \item [$\Longrightarrow$)] Suponemos $X$ e $Y$ independientes, es decir, $f_{i/j} = f_{i \cdot}$

    Por tanto,
    $$f_{ij} = f_{i/j}f_{\cdot j} = f_{i \cdot }f_{\cdot j}$$
    
    \item [$\Longleftarrow$)] Suponemos $f_{ij} = f_{i\cdot} f_{\cdot j}$.

    Probemos que $X$ e $Y$ son linealmente independientes.
    $$f_{\cdot j} = \frac{f_{ij}}{f_{i \cdot}} = f_{j/i}$$
    \end{description}
\end{proof}

\begin{prop}
    Si el carácter $X$ es independiente del carácter $Y$, entonces $Y$ es independiente de $X$ (la independencia es una propiedad recíproca).
\end{prop}
\begin{proof}
    Supuesto $X$ independiente de $Y$, tenemos $f_{i\cdot} = f_{i/j} \quad \forall j=1,\dots,p$
    \begin{equation*}
        f_{ij} = f_{i\cdot} f_{\cdot j} = f_{i\cdot}f_{j/i} \Longrightarrow f_{\cdot j} = f_{j/i}
    \end{equation*}
    demostrando así que $X$ es independiente de $Y$.
\end{proof}


Se dice que el carácter $X$ \underline{depende funcionalmente} del carácter $Y$ si a cada modalidad de $y_j$ de $Y$
le corresponde una única modalidad posible de $X$ con frecuencia no nula. Es decir:
$$\forall j \in \{1, \ldots, p\}, n_{ij} = 0 \text{ excepto para un valor } i = \varphi(j) \mid n_{ij} = n_{.j}$$

\begin{ejemplo} Consideramos la siguiente tabla estadística bidimensional:
    \begin{center}
    \begin{tabular}{c|c|c|c|c|c|c}
        $X \backslash Y$ & $y_1$ & $y_2$ & $y_3$ & $y_4$ & $y_5$ &    \\
        \hline
        $x_1$            & 3     & 0     & 6     & 0     & 0     & 9  \\
        \hline
        $x_2$            & 0     & 4     & 0     & 0     & 2     & 6  \\
        \hline
        $x_3$            & 0     & 0     & 0     & 5     & 0     & 5  \\
        \hline
                         & 3     & 4     & 6     & 5     & 2     & 20 \\
    \end{tabular}
    \end{center}
    
    
    En dicha distribución conjunta, $X$ depende funcionalmente del carácter $Y$. Sin embargo, $Y$ no depende
    funcionalmente del carácter $X$.
\end{ejemplo}

Si sucede que la dependencia funcional es bidireccional, hablaremos de una \underline{dependencia funcional recíproca}.
Notemos que esta es de poco interés estadístico.

\section{Momentos bidimensionales}

Dada una variable estadística bidimensional ($X,Y$) con una distribución conjunta
$\left\{ (x_i,y_j), n_{ij}\right\}_{\substack{i=1,\dots,k\\j=1,\dots,p}}$, se definen los momentos conjunto central y no central de órdenes $r$ y $s$
($r,s \in \N \cup \{0\}$) como:
$$\mu_{rs} = \sum_{i=1}^k \sum_{j=1}^p f_{ij} (x_i - \overline{x})^r (y_j - \overline{y})^s$$
$$m_{rs}=\sum_{i=1}^k \sum_{j=1}^p f_{ij} x_i^r y_j^s$$


Los momentos centrales más utilizados son las varianzas marginales, $\mu_{20} = \sigma_x^2$ y $\mu_{02}=\sigma_y^2$
y el momento $\mu_{11}$, cuya importancia se describe a continuación:

\subsection{Varianza}
\begin{definicion} Dadas dos variables estadísticas unidimensionales, $X$ y $Y$, se define la covarianza de las variables $X$ e $Y$ como:
\begin{equation*}
    \sigma_{xy} = Cov(X,Y) = \mu_{11}
\end{equation*}
\end{definicion}

\begin{prop} Dadas dos variables estadísticas unidimensionales, $X$ y $Y$, se tiene:
    $$\sigma_{xy} = \mu_{11} = m_{11} - m_{10}m_{01}$$
\end{prop}
\begin{proof}
    \begin{equation*}\begin{split}
    \sigma_{xy} &=  Cov(X,Y) = \mu_{11} = \sum_{i=1}^k \sum_{j=1}^p f_{ij}(x_i - \bar{x})(y_j - \bar{y}) = \sum_{i=1}^k \sum_{j=1}^p f_{ij}(x_iy_j - x_i\bar{y} - \bar{x}y_j + \bar{x}\bar{y})
    =\\&=
    \sum_{i=1}^k \sum_{j=1}^px_iy_jf_{ij} - \sum_{i=1}^k \sum_{j=1}^p x_i\bar{y}f_{ij} - \sum_{i=1}^k \sum_{j=1}^p\bar{x}y_jf_{ij} + \sum_{i=1}^k \sum_{j=1}^p \bar{x}\bar{y} f_{ij}
    =\\&=
    \sum_{i=1}^k \sum_{j=1}^px_iy_jf_{ij} - \bar{y}\sum_{i=1}^k x_if_{i\cdot} - \bar{x}\sum_{j=1}^p y_jf_{\cdot j} + \bar{x}\bar{y}\sum_{i=1}^k \sum_{j=1}^pf_{ij}
    =\\&=
    \sum_{i=1}^k \sum_{j=1}^p x_iy_jf_{ij} - \bar{y}\bar{x} - \bar{x}\bar{y} + \bar{x}\bar{y}
    =\\&=
    \sum_{i=1}^k \sum_{j=1}^px_iy_jf_{ij} - \bar{x}\bar{y} = m_{11} - m_{10}m_{01}
\end{split}\end{equation*}
\end{proof}


\begin{prop}
    Si $X$ e $Y$ son independientes $\Longrightarrow \left\{ \begin{array}{c}
        m_{rs}=m_{r0}m_{0s}  \\
        \mu_{rs}=\mu_{r0}\mu_{0s} 
    \end{array}\right.$
\end{prop}
\begin{proof} Supongo $X$ e $Y$ independientes, por lo que $f_{ij}=f_{i.}f_{.j}$. Entonces:
    \begin{equation*}
        m_{rs} = \sum_{i=1}^k \sum_{j=1}^p f_{ij}x_i^r y_j^s
        = \sum_{i=1}^k \sum_{j=1}^p f_{i.}f_{.j}x_i^r y_j^s
        = \sum_{i=1}^k f_{i.}x_i^r \sum_{j=1}^p f_{.j} y_j^s
        = m_{r0}m_{0s}
    \end{equation*}
    \begin{multline*}
        \mu_{rs} = \sum_{i=1}^k \sum_{j=1}^p f_{ij}(x_i-\bar{x})^r (y_j-\bar{y})^s
        = \sum_{i=1}^k \sum_{j=1}^p f_{i.}f_{.j}(x_i-\bar{x})^r (y_j-\bar{y})^s
        =\\=
        \sum_{i=1}^k f_{i.}(x_i-\bar{x})^r \sum_{j=1}^p f_{.j} (y_j-\bar{y})^s
        = \mu_{r0}\mu_{0s}
    \end{multline*}
\end{proof}

\begin{coro}
    Si $X$ e $Y$ son independientes $\Longrightarrow \sigma_{xy} = 0$
\end{coro}
\begin{proof} Supongo $X$ e $Y$ independientes, por lo que $m_{rs}=m_{r0}m_{0s}$. Entonces:
    \begin{equation*}
        \sigma_{xy} = m_{11} - m_{10}m_{01} = m_{10}m_{01} - m_{10}m_{01} = 0
    \end{equation*}
\end{proof}

\begin{prop}
    Si se transforman los valores de $x_i$ e $y_j$ mediante transformaciones lineales dadas por:
    \begin{equation*}
        \left\{\begin{array}{c}
            x_i' = ax_i + b  \\
            y_j' = cy_j + d 
        \end{array} \right.
    \end{equation*}
    La covarianza queda como:
    $$\sigma_{x'y'} = ac \sigma_{xy}$$
\end{prop}
\begin{proof}
    \begin{multline*}
        \sigma_{x'y'} = \sum_{i=1}^k \sum_{j=1}^p f_{ij} (x_i' - \bar{x}')(y_j'-\bar{y}')
        = \sum_{i=1}^k \sum_{j=1}^p f_{ij} [(ax_i+b) - (a\bar{x}+b)][(cy_j+d)-(c\bar{y}+d)]
        =\\=
        ac \sum_{i=1}^k \sum_{j=1}^p (x_i - \bar{x})(y_j- \bar{y})f_{ij} = ac \sigma_{xy}
    \end{multline*}
\end{proof}


Si expresamos nuevas variables a partir de otras, podemos calcular su covarianza a partir de la otra:
$$x_i' = ax_i+b~~y_j'=cy_j+d$$
$$\sigma_{X'Y'}=\sum_{i=1}^k \sum_{j=1}^p f_{ij} (ax_i+b-\overline{x'})(cy_j+d-\overline{y'}) = $$
$$=\sum_{i=1}^k \sum_{j=1}^p f_{ij}(ax_i+b-(a\overline{x}+b))(cy_j+d-(c\overline{y}+d))=$$
$$=\sum_{i=1}^k \sum_{j=1}^p f_{ij}(ax_i-a\overline{x})(cy_j-c\overline{y}) =
    ac \sum_{i=1}^k \sum_{j=1}^p f_{ij}(x_i-\overline{x})(y_j-\overline{y}) = ac\sigma_{xy}$$

\section{Regresión}

Pretendemos buscar que un número de magnitudes $X_1, \ldots, X_n$ se relacionen con una variable $Y$ mediante la expresión:
$$Y = f(X_1, \ldots, X_n)$$

Podemos abordar el problema desde dos enfoques:
\begin{itemize}
    \item Regresión: La determinación de la estructura de dependencia que mejor expresa la relación de la variable $Y$
          con las demás.
    \item Correlación: El estudio del grado de dependencia que existe entre las variables.
\end{itemize}


Si dos variables presentan una dependencia estadística (es decir, una dependencia no funcional), no es posible encontrar una ecuación tal que los valores que puedan presentar dichas variables la satisfagan. Es decir, no es posible encontrar una función que pase por todos los puntos del diagrama de dispersión que representa esa distribución conjunta. Por tato, tendremos que ajustar lo mejor posible una función a una serie de valores observados, encontrando una curva
que, aunque no pase por todos los puntos de la nube, más se aproxime a ellos. Dicho método recibe el nombre de \underline{ajuste por mínimos cuadrados}.

\subsection{Método de mínimos cuadrados}
Sea $f(x_i, a_0,\dots,a_n)$ la función que aproxima la variable $Y$ en función de los valores de $X$.
\begin{notacion}
    A los valores ajustados se les notará de la siguiente manera:
    \begin{equation*}
        \hat{y_j} = f(x_i;a_0,\dots,a_n)
    \end{equation*}
\end{notacion}

\begin{definicion}[Residuo]
Se define el residuo de la modalidad $y_j$ de la variable $Y$ como:
\begin{equation*}
    e_{ij} = y_j - \hat{y_j}
    = y_j - f(x_i, a_0, a_1, \ldots, a_n)
\end{equation*}
\end{definicion}



El método de mínimos cuadrados consiste en encontrar una función $f$ que minimice la media de los cuadrados de los
residuos:
\begin{equation*}
        ECM(f(x_i, a_0, a_1, \ldots, a_n)) = \psi(a_0, a_1, \ldots, a_n)
        = \sum_{i=1}^k \sum_{j=1}^p f_{ij} e_{ij}^2
\end{equation*}


La función $\psi$ se denomina el \underline{error cuadrático medio de la función} $f$, denotada $ECM(f(x_i, a_0, a_1, \ldots, a_n))$.
Como los parámetros $(x_i, a_0, a_1, \ldots, a_n)$ sólo están sometidos a sumas, productos y cuadrados dentro de $\psi$,
dicha función es derivable respecto a cada $a_i \ \forall i \in \{0, \ldots, n\}$. Además, se puede asegurar que
el punto $(\hat{a_0}, \hat{a_1}, \ldots, \hat{a_n})$ donde se anulan las derivadas parciales primeras respecto
de cada $a_i$ corresponde a un mínimo de la función $\psi$.


El cálculo de los parámetros de la función de ajuste óptima según el método de los mínimos cuadrados consiste en
resolver el siguiente sistema, llamado \underline{sistema de ecuaciones normales}:
$$\dfrac{\partial \psi}{\partial a_r}=0 \Rightarrow \sum_{i=1}^k \sum_{j=1}^p f_{ij} e_{ij}
    \dfrac{\partial f}{\partial a_r} = 0 \qquad \forall r \in \{0, \ldots n\}$$


Una de las funciones de regresión más utilizadas para expresar el comportamiento de una variable en función de la otra
es un polinomio de grado $n$ (comenzaremos con $n=1$).

\subsubsection{Ajuste lineal (recta de regresión)}\vspace{-0.5cm}
\begin{equation*}
    Y=f(X;a,b) = a + bX
\end{equation*}

Supongamos que queremos ajustar por el método de mínimos cuadrados una recta que exprese $Y$ en función de $X$.
La función sería $Y=f(X;a,b) = a + bX$, por lo que tendremos que calcular el mínimo en $a$ y $b$ de la función:
$$\psi(a,b) = ECM(a,b) = \sum_{i=1}^k \sum_{j=1}^p f_{ij} [y_j - (a + bx_i)]^2 $$

Obtenemos el sistema de ecuaciones normales:
\begin{equation*}
    \left\{
    \begin{array}{l}
        \dfrac{\partial \psi}{\partial a} = 0 \Rightarrow \displaystyle\sum_{i=1}^k \sum_{j=1}^p f_{ij} [y_j - (a+bx_i)]=0\\ \\
        \dfrac{\partial \psi}{\partial b} = 0 \Rightarrow \displaystyle\sum_{i=1}^k \sum_{j=1}^p f_{ij} [y_j - (a+bx_i)]x_i=0
    \end{array}
    \right\} \Longrightarrow
    \left\{
    \begin{array}{l}
        m_{01} = a + b m_{10}\\ \\
        m_{11} = a m_{10} + bm_{20}
    \end{array}
    \right.
\end{equation*}

La resolución de dicho sistema nos proporciona los coeficientes buscados:
$$\hat a = m_{01} - \dfrac{m_{11} - m_{10}m_{01}}{m_{02}-m_{01}^2}m_{10} = \overline{y} - \dfrac{\sigma_{xy}}{\sigma_x^2}\overline{x}$$
$$\hat b = \dfrac{m_{11} - m_{10}m_{01}}{m_{02}- m_{01}^2} = \dfrac{\sigma_{xy}}{\sigma_x^2}$$

Por tanto, \textbf{la recta de regresión de $Y$ sobre $X$} tiene por expresión:
\begin{equation*}
    Y = \dfrac{\sigma_{xy}}{\sigma_x^2}X + \overline{y} - \dfrac{\sigma_{xy}}{\sigma_x^2}\overline{x}
    \hspace{1cm}
    \left(\text{Equivalentemente}, Y - \overline{y} = \dfrac{\sigma_{xy}}{\sigma_x^2} (X - \overline{x})\right)
\end{equation*}


\begin{definicion}[Coeficiente de regresión lineal]
    Al coeficiente $\dfrac{\sigma_{xy}}{\sigma_x^2}$ se le denomina coeficiente de regresión lineal de $Y$ sobre $X$.

    Análogamente, se define el coeficiente de regresión lineal de $X$ sobre $Y$.
\end{definicion}


Análogamente, la recta de regresión mínimo cuadrática de $X$ sobre $Y$ es la recta $X = h(Y; c, d) = c+dY$ que minimiza
la función
$$\phi(c,d) = \sum_{i=1}^k \sum_{j=1}^p f_{ij} (x_i - \hat x_j )^2$$

donde $\hat x_j = c+dy_j$. Siguiente el procedimiento anterior, llegamos a que \textbf{la recta de regresión de $X$ sobre $Y$} es:
$$X - \overline{x} = \dfrac{\sigma_{xy}}{\sigma_y^2} (Y - \overline{y})$$


Los coeficientes de regresión son las pendientes de las rectas de regresión. Los signos de dichos coeficientes son
los mismos para ambas rectas e igual al signo de la covarianza. Cuando exista dependencia funcional lineal, las dos
rectas de regresión coincidirán con la recta de dependencia.\\

Algunas propiedades de la recta de regresión son:
\begin{lema}
    Las rectas de regresión pasan por el punto $(\bar{x}, \bar{y})$.
\end{lema}
\begin{proof}
    La recta de regresión de $Y$ sobre $X$ tiene la forma de $y-\bar{y} = K(x-\bar{x})$. Para $x=\bar{x}$, vemos que $y=\bar{y}$.

    Análogamente, la recta de regresión de $X$ sobre $Y$ tiene la forma de $x-\bar{x} = K(y-\bar{y})$. Para $y=\bar{y}$, vemos que $x=\bar{x}$.
\end{proof}

\begin{lema}\label{lema:2.8}
    La media de los valores ajustados coincide con la de los valores observados de la variable.
\end{lema}
\begin{proof}
    \begin{equation*}
        \overline{\hat{y}} = \sum_{i=1}^k \sum_{j=1}^p f_{ij}\hat{y_j}
        = \sum_{i=1}^k \sum_{j=1}^p f_{ij} (ax_i + b)
        = a\bar{x} + b = \bar{y}
    \end{equation*}
\end{proof}

\begin{coro}
    La media de los residuos vale 0.
\end{coro}
\begin{proof}
    \begin{equation*}
        \sum_{i=1}^k \sum_{j=1}^p f_{ij}e_{ij}
        = \sum_{i=1}^k \sum_{j=1}^p f_{ij} (y_j - \hat{y_j})
        =  \bar{y} - \overline{\hat{y}} = 0
    \end{equation*}
\end{proof}

\begin{coro}
    La media de los productos de los residuos por los valores de la variable explicativa vale cero.
\end{coro}
\begin{proof}
    \begin{equation*}
        \sum_{i=1}^k \sum_{j=1}^p f_{ij} e_{ij}x_i = \bar{x} \sum_{i=1}^k \sum_{j=1}^p f_{ij}e_{ij} = 0
    \end{equation*}
\end{proof}

\begin{coro}
    La media de los productos de los residuos por los valores ajustados vale cero.
\end{coro}
\begin{proof}
    \begin{equation*}
        \sum_{i=1}^k \sum_{j=1}^p f_{ij} e_{ij}\hat{y_j} = \bar{\hat{y}} \sum_{i=1}^k \sum_{j=1}^p f_{ij}e_{ij} = 0
    \end{equation*}
    donde se ha aplicado la Proposición \ref{prop:1.3}.
\end{proof}


\subsubsection{Ajuste polinómico}\vspace{-0.5cm}
\begin{equation*}
    Y=f(X;a_0,a_1,\dots,a_n) = a_0 + a_1X + \dots + a_nX^n
\end{equation*}

Si queremos aproximar mediante un polinomio de grado superior o igual a dos, el método
de mínimos cuadrados nos conducirá al sistema de ecuaciones:
\begin{equation*}
    \left\{
    \begin{array}{cl}
        m_{01} &= a_0 + a_1 m_{10} + \ldots + a_n m_{n0}\\
        m_{11} &= a_0m_{10} + a_1 m_{20} + \ldots + a_n m_{n+1,0}\\
        m_{21} &= a_0m_{20} + a_1 m_{30} + \ldots + a_n m_{n+2,0}\\
        &\vdots\\
        m_{n1} &= a_0 m_{n0} + a_1 m_{n+1,0} + \ldots + a_n m_{2n,0}
    \end{array}
    \right.
\end{equation*}



Para ajustar a la nube otro tipo de función, intentaremos pasar a un ajuste polinómico. Ejemplos de esto son los siguientes ajustes:

\subsubsection{Ajuste hiperbólico}\vspace{-0.5cm}
\begin{equation*}
    Y=f(X;a,b) = a+b\frac{1}{X}
\end{equation*}


Si queremos realizar un ajuste hiperbólico mediante una hipérbola equilátera realizamos la transformación $Z = \dfrac{1}{X}$, y realizamos el ajusta de mínimos cuadrados a la recta $Y = a + bZ$ sobre
las variables $(Z,Y)$.

\subsubsection{Ajuste potencial}\vspace{-0.5cm}
\begin{equation*}
    Y=f(X;a,b) = aX^b
\end{equation*}

De otra forma, si queremos aplicar el ajuste potencial hemos de aplicar el logaritmo y obtenemos la siguiente expresión:
$$\ln Y = \ln a + b \ln X$$

Llamando a las variables $V = \ln Y$, $U = \ln X$, $A = \ln a$, quedándonos la siguiente expresión a calcular el ajuste lineal: $$V= A + bU$$

\subsubsection{Ajuste exponencial}\vspace{-0.5cm}
\begin{equation*}
    Y=f(X;a,b) = ab^{x}
\end{equation*}

De otra forma, si queremos aplicar el ajuste exponencial hemos de aplicar el logaritmo y obtenemos la siguiente expresión:
$$\ln Y = \ln a + X \ln b$$

Llamando a las variables $V = \ln Y$, $A = \ln a$ y $B = \ln b$, quedándonos la siguiente expresión a calcular el ajuste lineal: $$V= A + BX$$

\subsection{Regresión de tipo I}

Podemos además realizar regresiones de una variable dependiente $Y$ dado el valor $x_i$ de una variable independiente
asociada $X$. Es decir, predecir el comportamiento de la variable condicionada $Y/X=x_i$.\\


Teniendo en cuenta la representatividad de le media en lo que al comportamiento de una variable se refiere, se define la curva de regresión de tipo I de $Y/X$ como la curva que pasa por los puntos $(x_i, \overline{y_i}) \
    \forall i \in \{1, \ldots, k\}$. Análogamente, se defien la curva de regresión de tipo I de $X/Y$ como la curva
que pasa por los puntos $(\overline{x_j}, y_j) \ \forall j \in \{1, \ldots, p\}$.

Estas curvas tienen la propiedad de ser entre todas las funciones las que mejor se ajustan a los datos observados
según el método de mínimos cuadrados. Estas curvas no son de gran utilidad práctica, pues el hecho de conocerla
solamente en puntos aislados hace que sea inútil para le predicción en los demás casos.

\section{Correlación}

El grado de asociación entre las variables nos indicará en qué medida la expresión encontrada mediante la regresión explica una variable en función de la otra. El estudio de la correlación también equivale al estudio de la bondad del ajuste de una curva a una nube de puntos.

Para ello, en primer lugar es importante diferenciar entre los ajustes lineales en los parámetros y los no lineales en los parámetros.

Los que sí son lineales en los parámetros son aquellos a los que no se les ha aplicado ninguna transformación a los parámetros. Ejemplo de estos son los ajustes mediante rectas, parábolas o hipérbolas equiláteras.

Los no lineales en los parámetros implican que a alguno de los parámetros se le ha aplicado alguna transformación. Ejemplos son el ajuste potencial o el exponencial.

\subsection{Varianza residual. Coeficiente de determinación}

El método de mínimos cuadrados toma como medida del error que se comente al ajustar una curva la siguiente medida:
\begin{definicion}[Varianza Residual]

    Se define la varianza residual del ajuste de $Y$ en función de $X$ como:
    $$\sigma_{ry}^2 = \sum_{i=1}^k \sum_{j=1}^p f_{ij} e_{ij}^2 = \sum_{i=1}^k \sum_{j=1}^p f_{ij}(y_j - \hat y_i)^2 = \sum_{i=1}^k \sum_{j=1}^p
    f_{ij} [y_j - f(x_i)]^2$$
\end{definicion}

Dicha cantidad se usa como medida de la bondad del ajuste. Por tanto, cuanto menor sea la varianza resiudal, mejor será el ajuste.
\begin{observacion}
    En funciones lineales de los parámetros la media de los residuos es cero (generalización del lema \ref{lema:2.8}), por lo que la expresión anterior es precisamente la varianza de los residuos, o \underline{varianza residual}.
    
    En funciones no lineales en los
    parámetros, la media de los residuos no es nula, aunque se sigue denominando varianza residual. Por ello, es importante no confundir la varianza residual con la varianza de los residuos en los ajustes no lineales en los parámetros.
\end{observacion}\bigskip


También se define la siguiente medida:
\begin{definicion}[Varianza Explicada]

    Se define la varianza residual de $Y$ como:
    \begin{equation*}
        \sigma_{ey}^2=\sum_{i=1}^k \sum_{j=1}^p f_{ij}
    (\hat y_j - \overline{y})^2
    \end{equation*}
\end{definicion}


Por norma general, se toma como medida del grado de ajuste el coeficiente de determinación.
\begin{definicion}[Coeficiente de determinación]
El coeficiente de determinación, o razón de correlación, es la proporción de la varianza total de la variable $Y$ explicada por la regresión. Esto es, el cociente o razón entre la varianza explicada y la total.
\begin{equation*}
    \eta_{Y/X}^2 = \dfrac{\sigma_{ey}^2}{\sigma_y^2}
\end{equation*}    
\end{definicion}


Por tanto, para comparar todo tipo de ajustes se puede emplear la \textbf{varianza residual} o el \textbf{coeficiente de determinación}, aunque se suele emplear la primera medida.

\subsubsection{Caso concreto de ajustes lineales en los parámetros}

En este caso, como la varianza residual coincide con la varianza de los residuos, se tiene que es posible descomponer la varianza en una suma de la varianza residual y la varianza explicada por la regresión:
$$\sigma_y^2 = \sigma_{ey}^2 + \sigma_{ry}^2$$

En este caso, se tiene que:
$$\eta_{Y/X}^2 := \dfrac{\sigma_{ey}^2}{\sigma_y^2} = 1 - \dfrac{\sigma_{ry}^2}{\sigma_y^2}$$


\subsubsection{Interpretación del coeficiente de correlación}

De la misma expresión se deduce que: $0 \leq \eta_{Y/X}^2 \leq 1$.

\begin{itemize}
    \item $\eta_{Y/X}^2 = 0 \Leftrightarrow \dfrac{\sigma_{ey}^2}{\sigma_y^2}=0 \Leftrightarrow \sigma_{ey}^2 = 0$. Es decir, el modelo no explica nada de $Y$ a partir de $X$. El ajuste es el peor posible que se puede hacer por mínimos cuadrados.
    \item $\eta_{Y/x}^2 = 1 \Leftrightarrow \dfrac{\sigma_{ey}^2}{\sigma_y^2}=1$. Es decir, todos los residuos son nulos y s explica la variable totalmente. El ajuste es perfecto.
    \item Para valores intermedios entre 0 y 1, según estén más próximos a un extremo o a otro nos indicarán un peor o mejor ajuste: Un ajute del 60\% explica que el 60\% de la variabilidad total de $Y$ la explica el modelo propuesto mediante la variable independiente.
\end{itemize}

\subsection{Correlación en el caso lineal}

En este caso, tenemos el siguiente resultado, muy útil para calcular la bondad de los ajustes lineales:
\begin{teo} En el caso de un ajuste lineal, el ajuste de determinación viene dado por:
    \begin{equation*}
        \eta_{Y/X}^2 = \eta_{X/Y}^2 = r^2 = \dfrac{\sigma_{xy}^2}{\sigma_x^2 \sigma_y^2}
    \end{equation*}
\end{teo}
\begin{proof}
    Demostramos para la recta de $Y$ sobre $X$, ya que en el otro caso sería análogo. La recta mencionada tiene por expresión:
    \begin{equation*}
        Y-\overline{y} = \dfrac{\sigma_{xy}}{\sigma_x^2}(X - \overline{x}) 
        \Longrightarrow
        Y = \overline{y} + \dfrac{\sigma_{xy}}{\sigma_x^2}(X - \overline{x})
    \end{equation*}

    Por tanto, la varianza residual en el caso de la recta de regresión es:
    \begin{equation*}\begin{split}
        \sigma_{ry}^2 &
        = \sum_{i=1}^k\sum_{j=1}^p f_{ij}[y_j - f(x_i)]^2
        = \sum_{i=1}^k\sum_{j=1}^p f_{ij}\left[y_j - \left(\overline{y} + \dfrac{\sigma_{xy}}{\sigma_x^2}(x_i - \overline{x})\right)\right]^2 = \\
        & = \sum_{i=1}^k\sum_{j=1}^p f_{ij}\left[(y_j - \overline{y}) - \left(\dfrac{\sigma_{xy}}{\sigma_x^2}(x_i - \overline{x})\right)\right]^2 = \\
        & = \sum_{i=1}^k\sum_{j=1}^p f_{ij}\left[(y_j - \overline{y})^2 + \dfrac{\sigma_{xy}^2}{(\sigma_x^2)^2}(x_i - \overline{x})^2 - 2\frac{\sigma_{xy}}{\sigma_x^2}(y_j-\overline{y})(x_i-\overline{x})\right] = \\
        &= \sigma_y^2 + \frac{\sigma_{xy}^2}{\sigma_x} - 2\frac{\sigma_{xy}^2}{\sigma_x^2}
        = \sigma_y^2 - \frac{\sigma_{xy}^2}{\sigma_x^2}
    \end{split}\end{equation*}

    Por tanto, como en este caso estamos ante un ajuste lineal en los parámetros, tenemos que:
    \begin{equation*}
        \eta_{Y/X}^2 = 1-\frac{\sigma_{ry}^2}{\sigma_y^2}
        = 1-\frac{\sigma_y^2 - \frac{\sigma_{xy}^2}{\sigma_x^2}}{\sigma_y^2}
        = 1-1+\frac{\sigma_{xy}^2}{\sigma_x^2\sigma_y^2} = \frac{\sigma_{xy}^2}{\sigma_x^2\sigma_y^2}
        \qedhere
    \end{equation*}
\end{proof}

Este resultado es de gran ayuda, ya que nos permite calcular $\eta_{Y/X}^2$ de una forma mucho más cómoda.

Es útil calcular la varianza residual en función de $r^2$, ya que el cálculo del segundo es mucho más sencillo. No obstante, es necesario a veces conocer la varianza residual para comparar con modelos no lineales en los parámetros. Por eso, se tiene que:
\begin{equation*}
    \eta_{Y/X}^2 = 1 - \dfrac{\sigma_{ry}^2}{\sigma_y^2}
    = r^2 \Longrightarrow \sigma_{ry}^2 = (1-r^2)\sigma_y^2
\end{equation*}


Por último, se introduce un nuevo coeficiente. La raíz cuadrada del coeficiente de determinación lineal anterior (con el signo de la covarianza) recibe el nombre
de \underline{coeficiente de correlación lineal}:
$$r = \pm \sqrt{r^2} = \dfrac{\sigma_{xy}}{\sigma_x \sigma_y}$$


Dicho coeficiente se usa para determinar el grado de dependencia lineal de la variable dependiente ante los valores
de la variable independiente. Esta dependencia puede ser directa (o positiva) o indirecta (o negativa), según
el signo de la covarianza. Adopta valroes entre $\pm1$ y 0.

\begin{itemize}
    \item \underline{Para la covarianza positiva}:

    Si $r=1$, existirá una dependencia lineal funcional, mientras que si $r=0$ no existirá ninguna dependencia o asociación entre las variables de tipo lineal, aunque sí puede haberla de otra naturaleza, convirtiéndose las rectas de regresión paralelas a los ejes de coordenadas.

    \item \underline{Para la covarianza negativa}:
    Si $r=-1$, existirá una correlación perfecta, con una dependencia funcional lineal, coincidiendo las dos rectas en una sola.
\end{itemize}

Para resumir, diremos que $-1 \leq r \leq 1$. Cuando varía de $-1$ a $0$ estamos en una correlación negativa y la dependencia
será mayor cuanto más se aproxime a $-1$ mientras que si varía de $0$ a $1$, la correlación es positiva y el grado de
dependencia será mayor cuanto más se aproxime a $1$.

\section{Predicciones}
Uno de los objetivos principales de la regresión y correlación es hacer predicciones de la variable dependiente en función de los valores que toma la variable independiente. Las predicciones se efectúan utilizando la función estimada por el método de mínimos cuadrados, $f$. Con la que obtenemos los valores teóricos que ajustan a los observados. La predicción será más fiable cuanto mayor sean los coeficientes de determinación correspondientes o razones de correlación, ya que menor será la varianza de los residuos, que nos indica la cuantía de la separación entre lo observado y estimado.

Hay que tener presente que la fiabilidad de las predicciones disminuye a medida que los valores de la variable independiente se aleja de su recorrido, pues puede que el modelo ajustado no sea válido para dicho valores en la medida dada por $\eta^2$