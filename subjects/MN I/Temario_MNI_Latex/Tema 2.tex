\chapter{Sistemas de Ecuaciones Lineales}
\begin{itemize}
    \item \textbf{Métodos directos:} Llegan a la solución en un número finito de operaciones.
    \item \textbf{Métodos iterativos:} Definen una sucesión que converge a la solución.
\end{itemize}

\section{Métodos directos}
\noindent
Notemos que los sistemas de ecuaciones lineales (SELs) más fáciles de resolver son:
\begin{itemize}
    \item \textbf{Sistemas diagonales:} Se obtiene la solución fácilmente despejando cada variable:
          $$Dx=b~~~~x_i = \dfrac{b_i}{d_{ii}}$$
    \item \textbf{Sistemas triangulares:} Que se resuelven por sustitución progresiva o regresiva:
          \begin{itemize}
              \item Sistemas triangulares superiores: $Ux=b$ (Si $d_{ii}\neq 0$)
                    $$x_n=\dfrac{b_n}{u_{nn}}~~x_i=\dfrac{1}{u_{ii}}\left(b_i - \sum_{j=i+1}^n u_{ij}x_j\right)~~i\in\{n-1, \ldots, 1\}$$
              \item Sistemas triangulares inferiores: $Lx=b$ (Si $u_{ii}\neq 0$)
                    $$x_1=\dfrac{b_1}{l_{11}}~~x_i=\dfrac{1}{l_{ii}}\left(b_i - \sum_{j=1}^{i-1}l_{ij}x_j\right)~~i\in\{2, \ldots, n\}$$
          \end{itemize}
\end{itemize}

\noindent
Diremos que dos sistemas $Ax=b, Cx=d$ son \underline{equivalentes} si $\exists \ T \in M_n(\bb{K})$ regular tal que $TA=C$ y $Tb=d$.

\subsection{Método de Gauss}
\noindent
Dado un sistema $Ax=b$, lo transformaremos en otro triangular superior equivalente, $Ux=c$, que resolveremos de la forma vista:

Si $a_{11}\neq 0 \Rightarrow$ se fija la 1ª fila para hacer ceros bajo $a_{11}$:

Multiplicamos la primera ecuación por $m_2=\dfrac{-a_{21}}{a_{11}}$ y se la sumamos a la i-ésima ecuación. $\left(m_i = \dfrac{-a_{i1}}{a_{11}}\right)$

\smallskip
Repetimos el proceso con cada $a_{ii}$ hasta $a_{nn}$.
Notemos que el método sólo funciona si $a_{ii}\neq 0 \ \todoi$

\begin{ejercicio*}
    Resolver ($t=4, b=10$), (solución: $x=10$, $y=1$):
    $$
        \left\{
        \begin{array}{c}
            0.003x+59.14y=59.17 \\
            5.291x-6.13y=46.78
        \end{array}\right.
    $$

    Solución: $x=-10, y=1.001$
\end{ejercicio*}

\bigskip
\noindent
Para evitar errores, es necesaria una elección de pivotes:
\begin{itemize}
    \item \textbf{Pivotación parcial:} Se busca el coeficiente cuyo valor absoluto es mayor en la primera columna.
          Dicha fila pasa a ser la primera, hacemos ceros bajo el nuevo $a_{11}$ y para elegir el pivote $a_{22}$
          se repite el proceso.
    \item \textbf{Pivotación total:} Se busca en toda la matriz el coeficiente cuyo valor absoluto es el mayor.
          Dicho elemento se pone en $a_{11}$ intercambiando filas y el orden de las incógnitas (columnas).
          Se hacen ceros debajo y repetir el proceso para $a_{22}$.
\end{itemize}

Notemos que los métodos de pivotación funcionan $\Leftrightarrow A$ es regular.

\begin{ejercicio*} Resolver mediante aritmética de 4 dígitos.
\begin{equation*}
    \left\{\begin{array}{rcrcr}
        0.003000x & + & 59.14y & = & 59.17  \\
        5.921x & - &6.130y & = & 46.78
    \end{array}
    \right.
\end{equation*}

\begin{multline*}
    \left(\begin{array}{cc|c}
        0.003 & 59.14 & 59.17  \\
        5.921 & 6.130 & 46.78
    \end{array}
    \right) \xrightarrow[m_{2,1}=-\frac{5.921}{0.003}\approx -1974]{F'_2=m_{2,1}F_1 + F_2}
    \left(\begin{array}{cc|c}
        0.003 & 59.14 & 59.17  \\
        0 & -116700 & -116800
    \end{array}
    \right)
\end{multline*}
Por tanto,
$$y=\frac{-116800}{-116700}\approx 1.001$$
$$x=\frac{59.17 - 59.14y}{0.003} \approx \frac{59.17 - 59.20}{0.003}
\approx \frac{-0.03}{0.003}
= -10$$
\end{ejercicio*}


\subsection{Factorización LU}
\noindent
Si se puede aplicar el método de Gauss sin intercambio de filas, habremos encontrado $n-1$ matrices triangulares
inferiores tales que: $M_{n-1} \ldots M_2 M_1 A = U$, siendo $U$ una matriz triangular superior.

\begin{itemize}
    \item El producto de matrices triangulares inferiores/superiores es una matriz inferior/superior.
    \item La inversa de una matriz triangular inferior/superior es una matriz inferior/superior.
\end{itemize}

Si $M=M_{n-1} \ldots M_2 M_1, L=M^{-1} \Rightarrow MA=U \Rightarrow A=LU$

Tenemos que $Ax=b \Leftrightarrow (LU)x=b \Leftrightarrow L(Ux)=b \Leftrightarrow Ly=b \Leftrightarrow Ux=y$.\\

\bigskip
\noindent
La Factorización LU no es única. Podemos seguir un estándar que sí la haga única:
\begin{itemize}
    \item \textbf{Variante de Doolittle:} $l_{ii}=1 \ \todoi$
    \item \textbf{Variante de Crout:} $u_{ii}=1 \ \todoi$
\end{itemize}

\noindent
Para calcular la fórmula de cada posición de la factorización LU, desarrollamos el siguiente esquema (Para la de Doolittle):
$$
    \left(
    \begin{array}{ccc}
            a_{11} & a_{12} & a_{13} \\
            a_{21} & a_{22} & a_{23} \\
            a_{31} & a_{32} & a_{33}
        \end{array}
    \right) =
    \left(
    \begin{array}{ccc}
            1      & 0      & 0 \\
            l_{21} & 1      & 0 \\
            l_{31} & l_{32} & 1
        \end{array}
    \right)
    \left(
    \begin{array}{ccc}
            u_{11} & u_{12} & u_{13} \\
            0      & u_{22} & u_{23} \\
            0      & 0      & u_{33}
        \end{array}
    \right)
$$

La factorización LU no siempre es posible. Para ver cuándo se asegura, se introducen los sisguientes conceptos:

\begin{definicion}[Menor Principal]
    Sea $A \in M_n(\bb{K})$, el \underline{menor principal} de orden $k$ de la matriz $A$ es el determinante obtenido con
    las primeras $k$ filas y $k$ columnas de la matriz.
\end{definicion}

\begin{prop}
    Sea $A \in M_n(\bb{K})$. Tenemos que $A$ admite factorización LU $\Longleftrightarrow$ $A$ tiene todos sus menores principales no nulos con $L$ y $U$ regulares  .
\end{prop}

\begin{definicion}[EDD]
    Sea $A \in M_n(\bb{K})$, decimos que es \underline{Estrictamente Diagonal Dominante} (EDD) (por filas) si
    $$|a_{ii}|>\sum_{j=1,j\neq i}^n |a_{ij}| \ \todoi$$
\end{definicion}

\begin{prop}
    Sea $A \in M_n(\bb{K})$:

    Si $A$ es EDD $\Rightarrow$ es invertible.

    Si $A$ es EDD $\Rightarrow$ admite factorización LU (sin intercambio de filas).
\end{prop}

\begin{definicion}
    Una matriz simétrica se dice \underline{definida positiva} si todos sus menores principales son positivos.
\end{definicion}

\begin{prop}
    Sea $A$ una matriz definida positiva (y por tanto, simétrica) $\Rightarrow$ admite factoriazación LU sin intercambio
    de filas.
\end{prop}

\begin{teo}
    Si $A \in M_n(\bb{K})$ es regular, entonces existe una permutación de filas tal que la matriz permutada admite factorización
    LU sin intercambio de filas.
\end{teo}

Nótese que podemos aplicar LU $\Leftrightarrow$ Podemos aplicar Gauss.

\begin{ejercicio*}
    Obtén la factorización LU de:
    \begin{equation*} A = \left(
        \begin{array}{ccc}
            2 & 3 & 5 \\
            4 & 7 & 8 \\
            -2 & 0 & -7
        \end{array}\right)
    \end{equation*}

    Primero, veo el valor de los menores principales.
    \begin{equation*}
        |2|=2 \qquad
        \left| \begin{array}{cc}
            2 & 3 \\
            4 & 7
        \end{array} \right| = 2 \qquad
        |A|=-7\cdot 2 -2\cdot (-11) = 8
    \end{equation*}
    Como todos son menores principales son no nulos, admite factorización LU.
    \begin{equation*}\begin{split} A = \left(
        \begin{array}{ccc}
            2 & 3 & 5 \\
            4 & 7 & 8 \\
            -2 & 0 & -7
        \end{array}\right) & 
        = \left(
        \begin{array}{ccc}
            1 & 0 & 0 \\
            l_{21} & 1 & 0 \\
            l_{31} & l_{32} & 1
        \end{array}\right) \left(
        \begin{array}{ccc}
            u_{11} & u_{12} & u_{13} \\
            0 & u_{22} & u_{23} \\
            0 & 0 & u_{33}
        \end{array}\right) \\ &
        = \left(
        \begin{array}{ccc}
            u_{11} & u_{12} & u_{13} \\
            l_{21}u_{11} & l_{21}u_{12}+u_{22} & l_{21}u_{13}+u_{23} \\
            l_{31}u_{11} & l_{31}u_{12} + l_{32}u_{22} & l_{31}u_{13}+l_{32}u_{23} + u_{33}
        \end{array}\right)
    \end{split}\end{equation*}
    
    Igualando componentes:
    \begin{equation*}
    \left\{ \begin{array}{l}
         u_{11} = 2 \\ 
         u_{12} = 3 \\
         u_{13} = 5 \\
         l_{21}u_{11} = 4 \longrightarrow l_{21}=2 \\
         l_{21}u_{12} + u_{22} = 7 \longrightarrow u_{22}=1\\
         l_{21}u_{13} + u_{23} = 8 \longrightarrow u_{23}=-2 \\
         l_{31}u_{11} = -2 \longrightarrow l_{31}=-1 \\
         l_{31}u_{12} + l_{32}u_{22} = 0 \longrightarrow l_{32}=3 \\
         l_{31}u_{13} + l_{32}u_{23} + u_{33} = -7 \longrightarrow u_{33}=4\\
    \end{array}\right.
    \end{equation*}

    Por tanto, y tras igualar componentes,
    \begin{equation*}
        A = \left(
        \begin{array}{ccc}
            2 & 3 & 5 \\
            4 & 7 & 8 \\
            -2 & 0 & -7
        \end{array}\right)
        = \left(
        \begin{array}{ccc}
            1 & 0 & 0 \\
            2 & 1 & 0 \\
            -1 & 3 & 1
        \end{array}\right) \left(
        \begin{array}{ccc}
            2 & 3 & 5 \\
            0 & 1 & -2 \\
            0 & 0 & 4
        \end{array}\right)
    \end{equation*}
\end{ejercicio*}

\subsection{Factorización de Cholesky}
\noindent
Sea $A \in M_n(\bb{K})$, podemos intentar factorizarla de la forma: $A=LL^t$:
$$
    \left(
    \begin{array}{ccc}
            a_{11} & a_{12} & a_{13} \\
            a_{21} & a_{22} & a_{23} \\
            a_{31} & a_{32} & a_{33}
        \end{array}
    \right) =
    \left(
    \begin{array}{ccc}
            l_{11} & 0      & 0      \\
            l_{21} & l_{22} & 0      \\
            l_{31} & l_{32} & l_{33}
        \end{array}
    \right)
    \left(
    \begin{array}{ccc}
            l_{11} & l_{12} & l_{13} \\
            0      & l_{22} & l_{23} \\
            0      & 0      & l_{33}
        \end{array}
    \right)
$$

\begin{prop}
    $A$ admite factoriazación de Cholesky $\Leftrightarrow A$ es definida positiva (y por tanto, simétrica).
\end{prop}

\begin{observacion}
    La factoriazación de Cholesky es estable numéricamente.
\end{observacion}

\section{Normas vectoriales y matriciales}
\begin{definicion}[Norma]
    Dado un espacio vectorial $E$, una \underline{norma} es una aplicación $\|\cdot\|:E \rightarrow \R$ que verifica:
    \begin{enumerate}
        \item $\|x\|\geq 0 \ \forall x \in E$. Además, $\|x\|=0 \Leftrightarrow x=0$. (Definida positiva)
        \item $\|cx\|=|c|\|x\| \ \forall c \in K~\forall x \in E$. (Homogeneidad)
        \item $\|x+y\|\leq \|x\|+\|y\| \ \forall x,y \in E$. (Desigualdad triangular)
    \end{enumerate}
\end{definicion}
\bigskip
\begin{ejemplo}
    Sea $x=(x_1, \ldots, x_n) \in \R^n$:
    \begin{itemize}
        \item Norma 1: $\|x\|_1 = |x_1| + \ldots + |x_n|$.
        \item Norma 2: $\|x\|_2 = \sqrt{x_1^2 + \ldots x_n^2}$.
        \item Norma infinito: $\|x\|_\infty = \max\{|x_1|, \ldots, |x_n|\}$
    \end{itemize}
\end{ejemplo}

\begin{ejercicio*}
    Demostrar que $||\cdot ||_1$ es una norma.
    \begin{proof} Ha de cumplir las tres propiedades:
    \begin{enumerate}
        \item $||x||_1 = |x_1| + \dots + |x_n| \geq 0$, ya que es la suma de elementos positivos.
        
        Además, es necesario ver que $||x||_1 = 0 \Longleftrightarrow x = 0$.
        \begin{multline*}
            ||x||_1 = 0 \Longleftrightarrow |x_1| + \dots + |x_n| = 0 \Longleftrightarrow |x_1| = \dots = |x_n| = 0 \\
            \Longleftrightarrow x_1 = \dots = x_n = 0 \Longleftrightarrow x = 0
        \end{multline*}

        \item Veamos si $||cx||_1 = |c| ||x||_1$
        \begin{equation*}
            ||cx||_1 = |cx_1| + \dots + |cx_n| = |c|(|x_1| + \dots + |x_n|) = |c|||x||_1
        \end{equation*}

        \item Veamos la desigualdad triangular.
        \begin{multline*}
            ||x+y||_1 = |x_1+y_1| + \dots + |x_n+y_n| \leq |x_1| + |y_1| + \dots + |x_n| + |y_n| = ||x||_1 + ||y||_1
        \end{multline*}
    \end{enumerate}
    \end{proof}
\end{ejercicio*}

\begin{ejercicio*}
    Demostrar que $||\cdot ||_\infty$ es una norma.
        \begin{proof} Ha de cumplir las tres propiedades:
    \begin{enumerate}
        \item $||x||_\infty = \max\{|x_1|,\dots, |x_n|\} \geq 0$, ya que todos los elementos son positivos.
        
        Además, es necesario ver que $||x||_\infty = 0 \Longleftrightarrow x = 0$.
        \begin{multline*}
            ||x||_\infty = 0 \Longleftrightarrow \max\{|x_1|,\dots, |x_n|\} \subset \bb{R}^+_0 = 0 \Longleftrightarrow |x_1| = \dots = |x_n| = 0 \\
            \Longleftrightarrow x_1 = \dots = x_n = 0 \Longleftrightarrow x = 0
        \end{multline*}

        \item Veamos si $||cx||_\infty = |c| ||x||_\infty$
        \begin{equation*}
            ||cx||_\infty = \max\{|cx_1|,\dots, |cx_n|\} = |c|\max\{|x_1|,\dots, |x_n|\} = |c|||x||_\infty
        \end{equation*}

        \item Veamos la desigualdad triangular.
        \begin{multline*}
            ||x+y||_\infty = \max\{|x_1+y_1|,\dots, |x_n + y_n|\} \leq \\
            \leq \max\{|x_1|,\dots, |x_n|\} + \max\{|y_1|,\dots, |y_n|\} = ||x||_\infty + ||y||_\infty
        \end{multline*}
    \end{enumerate}
    \end{proof}
\end{ejercicio*}

\begin{teo}[Equivalencia de normas]
    Sea $E$ un espacio vectorial, todas las normas vectoriales son equivalentes:

    Dadas $\|\cdot\|_a, \|\cdot\|_b, \ \exists A,B \geq 0 \mid A \|x\|_a \leq \|x\|_b \leq B\|x\|_a \ \forall x \in E$.
\end{teo}

\begin{ejemplo}
    En $\R^n:$
$$\|x\|_\infty \leq \|x\|_1 \leq n\|x\|_\infty \ \forall x \in \R^n$$
\end{ejemplo}

\begin{definicion}
    Dado un espacio normado $E$, decimos que la sucesión $\{x_n\}_{n \geq 0} \subseteq E$ converge a $x \in E$ en norma
    $\Longleftrightarrow \lim\limits_{n \to \infty}\|x_n-x\|=0$.
\end{definicion}

\noindent
La convergencia es independiente a la norma elegida, puesto que todas son equivalentes. La convergencia vectorial es
equivalente a la convergencia componente a componente.

\begin{definicion}[Norma Matricial]
    Definimos una \underline{norma matricial} como una aplicación $\|\cdot\|:M_n(\bb{K}) \rightarrow \R$ que verifica:
    \begin{enumerate}
        \item $\|A\|\geq 0 \ \forall A \in M_n(\bb{K})$. Además, $\|A\|=0 \Leftrightarrow A=0$. (Definida positiva)
        \item $\|cA\|=|c|\|A\| \ \forall c \in K~\forall A \in M_n(\bb{K})$. (Homogeneidad)
        \item $\|A+B\|\leq \|A\|+\|B\| \ \forall A,B \in M_n(\bb{K})$. (Desigualdad triangular)
        \item $\|AB\|\leq \|A\|\|B\| \ \forall A,B \in M_n(\bb{K})$

        Notemos por tanto que $\|A^k\|\leq \|A\|^k$
    \end{enumerate}
\end{definicion}

\begin{definicion}
    Dada una norma matricial $\|\cdot\|$, se define la \underline{norma matricial inducida} (o subordinada) en la forma:
    $$\|A\| = \max_{x \neq 0} \dfrac{\|Ax\|}{\|x\|}$$
\end{definicion}
\begin{prop} Se verifica la siguiente igualdad:
\begin{equation*}
    ||A||= \max_{x \;/\; ||x||=1} ||Ax||
\end{equation*}
\end{prop}
\begin{proof}
    $$\max_{x \neq 0} \dfrac{\|Ax\|}{\|x\|} = \max_{x \neq 0} \dfrac{1}{\|x\|} \|Ax\| = \max_{x \neq 0} \|A \dfrac{x}{\|x\|}\|=
        \max_{\|x\|=1}\|Ax\|$$
\end{proof}


\begin{prop}
    Dada la matriz $A$, demostrar que:
    $$||A||_1 = \max_{j=1,\dots,n} \sum_{i=1}^n |a_{ij}|$$
    \begin{proof}
        Sea $\displaystyle M=\max_{j=1,\dots,n} \sum_{i=1}^n |a_{ij}|$ y $x \in \bb{R}^n, \quad x\neq 0$.

        La componente $i$-ésima de $Ax$ es:
        $$(Ax)_{i} = a_{i1}x_1 + \dots + a_{in}x_n = \sum_{j=1}^na_{ij}x_j$$

        Por tanto:
        \begin{multline*}
            ||Ax||_1 = \sum_{i=1}^n \left| \sum_{j=1}^n a_{ij} x_j\right| \leq \sum_{i=1}^n\sum_{j=1}^n |a_{ij}| |x_j| = \sum_{j=1}^n \left(\sum_{i=1}^n |a_{ij}| \right)|x_j| \leq \sum_{j=1}^n M|x_j| = \\
            M \sum_{j=1}^n|x_j| = M||x||_1
        \end{multline*}

        Es decir, $$||Ax||_1 \leq M||x||_1 \Longrightarrow \frac{||Ax||_1}{||x||_1} \leq M$$

        Como esto es cierto $\forall x\neq 0$, también es cierto para el máximo.
        \begin{equation} \label{DemosNormaMatriz1}
            \max_{x\neq 0} \frac{||Ax||_1}{||x||_1} \leq M
        \end{equation}

        Supongo que el máximo de $M$ se alcanza en $j=k$.
        $$M = \sum_{i=1}^n |a_{ik}|$$

        Como esto es cierto $\forall x\neq 0$, tomemos $e_k = (0,\dots,1,\dots,0)$, con $||e_k||_1=1$,
        $$||Ae_k||_1 = ||(a_{1k}, \dots, a_{nk})||_1 = \sum_{i=1}^n |a_{ik}| = M$$

        Como hemos visto en la Ec. \ref{DemosNormaMatriz1}, el máximo es $\leq M$. Como hemos visto un vector $v\neq 0$ que cumple que el máximo es igual a M, entonces:
        $$||A||_1 :=  \max_{x\neq 0} \frac{||Ax||_1}{||x||_1} = M = \max_{j=1,\dots,n} \sum_{i=1}^n |a_{ij}|$$
        
    \end{proof}
\end{prop}

\begin{prop}
    Dada la matriz $A$, demostrar que:
    $$||A||_\infty = \max_{i=1,\dots,n} \sum_{j=1}^n |a_{ij}|$$
    \begin{proof}
        Sea $\displaystyle M=\max_{i=1,\dots,n} \sum_{j=1}^n |a_{ij}|$ y $x \in \bb{R}^n, \quad x\neq 0$.

        La componente $i$-ésima de $Ax$ es:
        $$(Ax)_{i} = a_{i1}x_1 + \dots + a_{in}x_n = \sum_{j=1}^na_{ij}x_j$$
        \begin{equation*}
            |(Ax)_{i}| = \left| \sum_{j=1}^na_{ij}x_j\right| \leq 
            \sum_{j=1}^n |a_{ij}||x_j| \leq ||x||_\infty \sum_{j=1}^n |a_{ij}| \leq  ||x||_\infty M
        \end{equation*}

        Por tanto:
        \begin{equation*}
            ||Ax||_\infty = \max\{|(Ax)_1|, \dots, |(Ax)_n|\} \leq ||x||_\infty M \Longrightarrow \frac{||Ax||_\infty}{||x||_\infty} \leq M
        \end{equation*}

        Como esto es cierto $\forall x\neq 0$, también es cierto para el máximo.
        \begin{equation}\label{DemosNormaMatriz2}
            \max_{x\neq 0} \frac{||Ax||_\infty}{||x||_\infty} \leq M
        \end{equation}

        Supongo que el máximo de $M$ se alcanza en $i=k$.
        $$M = \sum_{j=1}^n |a_{kj}|$$

        Como esto es cierto $\forall x\neq 0$, tomemos $t \in \bb{R}^n$, con $||t||_\infty=1$, de forma que su componente $i$-ésima sea:
        $$t_i = sgn(a_{kj})$$
        \begin{multline*}
            ||At||_\infty = \left|\left|\left(\sum_{j=1}^na_{1j}t_j, \dots, \sum_{j=1}^na_{nj}t_j\right)\right|\right|_\infty = \max \left\{\left|\sum_{j=1}^na_{1j}t_j\right|, \dots, \left|\sum_{j=1}^na_{nj}t_j\right|\right\} = \\=   \max_{i=1,\dots,n} \left|\sum_{j=1}^na_{ij}t_j\right|
            = \left|\sum_{j=1}^na_{hj}t_j\right|
            \text{ para algún } h\in\{1,\dots, n\}
        \end{multline*}

        Por tanto, sabemos que
        \begin{equation}\label{Desigualdad1}
            ||At||_\infty = \left|\sum_{j=1}^na_{hj}t_j\right| \geq M
        \end{equation}
        Además, como $||Ax||_\infty \leq ||x||_\infty M$, para $x=t$,
        \begin{equation}\label{Desigualdad2}
            ||At||_\infty \leq ||t||_\infty M = M
        \end{equation}

        Por tanto, por las ecuaciones \ref{Desigualdad1} y \ref{Desigualdad2}, $||At||_\infty = M$. Además, se comprueba también que $h=k$.


        Como hemos visto en la Ec. \ref{DemosNormaMatriz2}, el máximo es $\leq M$. Como hemos visto un vector $t\neq 0$ que cumple que el máximo es igual a M, entonces:
        $$||A||_\infty :=  \max_{x\neq 0} \frac{||Ax||_\infty}{||x||_\infty} = M = \max_{i=1,\dots,n} \sum_{j=1}^n |a_{ij}|$$
        
    \end{proof}
\end{prop}

\bigskip
\noindent
No todas las formas matriciales son inducidas. La norma de Frobenius es un ejemplo de ello:
\begin{definicion}[Norma de Frobenius]
    Definimos la norma de Frobenius por:
$$\|A\|_F = \sqrt{\sum_{i=1}^n\sum_{j=1}^n |a_{ij}^2|} = \sqrt{tr(A^\ast A)}$$

siendo $A^\ast$ la conjugada de $A$.
\end{definicion}

\noindent Veamos que no es una norma inducida.
\begin{lema}
    La norma de Frobenius no es una norma inducida.
\end{lema}
\begin{proof}
    Sea $A=I$. $$||I||_F=\sqrt{n}$$

    Sin embargo, calculando la norma inducida de $A$,
    \begin{equation*}
        ||A|| = \max_{x\neq 0} \frac{||Ix||}{||x||} = \max_{x\neq 0} \frac{||x||}{||x||} = 1
    \end{equation*}

    Por tanto, para $A=I_n$ y $n>1$, se demuestra que la norma de Frobenius no es inducida. Debido al contrajemplo, se generaliza a cualquier matriz.
\end{proof}

\begin{observacion}
    Si una norma es inducida $\Rightarrow \|I\|=1$
\end{observacion}

\begin{coro}
    Dada una norma vectorial y su matricial inducida, se verifica:
    $$\|Ax\| \leq \|A\|\|x\| (= \max_{x\neq 0}\|Ax\|)$$
\end{coro}

\begin{definicion}
    Dada una norma vectorial $\|\cdot\|_v$ y una matricial $\|\cdot\|_M$, decimos que ambas normas son \underline{compatibles}
    si $\forall A \in M_n(\bb{K})$ y $\forall x \in \bb{K}^n$ se verifica:
    $$\|Ax\|_v \leq \|A\|_M \|x\|_v$$
\end{definicion}

\newtheorem*{propCompatible}{Proposición}
\begin{prop}
    Dada una norma matricial $\|\cdot\|_M$, siempre existe una norma vectorial compatible con ella.
\end{prop}
\begin{proof}
    Definimos para cualquier $\|\cdot\|_M : \|x\|_v = \|x(v^{*})^t\|_M$ (donde $v^{*}=~(1, 0, \ldots, 0)$). Veamos que es una norma:
    \begin{itemize}
        \item $||x||_v \geq 0 \Longleftrightarrow ||x||_v = ||x(v^\ast)^t||_M \geq 0$, cierto ya que es una norma matricial.

        Además, vemos que $||x||_v = 0 \Longleftrightarrow x=0$
        $$||x||_v = 0 \Longleftrightarrow ||x(v^\ast)^t||_M = 0 \Longleftrightarrow x(v^\ast)^t = 0 \Longleftrightarrow x=0$$

        \item $||cx||_v = |c|||x||_v \Longleftrightarrow ||cx(v^\ast)^t||_M = |c|||x(v^\ast)^t||_M = |c| ||x||_v$

        \item $||x+y||_v \leq ||x||_v + ||y||_v \Longleftrightarrow ||x+y||_v = ||(x+y)(v^\ast)^t||_M = ||x(v^\ast)^t + y(v^\ast)^t||_M \leq ||x(v^\ast)^t||_M + ||y(v^\ast)^t||_M = ||x||_v + ||y||_v$
    \end{itemize}

    Veamos ahora que son compatibles. Para ello, comprobamos que $||Ax||_v \leq ||A||_M ||x||_v$
    \begin{multline*}
        ||Ax||_v \leq ||A||_M ||x||_v \Longleftrightarrow ||Ax||_v = ||(Ax)(v^\ast)^t||_M = ||A(x(v^\ast)^t)||_M
        \leq \\ \leq
        ||A||_M ||x(v^\ast)^t||_M = ||A||_M ||x||_v
    \end{multline*}
\end{proof}

\begin{observacion}
La norma de Frobenius es compatible con la norma Euclídea.
\begin{equation*}
    \|\cdot\|_F: \|x\|_v = \|x(v^{*})^t\|_M = \|x\|_2 \ \forall x \in \R^n
\end{equation*}
\end{observacion}

Notemos que:
$$\|A\|_M = \max_{x\neq 0}\dfrac{\|Ax\|_v}{\|x\|_v} \Rightarrow \|\cdot\|_M \mbox{ y } \|\cdot\|_v \mbox{ son compatibles.}$$
Pero sin embargo el contrarrecíproco es falso.

\subsection{Valores y vectores propios}
\begin{definicion}[Valor Propio]
    Sea $\lambda \in \bb{K}$. Decimos que $\lambda$ es valor propio de $A \in M_n(\bb{K}) \Leftrightarrow \ \exists v \in \bb{K}^n \setminus \{0\} \mid Av=\lambda v$.

    En dicho caso, $v$ es un vector propio asociado al valor propio $\lambda$.
\end{definicion}

Algunas definiciones importantes son:
\begin{itemize}
    \item Polinomio característico: $p(\lambda)=det(A - \lambda I)$
    \item Ecuación característica: $p(\lambda)=0$
    \item Espectro: $\sigma(A)=\{\lambda \in K \mid p(\lambda)=0\}$
    \item Radio espectral: $\rho(A) = \max\{|\lambda| \mid \lambda \in \sigma(A)\}$
\end{itemize}

\begin{prop}
    $$\forall A \in M_n(\bb{K}) \ \forall \|\cdot\|_M \Rightarrow \rho(A)\leq \|A\|_M$$
\end{prop}
\begin{proof}
    Si $\lambda \in \sigma(A)$ y $v$ es un vector propio de $A$, construimos $M=(v, 0, \ldots, 0)$.

    Como $v\neq 0 \Rightarrow \|M\|_M \neq 0$

    Además: $AM = (Av, 0, \ldots, 0) = (\lambda v, 0, \ldots, 0) = \lambda M$

    Por tanto:
    $$\left.\begin{array}{l}
            \|AM\|_M = \|\lambda M\|_M = |\lambda|\|M\|_M \\
            \|AM\|_M \leq \|A\|_M \|M\|_M
        \end{array}\right\} \Rightarrow |\lambda|\|M\|_M \leq \|A\|_M \|M\|_M$$

    Como $\|M\|_M \neq 0 \Rightarrow |\lambda|\leq \|A\|_M \ \forall \lambda \in \sigma(A) \Rightarrow \rho(A) \leq \|A\|_M$
\end{proof}

\begin{prop}
    $$\forall A \in M_n(\bb{K}) \rho(A) = \inf_{\|\cdot\|_M}\{\|A\|_M\}$$
\end{prop}

\begin{definicion}
    Decimos que la sucesión $\{A_k\}_{k \geq 0}$ converge a una matriz $A \in M_n(\bb{K})$ si $\exists \|\cdot\|_M$ tal que:
    $$\lim_{k\to \infty} \|A_k - A\|_M = 0$$
\end{definicion}

Notemos que, como todas las normas de dimensión finita son equivalentes, la convergencia es independiente de la norma elegida.
La convergencia matricial es equivalente a la convergencia componente a componente.

\begin{prop}
    $\forall A \in M_n(\bb{K})$, son equivalentes:
    \begin{enumerate}
        \item $\{A^k\}_{k \geq 0} \rightarrow 0$
        \item $\exists \|\cdot\|_M \mid \|A\|_M < 1$
        \item $\rho(A)<1$
    \end{enumerate}
\end{prop}
\begin{proof} Procedemos mediante doble implicación:

    \begin{description}
        \item [$\mathbf{2. \Longrightarrow 3.)}$] $\rho(A) = \inf\limits_{\|\cdot\|_M}\{\|A\|_M\}$
        
        \item [$\mathbf{1. \Longrightarrow 3.)}$] Sea $\lambda \in \sigma(A)$ y sea $v$ un vector propios asociado: $\{A^kv\}=\{\lambda^kv\}$
    
        Por tanto, si $\{A^k\}\rightarrow 0 \Rightarrow \{\lambda^kv\} \rightarrow 0$ y como $v \neq 0 \Rightarrow |\lambda|<1$
        \item [$\mathbf{2. \Longrightarrow 1.)}$]
            Suponemos que $\|A\|_M < 1$ luego:
        $$\lim_{x \to \infty}\|A^k\|_M \leq \lim_{x \to \infty}\|A\|^k_M = 0 \Rightarrow \lim_{x \to \infty}\|A^k\|_M = 0 \Rightarrow
            \{A^k\} \rightarrow 0$$
    \end{description}
\end{proof}

\subsection{Condicionamiento}
\noindent
Sea $A \in M_n(\bb{K})$ una matriz regular, consideramos el SEL: $Ax=b$, con solución $x=A^{-1}b$.
Si alteramos $b$, obtenemos $b^{*}$, tenemos que $x^{*}=A^{-1}b^{*}$.

Tomando normas calculamos el error:
$$\|x-x^{*}\| \leq \|A^{-1}\|\|b-b^{*}\| = \|A^{-1}\|\|Ax\| \dfrac{\|b-b^{*}\|}{\|b\|}$$

Y por tanto: $\dfrac{\|x-x^{*}\|}{\|x\|}\leq \|A^{-1}\|\|A\| \dfrac{\|b-b^{*}\|}{\|b\|}$

\begin{definicion}[Número de condición]
    El \underline{número de condición} de una matriz regular $A \in M_n(\bb{K})$, $k(A)$ se define como:
    $$k(A)=\|A\|\|A^{-1}\|$$

    Siendo $\|\cdot\|$ una norma matricial.
\end{definicion}

\begin{lema}
    Dada $A$ matriz cuadrada, tenemos que:
    $$k(A)\geq 1$$
\end{lema}
\begin{proof}
$$k(A)=\|A\|\|A^{-1}\| \geq \|AA^{-1}\| = \|I\| \geq 1$$
$$\|A\| = \|AI\| \leq \|A\|\|I\| \Rightarrow \|I\| \geq 1$$   
\end{proof}


Tenemos que el sistema se computará peor cuanto mayor sea $k(A)$.

Para $k(A)\approx 10^k$, esperamos una pérdida de $k$ dígitos significativos exactos.

\section{Métodos iterativos}
\noindent
Dado un SEL $Ax=b$, la idea de los métodos iterativos es transformarlo en uno equivalente de punto fijo: $x=Bx+c$
$$(Ax=b \Leftrightarrow x-x+Ax=b \Leftrightarrow x=x-Ax+b \Leftrightarrow x=(I-A)x+b)$$

\noindent
Dado un $x^{(0)}$ arbitrario, construiremos la sucesión: $x^{(k+1)}=Bx^{(k)}+c$. Si esta sucesión converge, lo hará
a la solución de nuestro sistema.


\begin{definicion}
    Un método iterativo se dice convergente si lo es la sucesión $\{x^{(k)}\}_k$ generada por dicho método.
\end{definicion}

\begin{teo}
    Dado el método $x^{(k+1)}=Bx^{(k)}+c$ si existe una norma matricial tal que $\|B\|<1 (\Leftrightarrow \rho(B)<1)$, entonces el método es convergente.
\end{teo}
\begin{proof}
    Sea $s$ la solución de nuestro SEL: $As=b \Leftrightarrow s=Bs+c$. Entonces:

    $\|x^{(k)}-s\| = \|Bx^{(k-1)}+c-(Bs+c)\| = \|B(x^{(k-1)}-s)\| \leq \|B\|\|x^{(k-1)}-s\|$

    Si repetimos el proceso $k$ veces: $\|x^{(k)}-s\|\leq \|B\|^k \|x^{(0)}-s\|$

    Tomando límite:
    $$\lim_{k \to +\infty}\|x^{(k)}-s\| \leq \lim_{k \to +\infty}\|B\|^k\|x^{(0)}-s\|=0$$
\end{proof}

\begin{teo}[Criterio de Pausa]
    Dado el método $x^{(k+1)}=Bx^{(k)}+c$, si $\exists \|\cdot\| \mid \|B\|<1$ entonces:
    $$\|x^{(k)}-s\| \leq \dfrac{\|B\|}{1-\|B\|}\|x^{(k)}-x^{(k-1)}\|$$
\end{teo}
\begin{proof}
    $\|x^{(k)}-s\|\leq \|B\|\|x^{(k-1)}-s\| = \|B\|\|x^{(k-1)}-x^{(k)}+x^{(k)}-s\| \leq
        \|B\|(\|x^{(k-1)}-x^{(k)}\| + \|x^{(k)}-s\|)$

    Luego: $\|x^{(k)}-s\|\leq \|B\|\|x^{(k-1)}-x^{(k)}\| + \|B\|\|x^{(k)}-s\|$

    $$\|x^{(k)}-s\|-\|B\|\|x^{(k)}-s\| \leq \|B\| \|x^{(k+1)}-x^{(k)}\|$$

    $$(1-\|B\|)\|x^{(k)}-s\| \leq \|B\|\|x^{(k)}-x^{(k-1)}\|$$

    Si $1-\|B\| > 0 \Leftrightarrow \|B\|<1$ Entonces:
    $$\|x^{(k)}-s\| \leq \dfrac{\|B\|}{1-\|B\|}\|x^{(k)}-x^{(k-1)}\|$$
\end{proof}

\noindent
Dicho teorema nos permite exigir un mínimo de precisión (tolerancia) en el cálculo de $s$. Si queremos una
precisión de $10^{-4}$: $\|x^{(k)}-s\| \leq \dfrac{\|B\|}{1-\|B\|}\|x^{(k)}-x^{(k-1)}\|\leq 10^{-4}$.

\subsection{Métodos de descomposición}
\noindent
Dado un SEL $Ax=b$ y una matriz $Q$ invertible llamada matriz de \underline{descomposición}, podemos escribir el SEL
como: $(Q-(Q-A))x=b$ luego:
$$Qx=(Q-A)x+b \Leftrightarrow x=Q^{-1}(Q-A)x + Q^{-1}b \Leftrightarrow x=(I-Q^{-1}A)x+Q^{-1}b$$
\noindent
con la que escribimos la sucesión $x^{(k+1)} = (I-Q^{-1}A)x^{(k)}+Q^{-1}b$.

El vector inicial suele ser $x^{(0)}=0$.

\bigskip \noindent
Cada método iterativo se define por la elección de $Q$ (Podemos escribir $A$ como $A=D+L+U$):
\begin{itemize}
    \item Jacobi: $Q=D \Rightarrow A=D+(L+U)$
          $$x^{(k+1)}=D^{-1}\left(L-(L+U)x^{(k)}+b\right)$$
    \item Gauss-Seidel: $Q=D+L \Rightarrow A=(D+L)+U$
          $$x^{(k+1)}=(D+L)^{-1}(-Ux^{(k)}+b)$$
    \item Relajación: $Q=\omega^{-1}(D+\omega L) \Rightarrow A=(\omega^{-1}D+L)+\omega^{-1}(\omega-1)D+U$
          $$x^{(k+1)}=\omega(D+\omega L)^{-1}(-Ux^{(k)}+b)+(1-\omega)(D+\omega L)^{-1}Dx^{(k)}$$
\end{itemize}

Ecuaciones de cada método:
\begin{itemize}
    \item Jacobi:
          $$x_i^{(k+1)} = \dfrac{1}{a_{ii}} \left(b_i - \sum_{j=1}^n a_{ij}x_j^{(k)}\right)$$
    \item Gauss-Seidel:
          $$x_i^{(k+1)} = \dfrac{1}{a_{ii}}\left( b_i - \sum_{j=1}^{i-1}a_{ij}x_j^{(k+1)} -
              \sum_{j=1+i}^n a_{ij}x_j^{(k)} \right)$$
\end{itemize}

\begin{ejemplo}
Dado el siguiente sistema, obtener las ecuaciones de los métodos iterativos:
$$
    \left.\begin{array}{r}
        3x+2y=5 \\
        x+2y=3
    \end{array}\right\}
$$

\begin{itemize}
    \item Método de Jacobi

    $$x^{(k+1)}=\dfrac{1}{3}(5-2y^{(k)})$$
    $$y^{(k+1)}=\dfrac{1}{2}(3-x^{(k)})$$
    Es decir:
    $$
        \left(\begin{array}{c}
                x^{(k+1)} \\
                y^{(k+1)}
            \end{array}\right) =
        \left(\begin{array}{cc}
                0    & -2/3 \\
                -1/2 & 0
            \end{array}\right)
        \left(\begin{array}{c}
                x^{(k)} \\
                y^{(k)}
            \end{array}\right) +
        \left(\begin{array}{c}
                5/3 \\
                3/2
            \end{array}\right)
    $$

    \item Método de Gauss-Seidel:
    
    Similar al anterior pero más rápido (dado el mismo SEL):
    $$x^{(k+1)}=\dfrac{1}{3}(5-2y^{(k)})$$
    $$y^{(k+1)}=\dfrac{1}{2}(3-x^{(k+1)})$$
    Es decir:
    $$
        \left(\begin{array}{c}
                x^{(k+1)} \\
                y^{(k+1)}
            \end{array}\right) =
        \left(\begin{array}{cc}
                0 & -2/3 \\
                0 & 1/3
            \end{array}\right)
        \left(\begin{array}{c}
                x^{(k)} \\
                y^{(k)}
            \end{array}\right) +
        \left(\begin{array}{c}
                5/3 \\
                2/3
            \end{array}\right)
    $$
    \end{itemize}
\end{ejemplo}

\begin{teo}[Convergencia de Jacobi y Gauss-Seidel]
    Si $\sum\limits_{j=1}^n\left|\dfrac{a_{ij}}{a_{ii}}\right| <1 \ \todoi$ (es decir, $A$ es EDD), entonces los métodos
    de Jacobi y Gauss-Seidel convergen.

    Puede ocurrir que uno sea convergente y el otro no.

    Si ambos lo son, el más rápido es el de Gauss-Seidel.
\end{teo}
\begin{proof}
    Sea $B_J$ la matriz del método de Jacobi:
    $$
        B_J = \left(\begin{array}{cccc}
                0                      & \frac{-a_{12}}{a_{11}} & \ldots & \frac{-a_{1n}}{a_{11}} \\
                \frac{-a_{21}}{a_{22}} & 0                      & \ldots & \frac{-a_{2n}}{a_{22}} \\
                \vdots                 & \vdots                 & \ddots & \vdots                 \\
                \frac{-a_{n1}}{a_{nn}} & \frac{-a_{n2}}{a_{nn}} & \ldots & 0
            \end{array}\right)
    $$

    Como $A$ es EDD $\Rightarrow \|B_J\|_\infty <1 \Rightarrow$ converge.\\

    Consideramos $B_G$ y $x,y\in\R^n \mid \|x\|_\infty=1 \ \land \ y=B_Gx$. Probemos que $\|y\|_\infty\leq C=\|B_J\|_\infty<1$:

    Para $k=1$: $|y_1|\leq \frac{1}{|a_{11|}}\sum\limits_{j=2}^n |a_{ij}||x_j| \leq \|x\|_\infty \sum\limits_{j=2}^n \dfrac{|a_{ij}|}{|a_{11}|}\leq C$

    Sea cierto hasta $k-1$: $$|y_k|\leq \dfrac{1}{|a_{kk}|}\left( \sum_{j=1}^{k-1}|a_{kj}||y_j| + \sum\limits_{j=k+1}^n |a_{kj}||x_j| \right)
        \leq$$ $$\leq \frac{1}{|a_{kk}|}\left(\sum_{j=1}^{k-1}|a_{kj}|C + \sum_{j=k+1}^n |a_{kj}|\|x\|_\infty \right) \leq
        \dfrac{1}{|a_{kk}|}\sum_{j\neq k}|a_{kj}|\leq C \Rightarrow$$ $$ \Rightarrow \|y\|_\infty \leq C \Rightarrow \|B_G\|_\infty =
        \max\limits_{\|x\|=1}\|B_Gx\|_\infty \leq C$$
\end{proof}

\begin{teo}
    Si $A$ es definida positiva (y por tanto simétrica) entonces el método de Gauss-Seidel es convergente.
\end{teo}

\begin{observacion} Cabe destacar los siguientes aspectos:
    \begin{itemize}
    \item Al aplicar una operación elemental al inicio podemos hacer que varíe la convergencia de los métodos, podemos
          hacer que deje de ser EDD o modificar el radio espectral.
    \item Cada iteración de un SEL $n\times n$ realiza $n^2$ operaciones en los métodos de Jacobi y Gauss-Seidel.
\end{itemize}
\end{observacion}

\begin{ejemplo}
Obtener las ecuaciones iterativas del siguiente sistema:
\begin{equation*}
    \left. \begin{array}{c}
        2x_1 + 3x_2 -x_3 = 0\\
        3x_1 + 5x_2 + 4x_3 = 7\\
        x_1-2x_2 + 5x_3 = 3
    \end{array}\right\}
\end{equation*}

Despejando,
\begin{equation*}
    \left. \begin{array}{c}
        \displaystyle x_1 = -\frac{3}{2} x_2 + \frac{1}{2}x_3 + 3 \\
        \displaystyle x_2 = -\frac{3}{5}x_1 - \frac{4}{5}x_3 + \frac{7}{5} \\
        \displaystyle x_3 = -\frac{1}{5}x_1 + \frac{2}{5}x_2 + \frac{3}{5}
    \end{array}\right\}
\end{equation*}

Método de Jacobi:
\begin{equation*}
    \left. \begin{array}{c}
        \displaystyle x_1^{(k+1)} = -\frac{3}{2} x_2^{(k)} + \frac{1}{2}x_3^{(k)} + 3 \\
        \displaystyle x_2^{(k+1)} = -\frac{3}{5}x_1^{(k)} - \frac{4}{5}x_3^{(k)} + \frac{7}{5} \\
        \displaystyle x_3^{(k+1)} = -\frac{1}{5}x_1^{(k)} + \frac{2}{5}x_2^{(k)} + \frac{3}{5}
    \end{array}\right\}
    \qquad \qquad
    B_J = \left( \begin{array}{ccc}
        0 & -\frac{3}{2} & \frac{1}{2} \\
        -\frac{3}{5} & 0 & -\frac{4}{5} \\
        -\frac{1}{5} & \frac{2}{5} & 0
    \end{array}\right)
\end{equation*}


Método de Gauss-Seidel:
\begin{equation*}
    \left. \begin{array}{c}
        \displaystyle x_1^{(k+1)} = -\frac{3}{2} x_2^{(k)} + \frac{1}{2}x_3^{(k)} + 3 \\
        \displaystyle x_2^{(k+1)} = -\frac{3}{5}x_1^{(k+1)} - \frac{4}{5}x_3^{(k)} + \frac{7}{5} \\
        \displaystyle x_3^{(k+1)} = -\frac{1}{5}x_1^{(k+1)} + \frac{2}{5}x_2^{(k+1)} + \frac{3}{5}
    \end{array}\right\}
\end{equation*}

\end{ejemplo}

\subsection{Métodos de relajación}
Basados en la descomposición $A=L+D+U$:
$$A=L+\frac{1}{\omega}D+\frac{\omega-1}{\omega}D+U \Rightarrow x^{(k+1)}=\left(L+\frac{1}{\omega}D\right)^{-1}
    \left(b - (\frac{\omega-1}{\omega}D + U)x^{(k)}\right)$$

Ecuaciones:
$$x_i^{(k+1)} = \dfrac{\omega}{a_{ii}} \left(b_i - \sum_{j=1}^{i-1}a_{ij}x_j^{(k+1)} - \sum_{j=i+1}^n a_{ij}x_j^{(k)}\right) +
    (1-\omega)x_i^{(k)}$$

Que son $\omega$ por las ecuaciones del método de Gauss-Seidel más $(1-\omega)x_i^{(k)}$.

\noindent
Si $\omega=1 \Rightarrow$ el método es igual a Gauss-Seidel.

Si $\omega<1$ hablamos de un método de subrelajación mientras que si $\omega>1$ hablamos de un método de sobrerelajación.

\bigskip
\begin{teo}
    El método de relajación sólo puede ser convergente si $0<\omega<2$ (Si converge $\Rightarrow 0<\omega<2$)
\end{teo}
\begin{proof}
    Sea $B_\omega = -\left(L+\frac{1}{\omega}D\right)^{-1}\left(\frac{\omega-1}{\omega}D+U\right)$ (las dos triangulares)

    $$det(B_\omega) = (-1)^n \omega^n det(D)^{-1} \left(\frac{\omega-1}{\omega}\right)^n det(D) = (1-\omega)^n \Rightarrow
        det(B_\omega) = \prod_{i=1}^n |\lambda_i| = |1-\omega|^n$$

    Luego $\rho(B_\omega)\geq |1-\omega|$ ya que si $\rho(B_\omega)<|1-\omega| \Rightarrow \prod\limits_{i=1}^n |\lambda_i|\leq \rho(B_\omega)^n < |1-\omega|^n$

    Por tanto: $\rho(B_\omega)<1 \Rightarrow |1-\omega| <1 \Rightarrow 0<\omega<2$
\end{proof}

\begin{teo}
    Si $A$ es EDD y $0<\omega\leq 1 \Rightarrow$ el método de relajación es convergente.
\end{teo}


\section{Ejercicios}
Los ejercicios relativos a este tema están disponbles en la sección \ref{sec:Rel2}.