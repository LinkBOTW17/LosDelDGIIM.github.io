\section{Ecuación Lineal de Orden Superior}

\begin{ejercicio}\label{ej:4.1}
    Encuentra funciones $a, b \in C(I)$ de modo que $t, t^2$ sean soluciones de una ecuación lineal
    \[
        x'' + a(t)x' + b(t)x = 0
    \]
    con $a, b \in C(I)$. Discute si el intervalo $I$ puede ser toda la recta real o no.\\

    Sea $L$ el operador diferencial $L$ asociado a dicha ecuación. Entonces, tenemos que:
    \begin{align*}
        L(t) = 0 & \Longleftrightarrow a(t) + b(t)t=0 \quad \forall t \in I\\
        L(t^2) = 0 & \Longleftrightarrow 2 + 2a(t)t + b(t)t^2 = 0 \quad \forall t \in I
    \end{align*}

    Por tanto, tenemos que $a(t)=-b(t)t$ para todo $t\in I$, por lo que:
    \begin{equation*}
        0=2-2b(t)t^2+b(t)t^2=2-b(t)t^2 \quad \forall t \in I
        \Longrightarrow \left\{
        \begin{aligned}
            b(t) &= \frac{2}{t^2}\\
            a(t) &= -\frac{2}{t}
        \end{aligned}
        \quad \forall t \in I
        \right.
    \end{equation*}

    Por tanto, tenemos que $0\notin I$. Como $I$ es un intervalo, entonces $I\subset \bb{R}^+$ o $I\subset \bb{R}^-$, pero no puede ser toda la recta real.
\end{ejercicio}

\begin{ejercicio}\label{ej:4.2}
    Encuentra un sistema fundamental de soluciones de la ecuación $3x'' - 2x' - 8x = 0$
    \begin{observacion}
        Busca soluciones de la forma $e^{\lambda t}$.
    \end{observacion}
    Por el método de variación de constantes, encuentra la solución general de la ecuación $3x'' - 2x' - 8x = \cosh(t)$.\\

    Supongamos $e^{\lm t}$ una solución de la ecuación dada. Entonces, tenemos que:
    \begin{equation*}
        L[e^{\lm t}]=0 \Longleftrightarrow 3\lm^2-2\lm-8=0
        \Longleftrightarrow \left\{
        \begin{aligned}
            \lm_1 &= 2\\
            \lm_2 &= -\frac{4}{3}
        \end{aligned}
        \right.
    \end{equation*}

    Por tanto, tenemos que $x_1(t)=e^{\lm_1 t}$ y $x_2(t)=e^{\lm_2 t}$ son soluciones de la ecuación dada. Veamos que son linealmente independientes:
    \begin{equation*}
        W(x_1,x_2)(t)=\begin{vmatrix}
            e^{\lm_1 t} & e^{\lm_2 t}\\
            \lm_1e^{\lm_1 t} & \lm_2e^{\lm_2 t}
        \end{vmatrix}=e^{(\lm_1+\lm_2)t}\begin{vmatrix}
            1 & 1\\
            \lm_1 & \lm_2
        \end{vmatrix}=e^{(\lm_1+\lm_2)t}(\lm_2-\lm_1)\neq 0
    \end{equation*}
    
    
    Para resolver la ecuación completa dada, buscamos una solución particular. Debido al término $\cosh(t)$, buscamos una solución de la forma:
    \begin{equation*}
        x_p(t)=\alpha\cosh(t)+\beta\senh(t)
    \end{equation*}

    Calculemos los valores de $\alpha$ y $\beta$:
    \begin{align*}
        x_p'(t)&=\alpha\senh(t)+\beta\cosh(t)\\
        x_p''(t)&=\alpha\cosh(t)+\beta\senh(t)
    \end{align*}

    Por tanto, que sea solución de la ecuación completa implica que:
    \begin{align*}
        3(\alpha\cosh(t)+\beta\senh(t))-2(\alpha\senh(t)+\beta\cosh(t))-8(\alpha\cosh(t)+\beta\senh(t))&=\cosh(t)\\
        (3\alpha-2\beta-8\alpha)\cosh(t)+(3\beta-2\alpha-8\beta)\senh(t)&=\cosh(t)
    \end{align*}

    Como $\senh,\cosh$ son linealmente independientes, entonces:
    \begin{equation*}
        \left\{
        \begin{aligned}
            -5\alpha-2\beta &= 1\\
            -2\alpha-5\beta &= 0
        \end{aligned}
        \right\}
        \Longrightarrow
        \left\{
        \begin{aligned}
            \alpha &= \nicefrac{-5}{21}\\
            \beta &= \nicefrac{2}{21}
        \end{aligned}
        \right.
    \end{equation*}

    Por tanto, tenemos que una solución particular de la ecuación dada es:
    \begin{equation*}
        x_p(t)=\nicefrac{-5}{21}\cosh(t)+\nicefrac{2}{21}\senh(t)
    \end{equation*}

    Por tanto, la solución general de la ecuación dada es:
    \begin{equation*}
        x(t)=c_1e^{2t}+c_2e^{-\nicefrac{4}{3}t}+\nicefrac{-5}{21}\cosh(t)+\nicefrac{2}{21}\senh(t)
        \qquad \text{con } t\in \bb{R},c_1,c_2\in \bb{R}
    \end{equation*}
\end{ejercicio}

\begin{ejercicio}\label{ej:4.3}
    Encuentra la solución general de la ecuación
    \[
        y'' + \frac{2}{x}\cdot y' + y = \frac{1}{x},
    \]
    sabiendo que dos soluciones de la ecuación homogénea son $\frac{\sen x}{x}$, $\frac{\cos x}{x}$.
    
    Como dominio, consideramos $I=\bb{R}^+$ (el caso $I=\bb{R}^-$ es análogo).
    Al ser el término independiente $\nicefrac{1}{x}$, buscamos una solución particular de la forma:
    \begin{equation*}
        y_p(x)=\alpha + \dfrac{\beta}{x} \quad \text{con } \alpha,\beta\in \bb{R}
    \end{equation*}

    Calculamos las derivadas:
    \begin{align*}
        y_p'(x)&=\dfrac{-\beta}{x^2}\qquad 
        y_p''(x)&=\dfrac{2\beta}{x^3}
    \end{align*}

    Por tanto, que sea solución de la ecuación dada implica que:
    \begin{align*}
        \dfrac{2\beta}{x^3}+\dfrac{2}{x}\cdot \dfrac{-\beta}{x^2}+\alpha+\dfrac{\beta}{x}&=\dfrac{1}{x}\\
        \cancel{\dfrac{2\beta}{x^3}}-\cancel{\dfrac{2\beta}{x^3}}+\alpha+\dfrac{\beta}{x}&=\dfrac{1}{x}
    \end{align*}

    Veamos en primer lugar que $1,\nicefrac{1}{x}$ son linealmente independientes. Para ello, calculamos su Wronskiano:
    \begin{equation*}
        W(1,\nicefrac{1}{x})=\begin{vmatrix}
            1 & \nicefrac{1}{x}\\
            0 & -\nicefrac{1}{x^2}
        \end{vmatrix}=-\nicefrac{1}{x^2}\neq 0
    \end{equation*}

    Por tanto, tenemos que $1,\nicefrac{1}{x}$ son linealmente independientes, por lo que la igualdad anterior se traduce en:
    \begin{equation*}
        \left\{
        \begin{aligned}
            \alpha&= 0\\
            \beta &= 1
        \end{aligned}
        \right.
    \end{equation*}

    Por tanto, tenemos que una solución particular de la ecuación dada es:
    \begin{equation*}
        y_p(x)=\dfrac{1}{x}
    \end{equation*}

    Por otro lado, comprobemos que las soluciones de la homogénea dadas son linealmente independientes. Para ello, calculamos su Wronskiano:
    \begin{align*}
        W\left(\dfrac{\sen x}{x},\dfrac{\cos x}{x}\right)(x)&=\begin{vmatrix}
            \dfrac{\sen x}{x} & \dfrac{\cos x}{x}\\
            \dfrac{x\cos x-\sen x}{x^2} & \dfrac{-x\sen x-\cos x}{x^2}
        \end{vmatrix}
        =\\&=\dfrac{-x\sen^2x -\sen x\cos x}{x^3}-\dfrac{x\cos^2x-\sen x\cos x}{x^3}
        =\\&=\dfrac{-x(\sen^2x+\cos^2x)}{x^3}
        = \dfrac{-1}{x^2}\neq 0
    \end{align*}

    Por tanto, tenemos que $\nicefrac{\sen x}{x},\nicefrac{\cos x}{x}$ son linealmente independientes. Por tanto, la solución general de la ecuación dada es:
    \begin{equation*}
        y(x)=c_1\cdot\frac{\sen x}{x}+c_2\cdot \dfrac{\cos x}{x}+\dfrac{1}{x}
        \qquad \text{con } x\in \bb{R}^+,c_1,c_2\in \bb{R}
    \end{equation*}
\end{ejercicio}

\begin{ejercicio}\label{ej:4.4}
    Se considera la ecuación
    \[
        (1 + t)x'' - (1 + 2t)x' + tx = t e^t.
    \]
    Se pide:
    \begin{enumerate}
        \item Comprueba que $z_0(t) = e^t$ es una solución particular de la ecuación homogénea.
        
        El dominio de la ecuación es $\bb{R}^2$, y $z_0\in C^2(\bb{R})$. Por tanto, $z_0$ puede ser solución; veamos si lo es:
        \begin{equation*}
            L[z_0]=e^t(1+t-1-2t+t)=e^0\cdot 0=0
        \end{equation*}
        Por tanto, $z_0$ es solución de la ecuación homogénea.
        \item Efectúa el cambio $x = uz_0$ en la ecuación completa para reducir su orden y poder integrarla.
        
        En primer lugar, tenemos que:
        \begin{align*}
            x' &= u'z_0 + uz_0'=e^t(u'+u)\\
            x'' &= u''z_0 + 2u'z_0' + uz_0'' = e^t(u''+2u'+u)
        \end{align*}

        Por tanto, sustituyendo en la ecuación dada, tenemos que:
        \begin{align*}
            (1+t)e^t(u''+2u'+u)-(1+2t)e^t(u'+u)+te^t u &= te^t\\
            (1+t)(u''+2u'+u)-(1+2t)(u'+u)+tu &= t\\ 
            (1+t)u''+(2(1+t)-(1+2t))u' + (1+t-(1+2t)+t)u &= t\\
            (1+t)u''+u' &= t
        \end{align*}

        Establecemos ahora otro cambio de variable $y=u'$, de modo que la ecuación anterior se convierte en:
        \begin{equation*}
            (1+t)y'+y=t
            \Longrightarrow
            y'=\dfrac{t-y}{1+t} \qquad \text{con dominio } \Omega=\begin{cases}
                \Omega_-=\left]-\infty,-1\right[\\
                \Omega_+=\left]-1,\infty\right[
            \end{cases}
        \end{equation*}

        Para resolverla, trabajaremos con $\Omega_+$, pero el procedimiento es análogo para $\Omega_-$.
        Esta es una ecuación lineal completa, que por el Capítulo 2 sabemos resolver. Por tanto, tenemos que:
        \begin{align*}
            y(t)&=\exp\left(\int \dfrac{-1}{1+t}dt\right)\left(C'+\int \dfrac{t}{1+t}\exp\left(-\int \dfrac{-1}{1+s}ds\right)dt\right)\\
            &= \exp(-\ln(1+t))\left(C'+\int \dfrac{t}{1+t}\cdot (1+t)dt\right)\\
            &= \frac{1}{1+t}\left(C'+\int tdt\right)
            = \frac{1}{1+t}\left(C'+\frac{t^2}{2}\right)
            = \frac{C'}{1+t}+\frac{t^2}{2(1+t)}
        \end{align*}

        Por tanto, tenemos que:
        \begin{align*}
            u(t)&=\int y(t)dt=\int \left(\frac{C'}{1+t}+\frac{t^2}{2(1+t)}\right)dt\\
            &= C'\int \frac{1}{1+t}dt + \frac{1}{2}\int \frac{t^2}{1+t}dt
        \end{align*}

        Realizamos la división de polinomios del segundo caso:
        \polyset{style=C, div=:, vars=t}
        $$\polylongdiv{t^2}{1+t}$$

        Por tanto, tenemos que:
        \begin{align*}
            u(t)&=C'\ln(1+t)+\frac{1}{2}\int \left(t-1+\frac{1}{1+t}\right)dt\\
            &= C'\ln(1+t)+\frac{1}{2}\left(\frac{t^2}{2}-t+\ln(1+t)+D'\right)\\
            &= C'\ln(1+t)+\frac{t^2}{4}-\frac{t}{2}+\frac{1}{2}\ln(1+t)+D\\
            &= \left(C'+\frac{1}{2}\right)\ln(1+t)+\frac{t^2}{4}-\frac{t}{2}+D\\
            &= C\ln(1+t)+\frac{t^2}{4}-\frac{t}{2}+D
        \end{align*}

        Por tanto, tenemos que:
        \begin{align*}
            x(t)&=u(t)e^t\\
            &= \left(C\ln(1+t)+\frac{t^2}{4}-\frac{t}{2}+D\right)e^t
            \qquad \text{con } t\in \Omega_+
        \end{align*}

    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}\label{ej:4.5}
    Consideremos la ecuación
    \[
        x^2 y'' - 7xy' + 16y = 0.
    \]
    Encuentra una solución particular de tipo potencia ($y_1(x) = x^m$) y usa la fórmula de Liouville para encontrar la solución general.\\

    Supongamos $y_1(x)=x^m$ una solución de la ecuación dada. Entonces, tenemos que:
    \begin{align*}
        L[y_1]=0 & \Longleftrightarrow x^2m(m-1)x^{m-2}-7xm x^{m-1}+16x^m=0\\
        & \Longleftrightarrow m(m-1)x^m-7mx^m+16x^m=0\\
        & \Longleftrightarrow x^m(m^2-8m+16)=0\\
        & \Longleftrightarrow x^m(m-4)^2=0
    \end{align*}

    Por tanto, para $m=4$ tenemos que $y_1(x)=x^4$ es solución de la ecuación dada.
    % // TODO: Fórmula de Liouville
\end{ejercicio}

\begin{ejercicio}\label{ej:4.6}
    Fijado $I\subset \bb{R}$, sean $\varphi_1(t), \varphi_2(t), \ldots, \varphi_k(t)\in C^k(I)$ que cumplen:
    $$W(\varphi_1, \varphi_2, \ldots, \varphi_k)(t) \neq 0~\forall t \in I$$
    Demuestra que existe una ecuación lineal homogénea
    \[
        x^{(k)} + a_{k-1}(t)x^{(k-1)} + \cdots + a_1(t)x' + a_0(t)x = 0
    \]
    con $a_{k-1}, \ldots, a_1, a_0 \in C(I)$ tal que $\varphi_1(t), \varphi_2(t), \ldots, \varphi_k(t)$ es un sistema fundamental. ¿Es cierta esta conclusión cuando $W(\varphi_1, \varphi_2, \ldots, \varphi_k)(t) = 0$ para cada $t \in I$?

    Como su Wronskiano es distinto de cero para algún $t\in I$, entonces $\{\varphi_1,\varphi_2,\ldots,\varphi_k\}$ son linealmente indepentientes en $\bb{F}(I,\bb{R})$. Construyamos ahora la ecuación diferencial. Como $\varphi_i\in \ker L$ para $i=1,2,\ldots,k$, entonces tenemos que:
    \begin{equation*}
        \varphi_i^{(k)} + a_{k-1}(t)\varphi_i^{(k-1)} + \cdots + a_1(t)\varphi_i' + a_0(t)\varphi_i = 0 \quad \forall i=1,2,\ldots,k
    \end{equation*}

    Por tanto, definimos el siguiente sistema:
    \begin{equation*}
        M=\begin{pmatrix}
            \varphi_1 & \varphi_1' & \cdots & \varphi_1^{(k-2)} & \varphi_1^{(k-1)}\\
            \varphi_2 & \varphi_2' & \cdots & \varphi_2^{(k-2)} & \varphi_2^{(k-1)}\\
            \vdots & \vdots & \ddots & \vdots & \vdots\\
            \varphi_k & \varphi_k' & \cdots & \varphi_k^{(k-2)} & \varphi_k^{(k-1)}
        \end{pmatrix}
        \qquad
        a=\begin{pmatrix}
            a_{k-1}\\
            a_{k-2}\\
            \vdots\\
            a_1\\
            a_0
        \end{pmatrix}
        \qquad
        b=\begin{pmatrix}
            -\varphi_1^{(k)}\\
            -\varphi_2^{(k)}\\
            \vdots\\
            -\varphi_k^{(k)}
        \end{pmatrix}
    \end{equation*}
    
    Por tanto, obtener los coeficientes $a$ es equivalente a resolver el sistema:
    \begin{equation*}
        M(t)a(t)=b(t) \quad \forall t\in I
    \end{equation*}

    Tenemos que:
    \begin{equation*}
        |M|=|M^t|=W(\varphi_1,\varphi_2,\ldots,\varphi_k)\neq 0
    \end{equation*}

    Por tanto, dicho sistema tiene solución única, existiendo entonces los coeficientes $a_{k-1},a_{k-2},\ldots,a_1,a_0$ buscados.
    No obstante, es necesario demostrar que $a_i\in C(I)$ para cada $i=0,1,\ldots,k-1$, algo que obtenemos al resolver el sistema mediante Cramer, ya que el determinante es una función continua.
    Por tanto, se tiene que existe una ecuación lineal homogénea de forma que $\varphi_1, \varphi_2, \ldots, \varphi_k$ son soluciones, y al ser linealmente independientes, forman un sistema fundamental.\\

    Por otro lado, si $W(\varphi_1, \varphi_2, \ldots, \varphi_k)(t) = 0$ para cada $t \in I$, entonces, suponiendo que existiese dicha ecuación lineal homogénea de la cual fueran solución, tendríamos que no son linealmente independientes, por lo que no formarían un sistema fundamental.
    Por tanto, en este segundo caso no se cumpliría la conclusión.
\end{ejercicio}

\begin{ejercicio}\label{ej:4.7}
    Se considera la ecuación
    \[
        y' + y^2 + \alpha(t)y + \beta(t) = 0
    \]
    donde $\alpha, \beta : I \to \bb{R}$ son funciones continuas. Dada una solución $y(t)$ definida en un intervalo abierto $J \subset I$ se define
    \[
        x(t) = c\cdot \exp\left(\int_{t_0}^t y(s)ds\right)\qquad \forall t \in J
    \]
    donde $c\in \bb{R}$ es una constante y $t_0 \in J$. Demuestra que $x(t)$ es solución de una ecuación lineal y homogénea de segundo orden.\\

    Como $y$ es continua en $J$, en particular lo es en cada compacto contenido en $J$, luego $x$ está bien definida. Además, por el Teorema Fundamental del Cálculo, $x\in C^1(J)$, con:
    \begin{equation*}
        x'(t)=c\exp\left(\int_{t_0}^t y(s)ds\right)y(t)=x(t)y(t)
    \end{equation*}

    Por tanto, tenemos que:
    \begin{equation*}
        x''=x'y+y'x=xy^2 + x\left(-y^2-\alpha\big|_J\cdot y-\beta\big|_J\right)=x\left(-y\alpha\big|_J-\beta\big|_J\right)
    \end{equation*}

    Por tanto, como $\alpha,\beta,y\in C(J)$, entonces $x\in C^2(J)$, y por tanto $x$ es solución de la ecuación lineal y homogénea de segundo orden siguiente:
    \begin{equation*}
        x''+x\left(y\alpha\big|_J+\beta\big|_J\right)=0 \quad \text{con dominio } J
    \end{equation*}
\end{ejercicio}

\begin{ejercicio}\label{ej:4.8}
    Se considera la ecuación $x'' + a(t)x = 0$ donde $a \in C^1(I)$.
    \begin{enumerate}
        \item Dadas dos soluciones $x_1(t)$ y $x_2(t)$ de la ecuación anterior, demuestra que la función producto $z(t) = x_1(t)x_2(t)$ es solución de la ecuación de tercer orden
        \[
            z''' + 4a(t)z' + 2a'(t)z = 0.
        \]

        Tenemos que:
        \begin{align*}
            z'&=x_1'x_2+x_1x_2'\\
            z''&=x_1''x_2+2x_1'x_2'+x_1x_2''\\
            z'''&=x_1'''x_2+x_1''x_2'+2x_1''x_2'+2x_1'x_2''+x_1'x_2''+x_1x_2'''
            =\\&= x_1'''x_2+3x_1''x_2'+3x_1'x_2''+x_1x_2'''
        \end{align*}

        donde hemos hecho uso de que, dada una solución $x$ de la primera ecuación, como $x''=-a(t)x$, entonces $x''\in C^1(I)$, y por tanto $x'''\in C(I)$, con:
        \begin{align*}
            x'''&=-a'(t)x - a(t)x'
        \end{align*}

        Por tanto, y usando que $x_1$ y $x_2$ son soluciones de la ecuación dada, tenemos que:
        \begin{align*}
            z'''&=x_1'''x_2+3x_1''x_2'+3x_1'x_2''+x_1x_2'''
            =\\&=(-a'(t)x_1-a(t)x_1')x_2+3(-a(t)x_1)x_2'+3x_1'(-a(t)x_2)+x_1(-a'(t)x_2-a(t)x_2')
            =\\&=-2a'(t)x_1x_2 -4a(t)x_1'x_2 -4a(t)x_1x_2'
            =\\&= -2a'z-4az'
        \end{align*}

        Por tanto, tenemos que $z'''+3az'+2a'z=0$, por lo que $z$ es solución de la ecuación de tercer orden dada.
        \item Se supone que $x_1(t)$ y $x_2(t)$ forman un sistema fundamental para la ecuación de segundo orden, demuestra que las funciones $x_1^2(t)$, $x_1(t)x_2(t)$, $x_2^2(t)$ forman un sistema fundamental de la ecuación de tercer orden.
        \begin{observacion}
            Prueba la identidad
            \[
                \begin{vmatrix}
                    v_1^2 & w_1^2 & v_1w_1\\
                    2v_1v_2 & 2w_1w_2 & v_2w_1 + v_1w_2\\
                    v_2^2 & w_2^2 & v_2w_2
                \end{vmatrix} = (w_1v_2 - v_1w_2)^3.
            \]
        \end{observacion}

        Por el apartado anterior, tenemos de forma directa que $x_1x_2$ es solución de la ecuación de tercer orden dada. Además, como en ningún momento hemos hecho uso de que $x_1$ y $x_2$ sean distintas, entonces $x_1^2$ y $x_2^2$ también son soluciones de la ecuación de tercer orden dada. Por tanto, tan solo nos es necesario que sean linealmente independientes.
        Para ello, calcularemos su Wronskiano:
        \begin{align*}
            W(x_1^2,x_1x_2,x_2^2)&=\begin{vmatrix}
                x_1^2 & x_1x_2 & x_2^2\\
                2x_1x_1' & x_1'x_2+x_1x_2' & 2x_2x_2'\\
                2(x_1')^2+2x_1x_1'' & x_1''x_2+2x_1'x_2'+x_1x_2'' & 2(x_2')^2+2x_2x_2''
            \end{vmatrix}
            =\\&=
            -\begin{vmatrix}
                x_1^2 & x_2^2 & x_1x_2\\
                2x_1x_1' & 2x_2x_2' & x_1'x_2+x_1x_2'\\
                2(x_1')^2+2x_1x_1'' & 2(x_2')^2+2x_2x_2'' & x_1''x_2+2x_1'x_2'+x_1x_2''
            \end{vmatrix}
            =\\&=
            -2\begin{vmatrix}
                x_1^2 & x_2^2 & x_1x_2\\
                2x_1x_1' & 2x_2x_2' & x_1'x_2+x_1x_2'\\
                (x_1')^2 & (x_2')^2 & x_1'x_2'
            \end{vmatrix}
            -\begin{vmatrix}
                x_1^2 & x_2^2 & x_1x_2\\
                2x_1x_1' & 2x_2x_2' & x_1'x_2+x_1x_2'\\
                2x_1x_1'' & 2x_2x_2'' & x_1''x_2+x_1x_2''
            \end{vmatrix}
        \end{align*}

        Usando en el primer determinante la identidad dada y en el segundo que $x_1,x_2$ son soluciones de la ecuación $x''+a(t)x=0$, tenemos que:
        \begin{align*}
            W(x_1^2,x_1x_2,x_2^2)&=-2(x_2x_1'-x_1x_2')^3
            -\begin{vmatrix}
                x_1^2 & x_2^2 & x_1x_2\\
                2x_1x_1' & 2x_2x_2' & x_1'x_2+x_1x_2'\\
                -2ax_1^2 & -2ax_2^2 & -2a(x_1x_2)
            \end{vmatrix}
        \end{align*}

        El segundo sumando es nulo por ser la tercera fila proporcional a la primera. Usando el Wroskiano de $x_1,x_2$, tenemos que:
        \begin{equation*}
            W(x_1^2,x_1x_2,x_2^2)=-2W(x_1,x_2)^3
        \end{equation*}

        Por tanto, como $W(x_1,x_2)\neq 0$ por ser $x_1,x_2$ linealmente independientes, entonces:
        \begin{equation*}
            W(x_1^2,x_1x_2,x_2^2)\neq 0
        \end{equation*}

        Por tanto, $x_1^2,x_1x_2,x_2^2$ son linealmente independientes, y como eran soluciones de la ecuación de tercer orden dada, forman un sistema fundamental.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}\label{ej:4.9}
    Demuestra que las funciones $f_j(t) = |t - j|$, $j = 1, \ldots, n$ son linealmente independientes en $I = ]0, \infty[$.
    \begin{observacion}
        Las funciones $f_j$ son derivables en cada intervalo $]0, 1[$, $]1, 2[$, $\ldots$.
    \end{observacion}

    En primer lugar, tenemos que, para cada $j=1,2,\dots,n$:
    \begin{equation*}
        f_j(t)=
        \begin{cases}
            j-t & \text{si } t<j\\
            t-j & \text{si } t\geq j
        \end{cases}
    \end{equation*}

    Por tanto, $f_j$ es derivable en $\bb{R}^+\setminus \bb{Z}$. Calculemos su derivada:
    \begin{equation*}
        f_j'(t)=
        \begin{cases}
            -1 & \text{si } t<j\\
            1 & \text{si } t\geq j
        \end{cases}\qquad \forall t\in \bb{R}^+\setminus \bb{Z}
    \end{equation*}

    Por tanto, buscamos coeficientes $c_1,c_2,\dots,c_n$ tales que:
    \begin{equation*}
        \sum_{j=1}^n c_jf_j(t)=0 \quad \forall t\in \bb{R}^+
    \end{equation*}

    Como al derivar ha de mantenerse la igualdad, tenemos que:
    \begin{equation*}
        \sum_{j=1}^n c_jf_j'(t)=0 \quad \forall t\in \bb{R}^+\setminus \bb{Z}
    \end{equation*}

    Para cada $i\in \{1,2,\dots,n\}$, consideramos $t_i\in \left]i,i+1\right[$, de modo que tenemos la siguiente ecuación:
    \begin{equation*}
        0=\sum_{j=1}^n c_jf_j'(t_i)=\sum_{j=1}^{i-1} c_jf_j'(t_i)+\sum_{j=i}^n c_jf_j'(t_i)
        = \sum_{j=1}^{i} c_j-\sum_{j=i+1}^n c_j
    \end{equation*}

    Por tanto, como para cada $i$ tenemos una ecuación de este tipo, entonces tenemos un sistema de $n$ ecuaciones lineales homogéneas con $n$ incógnitas. Su matriz de coeficientes es:
    \begin{equation*}
        A=\begin{pmatrix}
            1 & -1 & -1 & \cdots & -1\\
            1 & 1 & -1 & \cdots & -1\\
            \vdots & \vdots & \vdots & \ddots & \vdots\\
            1 & 1 & 1 & \cdots & -1\\
            1 & 1 & 1 & \cdots & 1
        \end{pmatrix}=(a_{ij})_{i,j}
        \text{ con } a_{ij}=\begin{cases}
            1 & \text{si } j\leq i\\
            -1 & \text{si } j>i
        \end{cases}
    \end{equation*}

    Restando la primera fila al resto, obtenemos:
    \begin{align*}
        |A|=1\cdot \begin{vmatrix}
            1 & -1 & -1 & -1 & \cdots & -1\\
            0 & 2 & 0 & 0 & \cdots & 0\\
            0 & 2 & 2 & 0 & \ddots & 0\\
            \vdots & \vdots & \ddots & \ddots & \ddots & 0\\
            0 & 2 & \cdots & 2 & 2 & 0 \\
            0 & 2 & \cdots & 2 & 2 & 2
        \end{vmatrix}=2^{n-1}\neq 0
    \end{align*}

    Por tanto, el sistema tiene solución única, y por tanto $f_1,f_2,\dots,f_n$ son linealmente independientes.
\end{ejercicio}

\begin{ejercicio}\label{ej:4.10} Fijado un intervalo $I\subset \bb{R}$, se pide:
    \begin{enumerate}
        \item Encuentra dos funciones $f_1, f_2 \in C^1(I)$ que sean linealmente independientes en $I$ mientras que sus derivadas son linealmente dependientes.
        
        Sean $f_1,f_2:I\to \bb{R}$ definidas por:
        \begin{equation*}
            f_1(t)=t\qquad f_2(t)=t+1
        \end{equation*}

        Tenemos que $f_1,f_2\in C^1(I)$ por ser polinómicas, con:
        \begin{equation*}
            f_1'(t)=1=f_2'(t)\qquad \forall t\in I
        \end{equation*}

        Veamos que $f_1,f_2$ son linealmente independientes calculando su Wronskiano:
        \begin{equation*}
            W(f_1,f_2)(t)=\begin{vmatrix}
                t & t+1\\
                1 & 1
            \end{vmatrix}=t-t-1=-1\neq 0 \quad \forall t\in I
        \end{equation*}

        Por tanto, como $\exists t\in I$ tal que $W(f_1,f_2)(t)\neq 0$, entonces $f_1,f_2$ son linealmente independientes. No obstante, ya que $f_1'=f_2'$, vemos que sus derivadas son linealmente dependientes.
        \item Demuestra que si $f_1, f_2, \ldots, f_n \in C^1(I)$ son funciones tales que $f_0, f_1, \ldots, f_n$ son linealmente independientes entonces $f_1', f_2', \ldots, f_n'$ son también linealmente independientes. La notación $f_0$ se emplea para la función constante $f_0(t)=1$.
        
        Como no podemos asegurar que sean derivables $k-1$ veces, no podemos considerar su Wronskiano.
        Para estudiar su independencia lineal, buscamos coeficientes $c_1,c_2,\dots,c_n$ tales que:
        \begin{equation*}
            \sum_{j=1}^n c_jf_j'(t)=0 \quad \forall t\in I
        \end{equation*}

        Integrando, obtenemos:
        \begin{equation*}
            \sum_{j=1}^n c_j\int f_j'(t)dt=0 \quad \forall t\in I
        \end{equation*}

        Por el Teorema Fundamental del Cálculo, y unificando constantes de integración en $C$, tenemos que:
        \begin{equation*}
            \sum_{j=1}^n c_jf_j(t)+C=0 \quad \forall t\in I
        \end{equation*}

        Definiendo $c_0=C$, tenemos que:
        \begin{equation*}
            \sum_{j=0}^n c_jf_j(t)=0 \quad \forall t\in I
        \end{equation*}

        Por ser $f_0,f_1,\dots,f_n$ linealmente independientes, entonces $c_j=0$ para todo $j=1,\dots,n$. Por tanto, $f_1',f_2',\dots,f_n'$ son linealmente independientes.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}\label{ej:4.11}~
    \begin{enumerate}
        \item Dada la ecuación del oscilador armónico $x'' + \omega^2 x = 0$ con $\omega > 0$, demuestra que las funciones $\cos \omega t$, $\sen \omega t$ forman un sistema fundamental.\\
        
        En primer lugar, hemos de comprobar que $x_1(t)=\cos \omega t$ y $x_2(t)=\sen \omega t$ son soluciones de la ecuación dada. Para ello, derivamos:
        \begin{equation*}
            \begin{array}{rlcrl}
                x_1'(t)&=-\omega \sen \omega t & \qquad & x_2'(t)&=\omega \cos \omega t\\
                x_1''(t)&=-\omega^2 \cos \omega t & \qquad & x_2''(t)&=-\omega^2 \sen \omega t
            \end{array}
        \end{equation*}

        Por tanto, sustituyendo en la ecuación dada, tenemos que:
        \begin{align*}
            x_1''(t)+\omega^2 x_1(t)&=-\omega^2 \cos \omega t + \omega^2 \cos \omega t=0\\
            x_2''(t)+\omega^2 x_2(t)&=-\omega^2 \sen \omega t - \omega^2 \sen \omega t=0
        \end{align*}

        Por tanto, $x_1,x_2$ son soluciones de la ecuación dada. Veamos ahora que son linealmente independientes:
        \begin{equation*}
            W(x_1,x_2)(t)=\begin{vmatrix}
                \cos \omega t & \sen \omega t\\
                -\omega \sen \omega t & \omega \cos \omega t
            \end{vmatrix}=\omega\cos^2 \omega t + \omega \sen^2 \omega t=\omega\neq 0 \quad \forall t\in \bb{R}
        \end{equation*}

        Por tanto, como $\exists t\in \bb{R}$ tal que $W(x_1,x_2)(t)\neq 0$, entonces $x_1,x_2$ son linealmente independientes, y por tanto forman un sistema fundamental.
        \item Consideramos ahora el oscilador forzado $x'' + \omega^2 x = A \sen(\Omega t) + B \cos(\Omega t)$, donde $\Omega > 0$ es un número real. Demuestra que esta ecuación admite una solución del tipo $x(t) = a \cos(\Omega t) + b \sen(\Omega t)$ si $\Omega \neq \omega$.\\
        
        Lo resolveremos mediante el principio de superposición. Sean:
        \begin{align*}
            b_1(t)&=\cos (\Omega t),\\
            b_2(t)&=\sen (\Omega t)
        \end{align*}

        Como buscamos resolver los sistemas $L[x]=b_1$ y $L[x]=b_2$, buscamos $x_1,x_2$ soluciones respectivamente, de la forma:
        \begin{align*}
            x_1(t)&=a\cos (\Omega t),\\
            x_2(t)&=b\sen (\Omega t)
        \end{align*}

        Impongamos ahora que $L[x_1]=b_1$ y $L[x_2]=b_2$. Para ello, derivamos:
        \begin{align*}
            x_1'(t)&=-a\Omega \sen (\Omega t),\\
            x_2'(t)&=b\Omega \cos (\Omega t),\\
            x_1''(t)&=-a\Omega^2 \cos (\Omega t),\\
            x_2''(t)&=-b\Omega^2 \sen (\Omega t)
        \end{align*}

        Por tanto, imponiendo que $L[x_1]=b_1$ y $L[x_2]=b_2$, tenemos que:
        \begin{align*}
            -a\Omega^2 \cos (\Omega t)+\omega^2 a\cos (\Omega t)&=\cos (\Omega t)=a\cos(\Omega t)(\omega^2-\Omega^2)\\
            -b\Omega^2 \sen (\Omega t)+\omega^2 b\sen (\Omega t)&=\sen (\Omega t)=b\sen(\Omega t)(\omega^2-\Omega^2)
        \end{align*}

        Por tanto, para que se tenga $L[x_1]=b_1$ y $L[x_2]=b_2$, es necesario que:
        \begin{equation*}
            a=b=\frac{1}{\omega^2-\Omega^2}
        \end{equation*}

        Por tanto, usando el principio de superposición, como $L[Ax_1+Bx_2]=AL[x_1]+BL[x_2]=Ab_1+Bb_2$, tenemos que una solución de la ecuación completa dada es:
        \begin{equation*}
            x(t)=Ax_1(t)+Bx_2(t)=\frac{A}{\omega^2-\Omega^2}\cos (\Omega t)+\frac{B}{\omega^2-\Omega^2}\sen (\Omega t)
        \end{equation*}

        \item Resuelve la ecuación $x'' + \omega^2 x = \sum\limits_{i = 1}^n \alpha_i \sen(\Omega_i t + \varphi_i)$ cuando $\omega \neq \Omega_i$ para cada $i\in \{1,\dots,n\}$.
        
        Para cada $i\in \{1,\dots,n\}$, consideramos la ecuación:
        \begin{equation*}
            x'' + \omega^2 x = \alpha_i \sen(\Omega_i t + \varphi_i)
        \end{equation*}

        Por el apartado anterior, sabemos que una ecuación particular de la ecuación anterior es:
        \begin{equation*}
            x_i(t)=\frac{\alpha_i}{\omega^2-\Omega_i^2}\sen(\Omega_i t + \varphi_i)
        \end{equation*}

        Por tanto, por el principio de superposición, una solución de la ecuación dada es:
        \begin{equation*}
            x(t)=\sum_{i=1}^n x_i(t)=\sum_{i=1}^n \frac{\alpha_i}{\omega^2-\Omega_i^2}\sen(\Omega_i t + \varphi_i)
        \end{equation*}

        Por tanto, la solución general de la ecuación dada es:
        \begin{equation*}
            x(t)= c_1\cos(\omega t)+c_2\sen(\omega t)+
            \sum_{i=1}^n \frac{\alpha_i}{\omega^2-\Omega_i^2}\sen(\Omega_i t + \varphi_i),\quad \forall t\in \bb{R},\quad c_1,c_2\in \bb{R}
        \end{equation*}
        \item ¿Cómo son las soluciones en el caso $\Omega_i = \omega$ para algún $i$?
        
        % // TODO: Completar
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}\label{ej:4.12}
    Se considera la ecuación
    \[
        x^{(k)} + a_{k-1}x^{(k-1)} + \cdots + a_1x' + a_0x = \sum_{i = 1}^n A_i e^{\lambda_i t}
    \]
    donde $a_0, \ldots, a_{k-1}$ y $\lambda_1, \ldots, \lambda_n$ son constantes. Encuentra la condición necesaria y suficiente para que la ecuación admita una solución del tipo $x(t) = \sum\limits_{i = 1}^n c_i e^{\lambda_i t}$.\\

    Para el razonamiento a seguir, es necesario considerar $\lm_i\neq \lm_j$ para $i\neq j$. En caso contrario, tendremos $n'$ sumandos distintos, donde $n'<n$, y los coeficientes $A_i$ se pueden reagrupar para obtener un único sumando con coeficiente $A_i+A_j$.\\

    Veamos en primer lugar que $x(t)$ puede ser solución. Como es suma de funciones de clase $C^{\infty}$, entonces $x\in C^{\infty}(\bb{R})$. Consideramos la derivada de orden $j$ de $x(t)$:
    \begin{equation*}
        x^{(j)}(t)=\sum_{i=1}^n c_i\lambda_i^j e^{\lambda_i t}
        \qquad \forall j=0,1,\dots,k
    \end{equation*}

    Por tanto, sustituyendo en la ecuación dada (definiendo $a_k=1$), tenemos que una condición necesaria y suficiente para que $x(t)$ sea solución es:
    \begin{equation*}
        \sum_{j=0}^k a_{j}x^{(j)}(t)
        =\sum_{i = 1}^n A_i e^{\lambda_i t}
        \Longleftrightarrow
        \sum_{j=0}^k \sum_{i=1}^n a_{j}c_i\lambda_i^j e^{\lambda_i t}
        =\sum_{i = 1}^n A_i e^{\lambda_i t}
        \Longleftrightarrow
        \sum_{i=1}^n c_ie^{\lambda_i t}\sum_{j=0}^k a_{j}\lambda_i^j 
        =\sum_{i = 1}^n A_i e^{\lambda_i t}
    \end{equation*}

    Como $\{e^{\lambda_1 t},e^{\lambda_2 t},\dots,e^{\lambda_n t}\}$ es linealmente independiente, entonces los coeficientes de los términos exponenciales han de ser iguales, es decir:
    \begin{equation*}
        c_i\sum_{j=0}^k a_{j}\lambda_i^j = A_i \quad \forall i=1,2,\dots,n
    \end{equation*}

    Por tanto, una condición necesaria y suficiente para que la ecuación admita una solución del tipo $x(t) = \sum\limits_{i = 1}^n c_i e^{\lambda_i t}$ es:
    \begin{equation*}
        c_i=\dfrac{A_i}{\sum\limits_{j=0}^k a_{j}\lambda_i^j} \qquad \forall i=1,2,\dots,n
    \end{equation*}
\end{ejercicio}