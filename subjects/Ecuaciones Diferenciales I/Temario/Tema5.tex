\newpage
\chapter{Sistemas Lineales}

\begin{notacion}
    Por comodidad, a lo largo de la sección notaremos al conjunto de matrices de orden $n\times m$ sobre $\mathbb{R}$ por:
    \begin{equation*}
        \mathbb{R}^{n\times m} = M_{n\times m}(\mathbb{R})
    \end{equation*}
\end{notacion}

\noindent
Estudiaremos sistemas lineales de primer orden, es decir, ecuaciones de la forma:
\begin{equation}\label{eq:sel_1orden}
    x' = A(t) x + b(t)
\end{equation}
Con $A:I\rightarrow\mathbb{R}^{d\times d}$ y $b:I\rightarrow\mathbb{R}^d$, funciones continuas\footnote{Recordemos que esto significa que sean continuas coordenada a coordenada.} en un intervalo abierto $I\subseteq \mathbb{R}$. Si notamos por $A = {(a_{ij})}_{i,j}$, $d = (b_i)$ y $x = (x_i)$ a las correspondientes coordenadas de $A$, $b$ y $x$, podemos reescribir~(\ref{eq:sel_1orden}) en forma de sistema, como:
\begin{equation}\label{eq:sistema_1orden}
    \left\{\begin{array}{ccccccccc}
            x_1' & = & a_{11}(t)x_1 & + & \cdots & + & a_{1d}(t)x_d & + & b_1(t) \\
            x_2' & = & a_{21}(t)x_1 & + & \cdots & + & a_{2d}(t)x_d & + & b_2(t) \\
            \vdots & & \vdots & & \ddots & & \vdots & & \vdots \\
            x_d' & = & a_{d1}(t)x_1 & + & \cdots & + & a_{dd}(t)x_d & + & b_d(t) \\
    \end{array}\right.
\end{equation}

\begin{ejemplo}
    Supongamos que estamos en la situación de la Figura~\ref{fig:muelles}, con dos masas $m_1$ y $m_2$, y dos muelles con constantes elásticas $k_1$ y $k_2$. Supongamos además que a la masa $m_2$ se le aplica una fuerza $F(t)$.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        % Dibuja la pared
        \draw[thick] (0,0.5) -- (0,1.5);

        % Dibuja el primer muelle
        \draw[thick, decoration={aspect=0.3, segment length=4mm, amplitude=3mm, coil}, decorate] (0,1) -- (4,1);
        % Etiqueta para el primer muelle
        \node[above] at (2,1.2) {$k_1$};

        % Dibuja la primera masa (cuadrado)
        \draw[fill=gray!30] (4,0.75) rectangle (4.5,1.25);
        % Etiqueta para la primera masa
        \node[below] at (4.25,0.75) {$m_1$};

        % Dibuja el segundo muelle (más ancho)
        \draw[thick, decoration={aspect=0.3, segment length=4mm, amplitude=4mm, coil}, decorate] (4.5,1) -- (8,1);
        % Etiqueta para el segundo muelle
        \node[above] at (6.25,1.3) {$k_2$};

        % Dibuja la segunda masa (cuadrado)
        \draw[fill=gray!30] (7.9,0.65) rectangle (8.6,1.35);
        % Etiqueta para la segunda masa
        \node[below] at (8.25,0.65) {$m_2$};

        % Dibuja un vector horizontal desde la masa derecha
        \draw[-{Latex[length=3mm, width=2mm]}, thick] (8.6,1) -- (9.6,1);
        % Etiqueta para el vector
        \node[above] at (9.1,1) {$F(t)$};
    \end{tikzpicture}
    \caption{Dos masas conectadas por muelles.}
    \label{fig:muelles}
\end{figure}
    Describiremos este sistema de forma matemática describiendo $x_1$, la distancia de la masa $m_1$ a su posición de equilibrio; y $x_2$, la distancia de la masa $m_2$ a su posición de equilibrio a lo largo del tiempo $t$.\\

    Suponiendo que inicialmente (en el instante $t_0$) el primer muelle está dilatado (es decir, $x_1(t_0) > 0$) y que el segundo muelle está contraido ($x_2(t_0) - x_1(t_0)<0$), aplicando las leyes de Newton, llegamos al sistema:
    \begin{equation*}
        \left\{\begin{array}{rcl}
                m_1 x_1 '' &=& -k_1 x_1 + k_2 (x_2 - x_1) \\
                m_2 x_2 '' &=& -k_2(x_2 - x_1) + F(t)
        \end{array}\right.
    \end{equation*}
    La máquina descrita sigue estas ecuaciones diferenciales, que no están en la categoría que nos interesa, por ser de segundo orden. Sin embargo, un sistema lineal de cualquier orden se puede hacer siempre de primer orden. Para ello, buscamos transformar dos ecuaciones de segundo orden en 4 ecuaciones de primer orden.\\

    \noindent
    El truco para cambiar orden por dimensión es llamar incógnita a las derivadas. Definimos:
    \begin{equation*}
        y_1 = x_1 \qquad y_2 = x_1' \qquad y_3 = x_2 \qquad y_4 = x_2'
    \end{equation*}
    De esta forma:
    \begin{equation*}
        \left\{\begin{array}{rl}
                y_1' &= y_2 \\
            y_2' &= \dfrac{-k_1}{m_1} y_1 + \dfrac{k_2}{m_1} (y_3-y_1) \\
            y_3' &= y_4 \\
            y_4' &= \dfrac{-k_2}{m_2}(y_3-y_1) + \dfrac{F(t)}{m_2}
        \end{array}\right.
    \end{equation*}
    Obtenemos ya un sistema de ecuaciones lineal de primer orden. Los físicos dicen que hemos pasado del espacio de las configuraciones al espacio de estados.\\

    Tenemos:
    \begin{equation*}
        A(t) = \left(\begin{array}{cccc}
                0 & 1 & 0 & 0 \\
                -\left(\frac{k_1+k_2}{m_1}\right) & 0 & \frac{k_2}{m_2} & 0 \\
                0 & 0 & 0 & 1 \\
                \frac{k_2}{m_2} & 0 & \frac{-k_2}{m_2} & 0
        \end{array}\right) \qquad b(t) = \left(\begin{array}{c}
            0 \\
            0 \\
            0 \\
            \dfrac{F(t)}{m_2}
        \end{array}\right)
    \end{equation*}
    Este cambio se puede aplicar siempre que queramos, y esta es la razón por la que en este capítulo solo estudiaremos sistemas de ecuaciones lineales de primer orden, porque sabiendo resolverlo sabemos resolver cualquier sistema lineal de orden superior.
\end{ejemplo}

\begin{teo}[Existencia y unicidad de las soluciones]\label{teo:existencia_unicidad_sistemas}
    Dados $t_0\in I$, $x_0\in \mathbb{R}^d$, existe una única solución del sistema:
    \begin{equation*}
        x' = A(t)x + b(t) \qquad x(t_0) = x_0
    \end{equation*}
    definida en \textbf{todo} el intervalo $I$.
\end{teo}
\noindent
Para su demostración, será necesario repasar varios conceptos ya vistos en otras asignaturas.

\begin{coro}
Ahora, si tenemos una ecuación lineal de orden superior:
\begin{equation*}
    x^{(k)} + a_{k-1}(t) x^{(k-1)} + \cdots + a_1(t)x' + a_0(t)x = b(t)
\end{equation*}
Lo que hacemos es tomar como incógnitas:
\begin{equation*}
    y_1 = x \qquad y_2 = x' \qquad \ldots \qquad y_k = x^{(k-1)}
\end{equation*}
Y plantear el sistema:
\begin{equation*}
    \left\{\begin{array}{rcl}
            y_1' &=& y_2 \\
            y_2' &=& y_3 \\
            \vdots && \vdots \\
            y_{k-1}' &=& y_k \\
            y_k' &=& -a_0(t)y_1 -a_1(t) y_2 - \cdots - a_{k-1}(t)y_k + b(t)
    \end{array}\right.
\end{equation*}
Con lo que el Teorema de existencia y unicidad del Capítulo anterior es un corolario del Teorema~\ref{teo:existencia_unicidad_sistemas}.
\end{coro}

\subsubsection{Normas matriciales}
Dada cualquier norma\footnote{Recordamos que una norma es cualquier función que cumpla la desigualdad triangular, homogeneidad por homotecias y no degeneración.} $\|\cdot \|:\mathbb{R}^d\rightarrow\mathbb{R}$, esta nos permite definir una norma matricial $\|\cdot \|:\mathbb{R}^{d\times d}\rightarrow\mathbb{R}$, dada por:
\begin{equation*}
    \|A\| = \max\{\|Ax\| \mid \|x\|\geq 1\} \qquad \forall A\in \mathbb{R}^{d\times d}
\end{equation*}
Notemos que está bien definida\footnote{Que en realidad existe un máximo.}, ya que lo que hacemos es considerar la función $x\longmapsto \|Ax\|$ (que es continua) definida en la bola unidad junto con su frontera (que es un conjunto compacto), por lo que la imagen de un compacto es un compacto y al estar en $\mathbb{R}$, es un conjunto cerrado y acotado.\\

De forma geométrica, cada $A$ es una transformación del espacio $\mathbb{R}^d$ en sí mismo. Lo que hacemos para calcular su norma es calcular las imágenes de todos los vectores de la bola unidad (junto con su frontera) y quedarnos con la mayor norma de todos ellos. Si consideramos la norma vectorial euclídea, lo que hacemos es coger la mayor distancia al origen.

\begin{ejemplo}
    Considerando el espacio normado $(\mathbb{R}^2, \|\cdot \|_2)$, si tomamos:
    \begin{equation*}
        A = \left(\begin{array}{cc}
                2 & 0 \\
                0 & \nicefrac{1}{2}
        \end{array}\right) \in \mathbb{R}^{2\times 2}
    \end{equation*}
    La aplicación lineal asociada a $A$ transforma $\bb{S}^1$ en una elipse de eje mayor 2 y eje menor $\nicefrac{1}{2}$, tal y como vemos en la Figura~\ref{fig:S1_fA}, con lo que $\|A\| = 2$.

    \begin{figure}[H]
        \centering
\begin{tikzpicture}[scale=1]
% Ejes de coordenadas originales
\draw[-Stealth] (-1.5,0) -- (1.5,0) node[anchor=north] {$x$};
\draw[-Stealth] (0,-1.5) -- (0,1.5) node[anchor=east] {$y$};

% Circunferencia inicial
\draw[thick,gray!90] (0,0) circle(1);

% Flecha de transformación curva (menos arqueada)
\draw[-Stealth,thick] 
    (1.2,0.2) .. controls (2.3,0.8) and (3.5,0.8) .. (4.5,0.5)
    node[midway,above] {$f_A$};

% Ejes de coordenadas transformados
\begin{scope}[shift={(6,0)}] % Aumentar la separación
    \draw[-Stealth] (-2.5,0) -- (2.5,0) node[anchor=north] {$x$};
    \draw[-Stealth] (0,-1.5) -- (0,1.5) node[anchor=east] {$y$};

    % Elipse transformada
    \draw[thick,gray!90] (0,0) ellipse (2 and 0.5);
\end{scope}
\end{tikzpicture}        
\caption{Transformación de $\bb{S}^1$ por la aplicación lineal asociada a $A$.}
\label{fig:S1_fA}
    \end{figure}
\end{ejemplo}

\noindent
Las normas matriciales así definidas tienen más propiedades que las normas vectoriales de las que provienen, que ya fueron vistas en Métodos Numéricos\footnote{Diríjase a dichos apuntes para consultar la demostración de la siguiente Proposición.} I\@:
\begin{prop}
    Se verifica que:
    \begin{enumerate}
        \item $\|I\| = 1$.
        \item $\|AB\| \leq \|A\|\|B\|$, $\forall A,B\in \mathbb{R}^{d\times d}$.
        \item $\|Ax\| \leq \|A\|\|x\|$, $\forall x\in \mathbb{R}^d, A\in \mathbb{R}^{d\times d}$.
    \end{enumerate}
    \begin{proof}
        Demostramos cada igualdad:
        \begin{enumerate}
            \item Evidente.
            \item A partir de la definición, sean $A,B\in \mathbb{R}^{d\times d}$:
                \begin{align*}
                    \|AB\| &= \min\{\|ABu\| \mid \|u\| = 1\} \leq \min\{\|A\|\|Bu\| \mid \|u\| = 1\} \\
                        &\leq \min\{\|A\|\|B\|\|u\| \mid \|u\| = 1\} = \|A\|\|B\|
                \end{align*}
            \item A partir de la definición de $\|A\|$, sabemos que $\|Au\| \leq \|A\|$ para cualquier $u\in \bb{S}^1$ y $A\in \mathbb{R}^{d\times d}$. De esta forma:
                \begin{equation*}
                    \left\|A \dfrac{x}{\|x\|}\right\| = \dfrac{1}{\|x\|} \cdot \|Ax\| \leq \|A\|
                \end{equation*}
                con lo que:
                \begin{equation*}
                    \|Ax\| \leq \|A\|\|x\|
                \end{equation*}
        \end{enumerate}
    \end{proof}
\end{prop}

\subsubsection{Integrales vectoriales}
Supongamos que tenemos $f:[a,b]\rightarrow \mathbb{R}^d$ una función continua en un intervalo compacto, con lo que $f$ tiene $d$ coordenadas: $f=(f_1,\ldots,f_d)$, todas ellas continuas. De esta forma, podemos definir la integral de $f$ como el vector formado por las integrales de sus componentes
\begin{equation*}
    \int_{a}^{b} f(t)~dt  = \left(\begin{array}{c}
        \displaystyle\int_{a}^{b} f_1(t)~dt  \\
        \displaystyle\int_{a}^{b} f_2(t)~dt  \\
        \vdots \\
        \displaystyle\int_{a}^{b} f_d(t)~dt  
    \end{array}\right)
\end{equation*}

\begin{prop}
    Sea $A\in \mathbb{R}^{d\times d}$, entonces:
    \begin{equation*}
        A\cdot \left(\int_{a}^{b} f(t)~dt \right) = \int_{a}^{b} [A\cdot f(t)]~dt
    \end{equation*}
    \begin{proof}
        Si notamos a las coordenadas de $A$ por $A={(a_{ij})}_{i,j}$:

        \begin{align*}
            A \left(\int_{a}^{b} f(t)~dt \right) &= 
            \left(\begin{array}{cccc}
                a_{11} & a_{12} & \cdots & a_{1d} \\
                a_{21} & a_{22} & \cdots & a_{2d} \\
                \vdots & \vdots & \ddots & \vdots \\
                a_{d1} & a_{d2} & \cdots & a_{dd} \\
            \end{array}\right) 
            \left(\begin{array}{c}
                \displaystyle\int_{a}^{b} f_1(t)~dt  \\
                \displaystyle\int_{a}^{b} f_2(t)~dt  \\
                \vdots \\
                \displaystyle\int_{a}^{b} f_d(t)~dt  
            \end{array}\right) = \left(\begin{array}{c}
                \displaystyle\sum_{j=1}^{d} \left(a_{1j}  \int_{a}^{b} f_{j}(t)~dt \right) \\
                \displaystyle\sum_{j=1}^{d} \left(a_{2j}  \int_{a}^{b} f_{j}(t)~dt \right) \\
                \vdots \\
                \displaystyle\sum_{j=1}^{d} \left(a_{dj}  \int_{a}^{b} f_{j}(t)~dt \right) 
            \end{array}\right) \\
                 &= \left(\begin{array}{c}
                     \displaystyle \int_{a}^{b} \left(\sum_{j=1}^{d}a_{1j} \cdot f_j(t)\right)~dt  \\
                     \displaystyle \int_{a}^{b} \left(\sum_{j=1}^{d}a_{2j} \cdot f_j(t)\right)~dt  \\
                     \vdots \\
                     \displaystyle \int_{a}^{b} \left(\sum_{j=1}^{d}a_{dj} \cdot f_j(t)\right)~dt  
             \end{array}\right) = \int_{a}^{b} [A\cdot f(t)]~dt 
        \end{align*}

    \end{proof}
\end{prop}

\begin{prop}
    Se verifica que:
    \begin{equation*}
        \left\|\int_{a}^{b} f(t)~dt \right\| \leq \int_{a}^{b} \|f(t)\|~dt 
    \end{equation*}
    Para cualquier norma.
    \begin{proof}
        Por comodidad, definimos $\Delta_m = \{1,\ldots,m\}$.

        Sea $P=\{a=x_0<x_1<\ldots<x_m=b\}$ una partición de $[a,b]$ y $\xi_j \in [x_{j-1},x_j]$ $\forall j\in \Delta_m$. Como las componentes de $f$ y la función $\|f\|$ son continuas, son integrables en el sentido de Riemann y podemos considerar las sumas de Riemann asociadas a la partición $P$ con etiquetas $\xi_j$ $\forall j\in \Delta_m$. Entonces, se cumple que:
        \begin{align*}
            \sigma(\|f\|,P) &= \sum_{j=1}^{d} \|f(\xi_j)\| (x_j-x_{j-1}) = \sum_{j=1}^{d}\|f(\xi_j)(x_j-x_{j-1})\| \geq \left\|\sum_{j=1}^{d}(f(\xi_j)(x_j-x_{j-1}))\right\| \\
                            &= \left\|\sum_{j=1}^{d}\left(\sum_{k=1}^{d}(f_k(\xi_j)(x_j-x_{j-1}))e_k\right)\right\|= \left\|\sum_{k=1}^{d}\left(\sum_{j=1}^{d}(f_k(\xi_j)(x_j-x_{j-1}))e_k\right)\right\| \\
                            &= \left\|\sum_{k=1}^{d}(\sigma(f_k,P)e_k)\right\| = \left\| \left(\begin{array}{c}
                                \sigma(f_1,P) \\
                                \sigma(f_2,P) \\
                                \vdots \\
                                \sigma(f_d,P) 
                            \end{array}\right)  \right\|
        \end{align*}
        Por lo que $\sigma(\|f\|,P) \geq \|(\sigma(f_1,P), \ldots, \sigma(f_d,P))\|$ para toda partición $P$ del intervalo $[a,b]$.\\

        Por tanto, si $\{P_n\}$ es una sucesión de particiones de $[a,b]$ tales que $\{\Delta P_n\}\longrightarrow 0$ (donde $\Delta P_n$ es el diámetro de la partición $P_n$), usando que $\|f\|$ y todas las componentes de $f$ son Riemann-integrables, se tiene que:
        \begin{align*}
            \int_{a}^{b} \|f(t)\|~dt &= \lim_{n\to\infty}\sigma(\|f\|, P_n) \geq \lim_{n\to\infty} \left\| \left(\begin{array}{c}
                \sigma(f_1,P_n) \\
                \vdots  \\
                \sigma(f_d,P_n) 
            \end{array}\right) \right\|    = \left\| \left(\begin{array}{c}
                \displaystyle\lim_{n\to\infty} \sigma(f_1,P_n) \\
                \vdots  \\
                \displaystyle\lim_{n\to\infty}  \sigma(f_d,P_n) 
            \end{array}\right) \right\| \\
                                     &=  \left\| \left(\begin{array}{c}
                \int_{a}^{b} f_1(t)~dt  \\
                \vdots \\
                \int_{a}^{b} f_d(t)~dt  
            \end{array}\right) \right\| = \left\|\int_{a}^{b} f(t)~dt \right\|
        \end{align*}

        Es decir:
    \begin{equation*}
        \left\|\int_{a}^{b} f(t)~dt \right\| \leq \int_{a}^{b} \|f(t)\|~dt 
    \end{equation*}
    \end{proof}
\end{prop}

% // TODO: Clase
\subsubsection{Convergencia uniforme}
\noindent
Dada cualquier norma vectorial $\|\cdot \|:\mathbb{R}^d\rightarrow\mathbb{R}$ y fijado un intervalo $I\subseteq \mathbb{R}$, podemos definir\footnote{En este caso, no obtenemos una norma, porque puede tomar el valor $\infty$.} $\|\cdot \|_\infty:\bb{F}(\text{I},\mathbb{R}^d)\rightarrow\mathbb{R}$ dada por:
\begin{equation*}
    \|\varphi\|_\infty = \sup_{t\in I}\|\varphi(t)\| \qquad \forall \varphi \in \bb{F}(\text{I},\mathbb{R}^d)
\end{equation*}

\begin{definicion}[Norma del máximo]
    Recordamos la definición de la norma vectorial del máximo, $\|\cdot \|_{\max}:\mathbb{R}^d\rightarrow\mathbb{R}$ dada por
    \begin{equation*}
        \|x\|_{\max} = \max\{|x_1|,|x_2|\} \qquad \forall x=(x_1,x_2)\in \mathbb{R}^2
    \end{equation*}
    Que en algunos contextos se denota también por $\|\cdot \|_1$.
\end{definicion}

\begin{ejemplo}
    En $(\mathbb{R}^2, \|\cdot \|_{\max})$:
    \begin{enumerate}
        \item Sea $\varphi_1:\left]0,1\right[\rightarrow\mathbb{R}^2$ dada por:
            \begin{equation*}
                \varphi_1(t) = \left(\begin{array}{c}
                    \cos t \\
                    e^t
                \end{array}\right) \qquad t\in \left]0,1\right[
            \end{equation*}
            Tenemos que $\|\varphi\| = e$.
        \item Sea ahora $\varphi_2:\left]0,1\right[\rightarrow\mathbb{R}^2$ dada por:
            \begin{equation*}
                \varphi_2(t) = \left(\begin{array}{c}
                        \nicefrac{1}{t} \\
                        t
                \end{array}\right) \qquad t\in \left]0,1\right[
            \end{equation*}
            Tenemos que $\|\varphi\| = \infty$.
    \end{enumerate}
\end{ejemplo}

\begin{definicion}
    Dada una sucesión de funciones $\{f_n\}$ y una función $f$, todas ellas definidas sobre un mismo intervalo $I$, decimos que $\{f_n\}$ converge uniformemente a $f$ si:
    \begin{equation*}
        \|f_n - f\|_\infty \rightarrow 0
    \end{equation*}
\end{definicion}

\begin{ejemplo}
    Dadas $f,\psi \in \bb{F}(\text{I},\mathbb{R})$ y $\delta\in \mathbb{R}^+$, que:
    \begin{equation*}
        \|f-\psi\|_\infty \leq \delta
    \end{equation*}
    significa que $\psi$ no se puede separar de $f$ más que $\delta$. Podemos observar esto gráficamente en la Figura~\ref{fig:conv_unif}, donde $\psi$ debe estar en la región delimitada por las líneas discontinuas.
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            % Ejes
            \draw[-Stealth] (-0.5, 0) -- (7, 0) node[right] {$x$};
            \draw[-Stealth] (0, -0.5) -- (0, 4.5) node[above] {$y$};

            % Función f(x)
            \draw[thick, gray!90, domain=0:6, smooth, samples=100] 
                plot(\x, {sin(deg(\x)) + 2}) node[right] {$f(x)$};

            % Función f(x) + 1
            \draw[thick, dashed, blue!50, domain=0:6, smooth, samples=100] 
                plot(\x, {sin(deg(\x)) + 2.5}) node[right] {};

            % Función f(x) - 1
            \draw[thick, dashed, blue!50, domain=0:6, smooth, samples=100] 
                plot(\x, {sin(deg(\x)) + 1.5}) node[right] {};
        \end{tikzpicture}
        \caption{Región en la que debe estar $\psi$.}
        \label{fig:conv_unif}
    \end{figure}

\end{ejemplo}

\noindent
Algunas propiedades de la convergencia uniforme\footnote{que ya fueron vistas en Análisis Matemático II} son:
\begin{prop}
    Si $f_n$ son continuas y convergen uniformemente a $f$, entonces $f$ es continua.
\end{prop}

\begin{prop}
    Si $[a,b]\subseteq I$ y tenemos $f_n:I\rightarrow\mathbb{R}^d$ funciones continuas que convergen uniformemente a $f:I\rightarrow\mathbb{R}^d$, entonces:
    \begin{equation*}
        \int_{a}^{b} f_n \rightarrow \int_a^b f
    \end{equation*}
\end{prop}

Sin embargo, si $f_n$ son derivables y convergen uniformemente a $f$, entonces no podemos asegurar que $f$ sea derivable:
\begin{ejemplo}
    Sean $f_n:\mathbb{R}\rightarrow\mathbb{R}$:
    \begin{equation*}
        f_n(t) = \dfrac{\sen(nt)}{n} \qquad n\in \mathbb{N} \quad t\in \mathbb{R}
    \end{equation*}
    Tenemos que $f_n\rightarrow f$ con $f:\mathbb{R}\rightarrow\mathbb{R}$ dada por $f(t) = 0$ $\forall t\in \mathbb{R}$, ya que:
    \begin{equation*}
        \|f_n - f\|_\infty = \sup_{t\in \mathbb{R}} \left|\dfrac{\sen(nt)}{n}\right| \leq \dfrac{1}{n} \longrightarrow 0
    \end{equation*}
    Y tenemos que:
    \begin{equation*}
        f_n'(t) = \cos(nt) \qquad \forall n\in \mathbb{N}, \quad t\in \mathbb{R}
    \end{equation*}
    Que no convergen a $f'$, ya que:
    \begin{equation*}
        f_n'(\pi) = {(-1)}^{n} \not\rightarrow f'(\pi) = 0
    \end{equation*}
\end{ejemplo}

\begin{prop}[Test de Weierstrass]
    Dadas $f_n:I\rightarrow\mathbb{R}^d$, resulta que:
    \begin{equation*}
        \|f_{n+1}(t) - f_n(t)\| \leq M_n \qquad \forall t\in I, \quad \forall n\geq 0
    \end{equation*}
    Si tenemos que $\sum M_n < \infty$. Entonces, $f_n$ converge uniformemente en $I$.
\end{prop}
Este Test de Weierstrass nos permite demostrar la existencia del límite de una sucesión de funciones sin conocer la función límite.

\begin{ejemplo}
    Sabemos que:
    \begin{equation*}
        e^t = \sum_{n=0}^{\infty} \dfrac{t^n}{n!} \qquad t\in \mathbb{R}
    \end{equation*}

    Si definimos:
    \begin{equation*}
        S_n(t) = \sum_{k=0}^{n} \dfrac{t^k}{k!} \qquad \forall n\in \mathbb{N}, \quad t\in \mathbb{R}
    \end{equation*}

    \begin{enumerate}
        \item Recordando la teoría que usábamos en Análisis Matemático I sobre el radio de convergencia, vemos que el radio de convergencia de $S_n$ es infinito, por lo que podemos garantizar convergencia uniforme en cada intervalo compacto de $\mathbb{R}$, pero no en todo $\mathbb{R}$.\\

            Pensando en que los polinomios siempre divergen en $-\infty$, podemos intuir que la convergencia en todo $\mathbb{R}$ no la tenemos garantizada, ya que la función exponencial tiede a 0 en dicho límite.

    De esta forma, una serie de polinomios nunca puede converger a una función que está acotada en un intervalo no acotado.
\item De forma análoga y usando el Test de Weierstrass, podemos desmotrar la convergencia de la suesión $\{S_n\}$ en cada intervalo compacto $[a,b]$:
            \begin{equation*}
                |S_{n+1}(t) - S_n(t)| = \dfrac{t^{n+1}}{(n+1)!} \leq \dfrac{b^{n+1}}{(n+1)!} \longrightarrow 0 \qquad \forall t\in [a,b]
            \end{equation*}
    \end{enumerate}
\end{ejemplo}

\noindent
Estamos ya listos para realizar la demostración del Teorema~\ref{teo:existencia_unicidad_sistemas}:

\begin{teo}[Existencia y unicidad de las soluciones]\label{teo:existencia_unicidad_sistemas}
    Dados $t_0\in I$, $x_0\in \mathbb{R}^d$, existe una única solución del sistema:
    \begin{equation}\label{eq:teo_pvi}
        x' = A(t)x + b(t) \qquad x(t_0) = x_0
    \end{equation}
    definida en \textbf{todo} el intervalo $I$.
    \begin{proof}
        Inicialmente, demostraremos el teorema en un caso particular y veremos que el caso general se puede reducir al particular:
        \begin{itemize}
            \item Supongamos que $I$ es un intervalo acotado de longitud $l$ y que:
                \begin{equation*}
                    \|A(t)\| \leq \alpha \quad \|b(t)\|\leq \beta \qquad \forall t\in I
                \end{equation*}
                \begin{description}
                    \item [Exitencia.] Queremos llegar a que tenemos una solución $x$ del problema de valores iniciales~(\ref{eq:teo_pvi}), esta cumplirá:
                        \begin{equation*}
                            x'(t) = A(t)x(t) + b(t)
                        \end{equation*}
                        Con lo que la integraremos (vectorialmente) cogiendo $t_0\in I$ (son funciones continuas en un compacto):
                        \begin{equation*}
                            \int_{t_0}^{t} x'(s)~ds  = \int_{t_0}^{t} [A(s)x(s) + b(s)]~ds 
                        \end{equation*}
                        Aplicando coordenada a coordenada la Regla de Barrow y que $x(t_0) = x_0$:
                        \begin{equation*}
                            x(t) - x_0 = x(t) - x(t_0) = \int_{t_0}^{t} x'(s)~ds  = \int_{t_0}^{t} [A(s)x(s) + b(s)]~ds 
                        \end{equation*}
                        Y hemos llegado a una ecuación integral que cumplirá la $x$ buscada:
                        \begin{equation*}
                            x(t) = x_0 + \int_{t_0}^{t} [A(s)x(s) + b(s)]~ds 
                        \end{equation*}
                        Buscaremos soluciones aproximadas (buscamos las iterantes de Picard):
                        La primera aproximación la tomamos como la condición inicial:
                            \begin{equation*}
                                x_0(t) = x_0 \qquad t\in I
                            \end{equation*}
                            con lo que podemos definir:
                            \begin{equation*}
                                x_{n+1}(t) = x_0 + \int_{t_0}^{t} [A(s)x_n(s) + b(s)]~ds \qquad \forall n\in \mathbb{N}
 :                           \end{equation*}
                        La idea de la demostración es construir las iterantes de Picard (que están bien definidas y todas de clase $C^1(I)$). Los pasos a seguir son:
                        \begin{enumerate}
                            \item Demostraremos que las iterantes de Picard convergen uniformemente (esto será una función continua).
                            \item Una vez que sabemos que $x_n$ tienden a un límite, haciendo $n$ tender a infinito, vamos a llegar a la ecuación integral, usando para ello la comnutación de integral con convergencia uniforme.

                                El límite de Picard es una solución integral.
                            \item Probar que una solución de la ecuación integral es una solución del problema de valores iniciales.
                        \end{enumerate}

                        % // Clase 2-12
                        Comenzando ahora con la demostración, definimos las Iterantes de Picard:
                        \begin{align*}
                            x_0(t) &= x_0 \\
                            x_{n+1}(t) &= x_0 + \int_{t_0}^{t} [A(s)x_n(s)+b(s)]~ds 
                        \end{align*}
                        Con $x_n:I\rightarrow\mathbb{R}^d$ bien definidas y continuas $\forall n\in \mathbb{N}$ (hágase por inducción). Además, $x_n\in C^1(I)$ $\forall n\in \mathbb{N}$, gracias al Teorema Fundamental del Cálculo.\\

                        Veamos que $\{x_n\}$ converge uniformemente en $I$, usando para ello el Test de Weierstrass. Comenzamos acotando la primera diferencia:
                        \begin{align*}
                            \|x_1(t) - x_0(t)\| &= \left\|\int_{t_0}^{t} [A(s)x_0+b(s)]~ds \right\| \leq \left|\int_{t_0}^{t}\| A(s)x_0 + b(s)\|~ds \right| \\
                                                &\leq \left|\int_{t_0}^{t}\left[ \|A(s)\|\|x_0\| + \|b(s)\|\right]~ds \right| \leq \left|\int_{t_0}^{t} [\alpha\cdot  \|x_0\| + \beta]~ds \right| \\
                                                &\leq (\alpha\cdot \|x_0\|+\beta)\cdot l = M_0 \in \mathbb{R} \qquad \forall t\in I
                        \end{align*}
                        Ahora:
                        \begin{align*}
                            \|x_2(t) - x_1(t)\| &= \left\|\int_{t_0}^{t} [A(s)(x_2(s)-x_1(s))]~ds \right\| \leq  \left|\int_{t_0}^{t} \|A(s)(x_2(s)-x_1(s))\|~ds \right| \\
                                                &\leq \alpha\left|\int_{t_0}^{t} \|x_1(s)-x_0(s)\|~ds \right| \leq \alpha M_0 \left|\int_{t_0}^{t} ~ds \right| \leq \alpha M_0 |t-t_0|
                        \end{align*}
                        Donde hemos mantenido la dependencia de $t$, ya que si ahora decimos que $\alpha M_0 |t-t_0|\leq \alpha M_0 l = M_1$, obtendremos luego una serie $\{\sum M_n\}$ que no converja, con lo que tratamos de mantener la dependencia de $t$ hasta el final:
                        \begin{align*}
                            \|x_3(t)-x_2(t)\| &\leq \alpha \left|\int_{t_0}^{t} \|x_2(s)-x_1(s)\|~ds \right| \leq \alpha^2 M_0 \left|\int_{t_0}^{t} |s-t_0|~ds \right| \\
                                              &= \alpha^2 M_0 \dfrac{{|t-t_0|}^{2}}{2}
                        \end{align*}
                        En definitiva, se puede probar por inducción que:
                        \begin{equation*}
                            \|x_{n+1}(t) - x_n(t)\| \leq M_0 \dfrac{\alpha^n{|t-t_0|}^{n}}{n!} \qquad \forall t\in I
                        \end{equation*}
                        Definimos ahora:
                        \begin{equation*}
                            M_n = M_0\cdot  \dfrac{{(\alpha\cdot l)}^{n}}{n!} \qquad \forall n\in \mathbb{N} \cup \{0\}
                        \end{equation*}
                        Una serie conocida, con lo que:
                        \begin{equation*}
                            \sum_{n=0}^\infty M_n = M_0\cdot  e^{\alpha\cdot l} \in \mathbb{R}
                        \end{equation*}
                        Por el Test de Weierstrass, concluimos que $\{x_n\}$ converge uniformemente a una función $x:I\rightarrow\mathbb{R}^d$, que por ahora solo sabemos que es continua, por ser $x_n$ continua $\forall n\in \mathbb{N}$.

                        Veamos ahora que $x$ es solución al problema de valores iniciales.  Como:
                        \begin{equation*}
                            \|A(t)x_n(t)-A(t)x(t)\| \leq \alpha \|x_n(t)-x(t)\|
                        \end{equation*}
                        Tenemos que $\{Ax_n\}\rightarrow Ax$, con lo que:
                        \begin{equation*}
                            \int_{t_n}^{t} A(s)x_n(s)~ds  \longrightarrow \int_{t_0}^{t} A(s)x(s)~ds 
                        \end{equation*}
                        En definitiva:
                        \begin{equation*}
                            x(t) = x_0 + \int_{t_0}^{t} [A(s)x(s)+b(s)]~ds 
                        \end{equation*}
                        Aplicando el Teorema Fundamental del Cálculo, tenemos que $x\in C^1(I,\mathbb{R}^d)$:
                        \begin{equation*}
                            x'(t) = A(t)x(t) + b(t) \qquad \forall t\in I
                        \end{equation*}
                        Con lo que $x$ es solución de~(\ref{eq:teo_pvi}) y se tiene que:
                        \begin{equation*}
                            x(t_0) = x_0 + \int_{t_0}^{t_0} [A(s)x(s)+b(s)]~ds  = x_0
                        \end{equation*}
                    \item [Unicidad.] Supongamos que $x,y:I\rightarrow\mathbb{R}^d$ son ambas soluciones de~(\ref{eq:teo_pvi}). Como son soluciones, también cumplen la ecuación integral:
                        \begin{align*}
                            x(t) &= x_0 + \int_{t_0}^{t} [A(s)x(s)+b(s)]~ds \\
                            y(t) &= x_0 + \int_{t_0}^{t} [A(s)y(s)+b(s)]~ds 
                        \end{align*}
                        Restando:
                        \begin{equation*}
                            x(t) - y(t) = \int_{t_0}^{t} [A(s)(x(s)-y(s))]~ds 
                        \end{equation*}
                        Con lo que:
                        \begin{equation*}
                            \|x(t) - y(t)\| = \left\|\int_{t_0}^{t} [A(s)(x(s)-y(s))]~ds \right\| \leq \alpha\left|\int_{t_0}^{t} \|x(s)-y(s)\|~ds \right| \quad \forall t\in I
                        \end{equation*}
                        Tomando $f(t) = \|x(t)-y(t)\|$ $\forall t\in I$, tenemos una función continua no negativa que está en las hipótesis del Lema~\ref{lema:unicidad_teo}, concluimos que $f(t) = 0$ $\forall t\in I$, con lo que $x(t) = y(t)$ $\forall t\in I$.
                \end{description}
            \item De vuelta al caso general, buscamos quitar las hipótesis de que $I$ sea un intervalo acotado. Para ello, tomamos una sucesión $\{I_n\}$ de intervalos abiertos y acotados de forma que $t_0\in I_0$, $I_n\subseteq I_{n+1}$, $\overline{I_n}\subseteq I$ $\forall n\in \mathbb{N}$ y:
                \begin{equation*}
                    \bigcup_{n=0}^\infty I_n = I
                \end{equation*}
                Podemos ahora definir: 
                \begin{equation*}
                    \alpha_n = \max_{t\in \overline{I_n}} \|A(t)\| \qquad \beta_n = \max_{t\in \overline{I_n}} \|b(t)\| \qquad \forall n\in \mathbb{N}
                \end{equation*}
                Con lo que la hipótesis extra anterior se verifica en cada intervalo $I_n$.

                \begin{description}
                    \item [Unicidad.] Si $x,y:I\rightarrow\mathbb{R}^d$ son soluciones de~(\ref{eq:teo_pvi}), entonces $x_{|I_n}$ y $y_{|I_n}$ son soluciones de~(\ref{eq:teo_pvi}) en $I_n$, donde sabemos que se verifica $x(t) = y(t)$ $\forall t\in I_n$, para todo $n\in \mathbb{N}$, con lo que $x(t) = y(t)$ $\forall t\in I$.
                    \item [Existencia.] Si ahora llamamos $x_n$ a la solución del problema de valores iniciales en el intervalo $I_n$, definimos $x:I\rightarrow\mathbb{R}^d$ 
                        \begin{equation*}
                            x(t) = x_n(t) \text{\ si\ } t\in I_n
                        \end{equation*}
                        Es una función bien definida gracias a la unicidad en cada $I_n$, es derivable y cumple la ecuación diferencial porque lo es y la cumple en cada $I_n$.
                \end{description}
        \end{itemize}
    \end{proof}
\end{teo}

\begin{lema}\label{lema:unicidad_teo}
    Sea $J$ un intervalo y $f:J\rightarrow \mathbb{R}^+_0$ continua, sean $t_0\in J$, $\alpha >0$:
    \begin{equation*}
        f(t) \leq \alpha\left|\int_{t_0}^{t} f(s)~ds \right| \quad \forall t\in J \Longrightarrow f(t) = 0 \quad \forall t\in J
    \end{equation*}
    \begin{proof}
        Realizando primero la demostración en un caso más específico:
        \begin{itemize}
            \item Si $J$ es compacto, $\exists \max_{t\in J} f(t) = m$, con lo que:
                \begin{equation*}
                    f(t) \leq \alpha \cdot m\cdot  |t-t_0| \qquad \forall t\in J
                \end{equation*}
                \begin{equation*}
                    f(t) \leq \alpha \left|\int_{t_0}^{t} [\alpha \cdot m\cdot  |s-t_0|]~ds \right| \leq m\cdot  \dfrac{\alpha^2 {|t-t_0|}^{2}}{2}\qquad \forall t\in J
                \end{equation*}
                En definitiva, se puede probar por inducción que:
                \begin{equation*}
                    0\leq f(t) \leq m\cdot  \dfrac{\alpha^n {|t-t_0|}^{n}}{n!} \qquad \forall t\in J
                \end{equation*}
                Como sabemos que la serie de dichos términos converge, sabemos que la sucesión tiende a 0, luego tomando límites llegamos a que $0\leq f(t) \leq 0$ $\forall t\in J$, concluimos que $f(t) = 0$ $\forall t\in J$.
            \item Sea ahora $J$ cualquier intervalo, tomamos $J_n$ un intervalo compacto de forma que $J_n \subseteq J_{n+1}$ con $t_0 \in J_0$ y:
                \begin{equation*}
                    \bigcup_{n=0}^\infty J_n = J
                \end{equation*}
                Por el paso anterior, $f(t) = 0$ $\forall t\in J_n$, para todo $n\in \mathbb{N}$, con lo que $f(t) = 0$ $\forall t\in J$.
        \end{itemize}
    \end{proof}
\end{lema}

\section{Sistemas lineales homogéneos}
\noindent
Nos preocupamos ahora por sistemas de la forma
\begin{equation*}\label{eq:lin_sup_h}
    x' = A(t) x
\end{equation*}
con $A:I\rightarrow\mathbb{R}^{d\times d}$ una función continua. Sea $V = C^1(I,\mathbb{R}^d)$ el espacio vectorial de las funciones continuas sobre el intervalo $I$ en $\mathbb{R}^d$. Definimos además $W=C^0(I,\mathbb{R}^d)$.

\begin{definicion}
    Dado un sistema lineal homogéneo de la forma~(\ref{eq:lin_sup_h}), definimos el operador asociado a la ecuación como la aplicación $L:V\rightarrow W$ dado por:
    \begin{equation*}
        L[x] = x' - Ax
    \end{equation*}
\end{definicion}
Se verifica que el operador lineal $L$ asociado a la ecuación~(\ref{eq:lin_sup_h}) es lineal. Más aún, se verifica que $Z=\ker L$ es el espacio vectorial de las soluciones de la ecuación~(\ref{eq:lin_sup_h}). 

\begin{prop}
    $\dim Z = d$.
    \begin{proof}
        Para ello, fijado $t_0\in I$, definimos $\Phi_{t_0}:Z\rightarrow\mathbb{R}^d$ dada por\newline $\Phi_{t_0}(x) = x(t_0)$, que es un isomorfismo entre $Z$ y $\mathbb{R}^d$ gracias al Teorema~\ref{teo:existencia_unicidad_sistemas}, concluimos que $\dim Z = d$.
    \end{proof}
\end{prop}

Dados $\phi_1,\ldots,\phi_d\in Z$ funciones linealmente independientes en $V$, todas las soluciones de~(\ref{eq:lin_sup_h}) las obtendremos mediante combinaciones lineales de dichas funciones:
\begin{equation*}
    x(t) = c_1\phi_1(t) + \ldots + c_d\phi_d(t) \qquad c_1,\ldots,c_d\in \mathbb{R}
\end{equation*}

\begin{prop}
    Dadas $\phi_1,\ldots,\phi_d\in Z$, son equivalentes:
    \begin{enumerate}
        \item[$i)$] $\{\phi_1,\ldots,\phi_d\}$ es una base.
        \item[$ii)$] $\det(\phi_1(t)|\ldots|\phi_d(t)) \neq 0$ $\forall t\in I$.
        \item[$iii)$] $\det(\phi_1(t)|\ldots|\phi_d(t)) \neq 0$ para cierto $t\in I$.
    \end{enumerate}
    \begin{proof}
        Se deja como ejercicio para el lector.
    \end{proof}
\end{prop}~\\

\noindent
Sabemos que la ecuación de la forma~(\ref{eq:lin_sup_h}) no se puede resolver de forma explícita para $d\geq 2$. En el siguiente ejemplo, veremos soluciones de ecuaciones de la forma~(\ref{eq:lin_sup_h}) que sí se pueden resolver de forma explícita.
\begin{ejemplo}
Un primer ejemplo de estos son los sistemas triangulares.
    Consideramos el sistema:
    \begin{equation*}
        \left\{\begin{array}{rl}
                x_1' &= x_1 + x_2 \\
            x_2' &= \frac{1}{t}x_2
        \end{array}\right.
    \end{equation*}
    Estamos trabajando con $I = \mathbb{R}^+$, $d=2$ y:
    \begin{equation*}
        x = \left(\begin{array}{c}
            x_1 \\
            x_2
        \end{array}\right) \qquad A(t) = \left(\begin{array}{cc}
            1 & 1 \\
            0 & \nicefrac{1}{t}
        \end{array}\right) \quad \forall t\in I
    \end{equation*}
    Comenzaremos resolviendo la primera ecuación y luego sustituyendo en la primera:
    \begin{equation*}
        x_2' = \dfrac{1}{t}x_2
    \end{equation*}
    Sabemos que las soluciones de esta ecuación son de la forma $x_2:I\rightarrow\mathbb{R}$
    \begin{equation*}
        x_2(t) = c_2 t \qquad c_2\in \mathbb{R} \quad t\in I
    \end{equation*}
    Trataremos de resolver ahora la ecuación:
    \begin{equation*}
        x_1' = x_1 + c_2 t
    \end{equation*}
    que es una ecuación lineal completa. Para resolverla, haremos uso de su estructura afín: buscaremos una solución a ojo y le sumaremos las soluciones de su ecuación homogénea. Buscamos con una función de la forma:
    \begin{equation*}
        x_1(t) = \alpha t + \beta \qquad t\in I
    \end{equation*}
    Derivando:
    \begin{equation*}
        \alpha = \alpha t + \beta + c_2 t
    \end{equation*}
    Que nos lleva a unas ecuaciones:
    \begin{equation*}
        \left\{\begin{array}{rl}
                \alpha &= \beta \\
                \alpha + c_2 &= 0
        \end{array}\right. 
    \end{equation*}
    Con lo que la solución particular buscada es:
    \begin{equation*}
        x_1(t) = -c_2(t+1) \qquad t\in I
    \end{equation*}
    Finalmente, una solución de la ecuación completa es $x_1:I\rightarrow\mathbb{R}$ dada por:
    \begin{equation*}
        x_1(t) = -c_2(t+1) + c_1 e^t \qquad c_2\in \mathbb{R} \quad  t\in I
    \end{equation*}
    Para buscar una base que nos dé el espacio de soluciones para el sistema, haremos elecciones de $c_1$ y $c_2$ para obtener dos funciones linealmente independientes. De esta forma, una base la obtenemos con:
    \begin{equation*}
        \phi_1(t) = \left(\begin{array}{c}
            e^t \\
            0
        \end{array}\right) \qquad 
        \phi_2(t) = \left(\begin{array}{c}
            -(t+1) \\
            t
        \end{array}\right) \qquad \forall t\in I
    \end{equation*}
    Que son dos funciones $\phi_1,\phi_2:I\rightarrow\mathbb{R}$ linealmente independientes, ya que:
    \begin{equation*}
        \det(\phi_1(t)|\phi_2(t)) = t\cdot e^t \neq 0 \qquad \forall t\in I
    \end{equation*}
\end{ejemplo}

\subsection{Sistemas de coeficientes constantes}
\noindent
Un tipo de sistemas que también se puede resolver siempre es cuando la función $A$ es constante. Veamos este ejemplo, donde trabajamos con una matriz $A\in \mathbb{R}^{d\times d}$, con lo que $I=\mathbb{R}$.\\

Supongamos que $\lm \in \sigma(A)\cap \mathbb{R}$ es un valor propio no trivial de $A$, siendo $v\in \mathbb{R}^d\setminus\{0\}$ un vector propio asociado a $\lm$. En dicho caso, la función $x:I\rightarrow\mathbb{R}$ dada por
\begin{equation*}
    x(t) = e^{\lm t} \cdot v \qquad t\in I
\end{equation*}
Es una solución del sistema, ya que:
\begin{equation*}
    x'(t) = \lm e^{\lm t} v 
\end{equation*}
Y se tiene que:
\begin{equation*}
    Ax(t) = e^{\lm t} Av = \lm e^{\lm t} v = x'(t) \qquad \forall t\in I
\end{equation*}
De esta forma, ante un sistema de coeficientes constantes en el que la matriz $A$ sea diagonalizable, bastará encontrar los valores y vectores propios de la matriz para hayar las soluciones.

\begin{ejemplo}
    Sea el sistema de ecuaciones diferenciales dado por la matriz:
    \begin{equation*}
        A = \left(\begin{array}{cc}
                3 & 1 \\
                1 & 3
        \end{array}\right)
    \end{equation*}
    Tenemos que $\sigma(A) = \{\lm_1, \lm_2\}$, con $\lm_1 = 4$ y $\lm_2 = -2$. Además, sabemos que $v_1=(1, 1)$ y $v_2 = (1, -1)$ son vectores propios asociados a dichos valores, respectivamente. De esta forma, sabemos que:
    \begin{equation*}
        \phi_1(t) = e^{4t} \left(\begin{array}{c}
            1 \\
            1
        \end{array}\right) \qquad 
        \phi_2(t) = e^{-2t} \left(\begin{array}{c}
            1 \\
            -1
        \end{array}\right) \qquad \forall t\in I
    \end{equation*}
    Son soluciones del sistema, que además son linealmente independientes, ya que:
    \begin{equation*}
        \det(\phi_1(t)|\phi_2(t)) = e^{2t} \det(v_1|v_2) \neq 0 \qquad \forall t\in I
    \end{equation*}
\end{ejemplo}

\subsubsection{Valores propios complejos}
Dada una matriz $A\in \mathbb{R}^{d\times d}$, si tomamos $\lm \in \sigma(A)\cap (\bb{C}\setminus\bb{R})$, con vector propio $w\in \bb{C}^d\setminus \{0\}$. Lo que haremos ahora será buscar soluciones del sistema en los complejos, es decir, buscar una función $x:\mathbb{R}\rightarrow\bb{C}^d$ pensando en $\bb{C}$ como en $\mathbb{R}^2$: $x = u + iv$. Al obtener una solución compleja $x$, su parte real $u = Re(x)$ y su parte imaginaria $v = Im(x)$ son soluciones reales:
\begin{equation*}
    \left.\begin{array}{r}
        x' = u' + iv' \\
        x' = Ax = Au + iAv
    \end{array}\right\} \Longrightarrow u' + iv' = A\cdot (u+iv)
\end{equation*}

\begin{ejemplo}
    Si ahora tomamos la matriz:
    \begin{equation*}
        \left(\begin{array}{cc}
                0 & 1 \\
                -1 & 0
        \end{array}\right) \in \mathbb{R}^{2\times 2}
    \end{equation*}
    Es la matriz asociada a la rotación de 90º, que no tiene valores propios reales, sino complejos:
    \begin{equation*}
        \sigma(A) = \{\lm_1 = i, \lm_2 = -i\}
    \end{equation*}
    Con vectores propios asociados $v_1=(1,i)$, $v_2=(1,-i)$ linealmente independientes. Podemos construir una solución compleja:
    \begin{equation*}
        \psi(t) = e^{it} 
        \left(\begin{array}{c}
                1\\
                i
        \end{array}\right) = 
        \left(\begin{array}{c}
                e^{it} \\
                ie^{it}
        \end{array}\right) = 
        \left(\begin{array}{rcl}
                \cos t &+& i\sen t \\
                -\sen t &+& i\cos t
        \end{array}\right) \qquad \forall t\in I
    \end{equation*}
    De donde podemos obtener dos soluciones reales:
    \begin{equation*}
        \phi_1(t) = \left(\begin{array}{c}
            \cos t\\
            -\sen t
        \end{array}\right) \qquad 
        \phi_2(t) = \left(\begin{array}{c}
            \sen t\\
            \cos t
        \end{array}\right) \qquad t\in I
    \end{equation*}
    Que son linealmente independientes, por ser $\det(\phi_1(t)|\phi_2(t))\neq 0$ $\forall t\in I$.\\

    % // TODO: Clase 3-12

    \noindent
    Se puede hacer más rápido, considerando el sistema:
    \begin{equation*}
        \left\{\begin{array}{rcl}
                x_1' &=& x_2 \\
                x_2' &=& -x_1
        \end{array}\right.
    \end{equation*}
    Derivando:
    \begin{equation*}
        x_1'' = x_2' = -x_1
    \end{equation*}
    y:
    \begin{equation*}
        x_1'' + x_1 = 0
    \end{equation*}
    Con polinomio característico $p(\lm)=\lm^2 + 1$. Tiene soluciones complejas y si tomamos sus soluciones:
    \begin{equation*}
        e^{it}=\cos t + i\sen t \qquad e^{-it}
    \end{equation*}
    Con lo que una solución es
    \begin{equation*}
        x_1(t) = c_1 \cos t + c_2\sen t
    \end{equation*}
    \begin{equation*}
        x_2(t) = -c_1\sen t + c_2\cos t
    \end{equation*}
\end{ejemplo}

\subsection{Matriz solución y matriz fundamental}
\noindent
Trabajamos ahora con un sistema lineal homogéneo de la forma~(\ref{eq:lin_sup_h}), con una función $A:I\rightarrow\mathbb{R}^{d\times d}$ continua. Recordamos que buscamos una solución $x:I\rightarrow\mathbb{R}^d$.\\

Sin embargo, podemos pensar en $x$ no como en un vector sino como en una matriz cuadrada.
\begin{definicion}
    Dada una ecuación de la forma~(\ref{eq:lin_sup_h}), una matriz solución de la ecuación va a ser una función $\Phi:I\rightarrow\mathbb{R}^{d\times d}$ con coeficientes $\Phi(t) = {(\phi_{ij}(t))}_{i,j}$ que sea derivable\footnote{Recordemos que esto es equivalente a que cada coordenada $\phi_{ij}$ sea derivable.} y que cumpla que:
    \begin{equation*}
        \Phi'(t) = A(t)\Phi(t) \qquad \forall t\in I
    \end{equation*}
\end{definicion}

\begin{ejemplo}
    Antes teníamos el sistema $x' = Ax$ dado por:
    \begin{equation*}
        \left(\begin{array}{cc}
                0 & 1 \\
                -1 & 0
        \end{array}\right)
    \end{equation*}
    Puede comprobarse que $\Phi:\mathbb{R}\rightarrow\mathbb{R}^{2\times 2}$ dada por:
    \begin{equation*}
        \Phi(t) = \left(\begin{array}{cc}
                \cos t & \sen t \\
                -\sen t & \cos t
        \end{array}\right) \qquad t\in I
    \end{equation*}
    es una matriz solución del sistema. También lo es:
    \begin{equation*}
        \Phi_1(t) = \left(\begin{array}{cc}
                2\cos t & 3\sen t \\
                -2\sen t & 3\cos t
        \end{array}\right) \qquad t\in I
    \end{equation*}
\end{ejemplo}

\begin{observacion}
    Si tenemos una matriz solución $\Phi$, si notamos a sus columnas por $\phi_1,\ldots,\phi_d$:
    \begin{equation*}
        \Phi = (\phi_1|\ldots|\phi_d)
    \end{equation*}
    Es equivalente a que $\phi_1,\ldots,\phi_d \in Z$.

    Que proviene de que si tenemos una matriz $B=(b_1|\ldots|b_d)$, entonces:
    \begin{equation*}
        A\cdot B = (Ab_1|\ldots|Ab_d)
    \end{equation*}
    Con lo que si se cumple $\Phi'(t) = A(t)\Phi(t)$, entonces:
    \begin{equation*}
        (\phi_1'|\ldots|\phi_d') = A(\phi_1|\ldots|\phi_d)
    \end{equation*}
\end{observacion}
Vista esta observación, una matriz solución no es nada más que una matriz cuyas columnas son soluciones.

\begin{definicion}[Matriz fundamental]
    Sea $\Phi(t)$ una matriz solución, si existe\footnote{Esto es equivalente a que el determinante sea distinto de 0 para todo $t\in I$, gracias a la teoría desarrollada.} $t\in I$ tal que $\det(\Phi(t)) \neq 0$, diremos que $\Phi$ es una matriz fundamental.
\end{definicion}
\begin{observacion}
Notemos que una matriz fundamental es una matriz cuyas columnas formen una base. De esta forma, se tiene la identidad:
    \begin{equation*}
        Z = \{\Phi(t)c \mid c\in \mathbb{R}^d\}
    \end{equation*}
    Notemos que si exigimos que $\Phi$ es una matriz solución (no necesariamente fundamental), solo obtenemos la inclusión:
    \begin{equation*}
        \{\Phi(t)c \mid c\in \mathbb{R}^d\} \subseteq Z
    \end{equation*}
\end{observacion}

\subsubsection{Derivación del producto de dos matrices}
Con vistas a demostrar una proposición, aprenderemos ahora a derivar un producto de matrices
\begin{lema}\label{lema:prop_deriv}
    Sean pues $\Phi,\Psi:I\rightarrow\mathbb{R}^{d\times d}$ funciones derivables, entonces $\Phi\cdot \Psi$ es derivable, con\footnote{Recordemos que el producto de matrices no es conmutativo, luego debemos mantener el órden en la fórmula.}:
    \begin{equation*}
        (\Phi\cdot \Psi)' = \Phi'\cdot \Psi + \Phi\cdot \Psi
    \end{equation*}
    \begin{proof}
        Las coordenadas de $\Phi\cdot \Psi$ son sumas y productos de funciones derivables:

        Si $\Phi = {(\phi_{ij})}_{i,j}$ y $\Psi = {(\psi_{ij})}_{i,j}$, tendremos que:
        \begin{equation*}
            \xi_{ij} = \sum_{k=1}^d \phi_{ik}\cdot  \psi_{kj} \qquad \forall i,j \in \{1,\ldots,d\}
        \end{equation*}
        Y que $\Phi\cdot \Psi = {(\xi_{ij})}_{i,j}$. Ahora, si escribimos el cociente incremental de la función $\Phi\cdot \Psi$:
        \begin{multline*}
            \dfrac{1}{h}[\Phi(t+h)\cdot \Psi(t+h) - \Phi(t)\Psi(t)] =\\ \dfrac{1}{h} [\Phi(t+h)\cdot \Psi(t+h)-\Phi(t+h)\cdot \Psi(t)] + \dfrac{1}{h} [\Phi(t+h)\Psi(t)-\Phi(t)\Psi(t)]
        \end{multline*}
        Sacando factor común:
        \begin{multline*}
            \dfrac{1}{h}[\Phi(t+h)\cdot \Psi(t+h) - \Phi(t)\Psi(t)] =\\ \dfrac{\Phi(t+h)}{h} [\Psi(t+h)-\Psi(t)] + \dfrac{1}{h} [\Phi(t+h)-\Phi(t)] \Psi(t)
        \end{multline*}
        Y si ahora hacemos $h\rightarrow 0$:
        \begin{equation*}
            (\Phi\cdot \Psi)(t) = \Phi(t)\cdot \Psi'(t) + \Phi'(t) \cdot \Psi(t) =  \Phi(t)\cdot \Psi'(t) +  \Phi'(t) \cdot \Psi(t)  \qquad \forall t\in I
        \end{equation*}
    \end{proof}
\end{lema}


\begin{prop}
    Supongamos que $\Phi(t)$ es una matriz solución y que $C\in \mathbb{R}^{d\times d}$. Entonces $\Phi(t)\cdot C$ es una matriz solución.
    \begin{proof}
        Que $\Phi$ sea una matriz solución significa que es derivable y que:
        \begin{equation*}
            \Phi'(t) = A(t)\cdot \Phi(t) \qquad \forall t\in I
        \end{equation*}
        Por el Lema~\ref{lema:prop_deriv}:
        \begin{equation*}
            (\Phi\cdot C)' = \Phi'\cdot C = (A\cdot \Phi)\cdot C = A\cdot (\Phi \cdot C)
        \end{equation*}
        Con lo que $\Phi\cdot C$ es una matriz solución.
    \end{proof}
\end{prop}

\begin{coro}
    Si $\Phi(t)$ es una matriz fundamental y $C\in \mathbb{R}^{d\times d}$ con $\det(C)\neq 0$. Entonces $\Phi(t)\cdot C$ es una matriz fundamental.
\end{coro}

\begin{ejercicio*}
    Si tenemos una matriz fundamental $\Phi$, podemos obtener todas las bases de soluciones si multiplicamos por cada matriz $C\in \mathbb{R}^{d\times d}$ con $\det(C)\neq 0$.
\end{ejercicio*}

\begin{definicion}[Matriz fundamental principal en un punto]
    Dado $t_0\in I$, decimos que $\Phi$ es una matriz fundamental principal en $t_0$ si $\Phi$ es una matriz fundamental y se verifica que
    \begin{equation*}
        \Phi(t_0) = I
    \end{equation*}
\end{definicion}

\begin{ejemplo}
    El sistema anterior $x' = Ax$ dado por la matriz:
    \begin{equation*}
        A = \left(\begin{array}{cc}
                0 & 1 \\
                -1 & 0
        \end{array}\right)
    \end{equation*}
    Sabemos que la matriz $\Phi:I\rightarrow\mathbb{R}$ dada por:
    \begin{equation*}
        \Phi(t) = \left(\begin{array}{cc}
                \cos t & \sen t \\
                -\sen t & \cos t
        \end{array}\right) \qquad t\in I
    \end{equation*}
    Es fundamental. Además, es principal en cualquier punto de la forma $2k\pi$, con $k\in \mathbb{Z}$.
\end{ejemplo}

\begin{prop}
    Dado un sistema lineal homogéneo de la forma~(\ref{eq:lin_sup_h}) y $t_0\in I$, entonces existe una única matriz fundamental principal en $t_0$.
    \begin{proof}
        %  // TODO: Hacer
    \end{proof}
\end{prop}

\begin{ejercicio*}
    Existe una Fórmula de Jacobi-Liouville para sistemas:

    Dada una matriz solución $\Phi(t)$, tomamos $t_0\in I$. Resulta que:
    \begin{equation*}
        \det\Phi(t) = \det\Phi(t_0)\cdot e^{\displaystyle \int_{t_0}^{t} tr A(s)~ds }
    \end{equation*}
    Donde notamos por $trA(s)$ a la traza de la matriz $A$ en el punto $s\in \mathbb{R}$. Se pide:
    \begin{itemize}
        \item Demostrar la fórmula.
        \item Ver que la fórmula del Capítulo anterior es un caso particular de esta.
    \end{itemize}
    (\textbf{Pista}: derivar la función $\det\Phi(t)$, sacar una ecuación diferencial de primer orden del que es solución y comprobar que la expresión de la derecha también es solución del mismo problema de valores iniciales.)
\end{ejercicio*}

\section{Exponencial de una matriz}
Como motivación, volvemos a la ecuación dieferncial del inicio del curso:
\begin{equation*}
    x' = \lm x
\end{equation*}
Tenemos que una solución es:
\begin{equation*}
    x(t) = e^{\lm t} x_0
\end{equation*}
Si ahora consideramos el sistema:
\begin{equation*}
    x' = Ax
\end{equation*}
Buscamos que las soluciones del sistema sean de la forma:
\begin{equation*}
    x(t) = e^{tA} x_0
\end{equation*}

\noindent
Como motivación, si tenemos un polinomio $p(\lm) = \lm^3 -\lm +  3$, podemos evaluarlo en una matriz $A$: 
$p(A) =A^3 -A+3I$ y una forma de definir la exponencial es:
\begin{equation*}
    e^\lm = \sum_{n=0}^{\infty} \dfrac{\lm^n}{n!}
\end{equation*}
que puede entenderse como un polinomio de grado infinito.
\begin{definicion}[Exponencial de una matriz cuadrada]
    Sea $A\in \mathbb{R}^{d\times d}$ una matriz cuadrada, definimos la sucesión:
    \begin{equation*}
        S_k = \sum_{n=0}^{k} \dfrac{1}{n!} A^n \qquad \forall k\in \mathbb{N}
    \end{equation*}
    Se verifica que $\{S_k\}$ converge a una matriz, a la que llamamos exponencial de $A$:
    \begin{equation*}
        e^A = \sum_{n=0}^{\infty} \dfrac{1}{n!} A^n
    \end{equation*}
\end{definicion}
