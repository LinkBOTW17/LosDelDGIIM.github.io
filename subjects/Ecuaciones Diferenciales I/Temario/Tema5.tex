\newpage
\chapter{Sistemas Lineales}

\begin{notacion}
    Por comodidad, a lo largo de la sección notaremos al conjunto de matrices de orden $n\times m$ sobre $\mathbb{R}$ por:
    \begin{equation*}
        \mathbb{R}^{n\times m} = M_{n\times m}(\mathbb{R})
    \end{equation*}
\end{notacion}

\noindent
Estudiaremos sistemas lineales de primer orden, es decir, ecuaciones de la forma:
\begin{equation}\label{eq:sel_1orden}
    x' = A(t) x + b(t)
\end{equation}
Con $A:I\rightarrow\mathbb{R}^{d\times d}$ y $b:I\rightarrow\mathbb{R}^d$, funciones continuas\footnote{Recordemos que esto significa que sean continuas coordenada a coordenada.} en un intervalo abierto $I\subseteq \mathbb{R}$. Si notamos por $A = {(a_{ij})}_{i,j}$, $d = (b_i)$ y $x = (x_i)$ a las correspondientes coordenadas de $A$, $b$ y $x$, podemos reescribir~(\ref{eq:sel_1orden}) en forma de sistema, como:
\begin{equation}\label{eq:sistema_1orden}
    \left\{\begin{array}{ccccccccc}
            x_1' & = & a_{11}(t)x_1 & + & \cdots & + & a_{1d}(t)x_d & + & b_1(t) \\
            x_2' & = & a_{21}(t)x_1 & + & \cdots & + & a_{2d}(t)x_d & + & b_2(t) \\
            \vdots & & \vdots & & \ddots & & \vdots & & \vdots \\
            x_d' & = & a_{d1}(t)x_1 & + & \cdots & + & a_{dd}(t)x_d & + & b_d(t) \\
    \end{array}\right.
\end{equation}

\begin{ejemplo}
    Supongamos que estamos en la situación de la Figura~\ref{fig:muelles}, con dos masas $m_1$ y $m_2$, y dos muelles con constantes elásticas $k_1$ y $k_2$. Supongamos además que a la masa $m_2$ se le aplica una fuerza $F(t)$.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        % Dibuja la pared
        \draw[thick] (0,0.5) -- (0,1.5);

        % Dibuja el primer muelle
        \draw[thick, decoration={aspect=0.3, segment length=4mm, amplitude=3mm, coil}, decorate] (0,1) -- (4,1);
        % Etiqueta para el primer muelle
        \node[above] at (2,1.2) {$k_1$};

        % Dibuja la primera masa (cuadrado)
        \draw[fill=gray!30] (4,0.75) rectangle (4.5,1.25);
        % Etiqueta para la primera masa
        \node[below] at (4.25,0.75) {$m_1$};

        % Dibuja el segundo muelle (más ancho)
        \draw[thick, decoration={aspect=0.3, segment length=4mm, amplitude=4mm, coil}, decorate] (4.5,1) -- (8,1);
        % Etiqueta para el segundo muelle
        \node[above] at (6.25,1.3) {$k_2$};

        % Dibuja la segunda masa (cuadrado)
        \draw[fill=gray!30] (7.9,0.65) rectangle (8.6,1.35);
        % Etiqueta para la segunda masa
        \node[below] at (8.25,0.65) {$m_2$};

        % Dibuja un vector horizontal desde la masa derecha
        \draw[-{Latex[length=3mm, width=2mm]}, thick] (8.6,1) -- (9.6,1);
        % Etiqueta para el vector
        \node[above] at (9.1,1) {$F(t)$};
    \end{tikzpicture}
    \caption{Dos masas conectadas por muelles.}
    \label{fig:muelles}
\end{figure}
    Describiremos este sistema de forma matemática describiendo $x_1$, la distancia de la masa $m_1$ a su posición de equilibrio; y $x_2$, la distancia de la masa $m_2$ a su posición de equilibrio a lo largo del tiempo $t$.\\

    Suponiendo que inicialmente (en el instante $t_0$) el primer muelle está dilatado (es decir, $x_1(t_0) > 0$) y que el segundo muelle está contraido ($x_2(t_0) - x_1(t_0)<0$), aplicando las leyes de Newton, llegamos al sistema:
    \begin{equation*}
        \left\{\begin{array}{rcl}
                m_1 x_1 '' &=& -k_1 x_1 + k_2 (x_2 - x_1) \\
                m_2 x_2 '' &=& -k_2(x_2 - x_1) + F(t)
        \end{array}\right.
    \end{equation*}
    La máquina descrita sigue estas ecuaciones diferenciales, que no están en la categoría que nos interesa, por ser de segundo orden. Sin embargo, un sistema lineal de cualquier orden se puede hacer siempre de primer orden. Para ello, buscamos transformar dos ecuaciones de segundo orden en 4 ecuaciones de primer orden.\\

    \noindent
    El truco para cambiar orden por dimensión es llamar incógnita a las derivadas. Definimos:
    \begin{equation*}
        y_1 = x_1 \qquad y_2 = x_1' \qquad y_3 = x_2 \qquad y_4 = x_2'
    \end{equation*}
    De esta forma:
    \begin{equation*}
        \left\{\begin{array}{rl}
                y_1' &= y_2 \\
            y_2' &= \dfrac{-k_1}{m_1} y_1 + \dfrac{k_2}{m_1} (y_3-y_1) \\
            y_3' &= y_4 \\
            y_4' &= \dfrac{-k_2}{m_2}(y_3-y_1) + \dfrac{F(t)}{m_2}
        \end{array}\right.
    \end{equation*}
    Obtenemos ya un sistema de ecuaciones lineal de primer orden. Los físicos dicen que hemos pasado del espacio de las configuraciones al espacio de estados.\\

    Tenemos:
    \begin{equation*}
        A(t) = \left(\begin{array}{cccc}
                0 & 1 & 0 & 0 \\
                -\left(\frac{k_1+k_2}{m_1}\right) & 0 & \frac{k_2}{m_2} & 0 \\
                0 & 0 & 0 & 1 \\
                \frac{k_2}{m_2} & 0 & \frac{-k_2}{m_2} & 0
        \end{array}\right) \qquad b(t) = \left(\begin{array}{c}
            0 \\
            0 \\
            0 \\
            \dfrac{F(t)}{m_2}
        \end{array}\right)
    \end{equation*}
    Este cambio se puede aplicar siempre que queramos, y esta es la razón por la que en este capítulo solo estudiaremos sistemas de ecuaciones lineales de primer orden, porque sabiendo resolverlo sabemos resolver cualquier sistema lineal de orden superior.
\end{ejemplo}

\begin{teo}[Existencia y unicidad de las soluciones]\label{teo:existencia_unicidad_sistemas}
    Dados $t_0\in I$, $x_0\in \mathbb{R}^d$, existe una única solución del sistema:
    \begin{equation*}
        x' = A(t)x + b(t) \qquad x(t_0) = x_0
    \end{equation*}
    definida en \textbf{todo} el intervalo $I$.
\end{teo}
Para su demostración, será necesario repasar varios conceptos ya vistos en otras asignaturas.

\begin{coro}
Ahora, si tenemos una ecuación lineal de orden superior:
\begin{equation*}
    x^{(k)} + a_{k-1}(t) x^{(k-1)} + \cdots + a_1(t)x' + a_0(t)x = b(t)
\end{equation*}
Lo que hacemos es tomar como incógnitas:
\begin{equation*}
    y_1 = x \qquad y_2 = x' \qquad \ldots \qquad y_k = x^{(k-1)}
\end{equation*}
Y plantear el sistema:
\begin{equation*}
    \left\{\begin{array}{rcl}
            y_1' &=& y_2 \\
            y_2' &=& y_3 \\
            \vdots && \vdots \\
            y_{k-1}' &=& y_k \\
            y_k' &=& -a_0(t)y_1 -a_1(t) y_2 - \cdots - a_{k-1}(t)y_k + b(t)
    \end{array}\right.
\end{equation*}
Con lo que el Teorema de existencia y unicidad del Capítulo anterior es un corolario del Teorema~\ref{teo:existencia_unicidad_sistemas}.
\end{coro}

\subsubsection{Normas matriciales}
Dada cualquier norma\footnote{Recordamos que una norma es cualquier función que cumpla la desigualdad triangular, homogeneidad por homotecias y no degeneración.} $\|\cdot \|:\mathbb{R}^d\rightarrow\mathbb{R}$, esta nos permite definir una norma matricial $\|\cdot \|:\mathbb{R}^{d\times d}\rightarrow\mathbb{R}$, dada por:
\begin{equation*}
    \|A\| = \max\{\|Ax\| \mid \|x\|\geq 1\} \qquad \forall A\in \mathbb{R}^{d\times d}
\end{equation*}
Notemos que está bien definida\footnote{Que en realidad existe un máximo.}, ya que lo que hacemos es considerar la función $x\longmapsto \|Ax\|$ (que es continua) definida en la bola unidad junto con su frontera (que es un conjunto compacto), por lo que la imagen de un compacto es un compacto y al estar en $\mathbb{R}$, es un conjunto cerrado y acotado.\\

De forma geométrica, cada $A$ es una transformación del espacio $\mathbb{R}^d$ en sí mismo. Lo que hacemos es tomar el punto más lejano al origen que es imagen de un vector de norma 1 en el dominio.

\begin{ejemplo}
    Considerando el espacio normado $(\mathbb{R}^2, \|\cdot \|_2)$, si tomamos:
    \begin{equation*}
        A = \left(\begin{array}{cc}
                2 & 0 \\
                0 & \nicefrac{1}{2}
        \end{array}\right) \in \mathbb{R}^{2\times 2}
    \end{equation*}
    % // TODO: Dibujar la transformacion de la bola unidad
    La aplicación lineal asociada a $A$ transforma $\bb{S}^1$ en una elipse de eje mayor 2 y eje menor $\nicefrac{1}{2}$, tal y como vemos en la Figura~\ref{fig:S1_fA}, con lo que $\|A\| = 2$.

    \begin{figure}[H]
        \centering
\begin{tikzpicture}[scale=1]
% Ejes de coordenadas originales
\draw[-Stealth] (-1.5,0) -- (1.5,0) node[anchor=north] {$x$};
\draw[-Stealth] (0,-1.5) -- (0,1.5) node[anchor=east] {$y$};

% Circunferencia inicial
\draw[thick,gray!90] (0,0) circle(1);

% Flecha de transformación curva (menos arqueada)
\draw[-Stealth,thick] 
    (1.2,0.2) .. controls (2.3,0.8) and (3.5,0.8) .. (4.5,0.5)
    node[midway,above] {$f_A$};

% Ejes de coordenadas transformados
\begin{scope}[shift={(6,0)}] % Aumentar la separación
    \draw[-Stealth] (-2.5,0) -- (2.5,0) node[anchor=north] {$x$};
    \draw[-Stealth] (0,-1.5) -- (0,1.5) node[anchor=east] {$y$};

    % Elipse transformada
    \draw[thick,gray!90] (0,0) ellipse (2 and 0.5);
\end{scope}
\end{tikzpicture}        
\caption{Transformación de $\bb{S}^1$ por la aplicación lineal asociada a $A$.}
\label{fig:S1_fA}
    \end{figure}
\end{ejemplo}

\noindent
Las normas matriciales así definidas tienen más propiedades que las normas vectoriales de las que provienen, que ya fueron vistas en Métodos Numéricos\footnote{Diríjase a dichos apuntes para consultar la demostración de la siguiente Proposición.} I\@:
\begin{prop}
    Se verifica que:
    \begin{enumerate}
        \item $\|I\| = 1$.
        \item $\|AB\| \leq \|A\|\|B\|$, $\forall A,B\in \mathbb{R}^{d\times d}$.
        \item $\|Ax\| \leq \|A\|\|x\|$, $\forall x\in \mathbb{R}^d, A\in \mathbb{R}^{d\times d}$.
    \end{enumerate}
\end{prop}

\subsubsection{Integrales vectoriales}
Supongamos que tenemos $f:[a,b]\rightarrow \mathbb{R}^d$ una función continua en un intervalo compacto, con lo que $f$ tiene $d$ coordenadas: $f=(f_1,\ldots,f_d)$, todas ellas continuas. De esta forma, podemos definir la integral de $f$ como el vector formado por las integrales de sus componentes
\begin{equation*}
    \int_{a}^{b} f(t)~dt  = \left(\begin{array}{c}
        \displaystyle\int_{a}^{b} f_1(t)~dt  \\
        \displaystyle\int_{a}^{b} f_2(t)~dt  \\
        \vdots \\
        \displaystyle\int_{a}^{b} f_d(t)~dt  
    \end{array}\right)
\end{equation*}

\begin{prop}
    Sea $A\in \mathbb{R}^{d\times d}$, entonces:
    \begin{equation*}
        A\cdot \left(\int_{a}^{b} f(t)~dt \right) = \int_{a}^{b} [A\cdot f(t)]~dt
    \end{equation*}
    \begin{proof}
        Si notamos a las coordenadas de $A$ por $A={(a_{ij})}_{i,j}$:

        \begin{align*}
            A \left(\int_{a}^{b} f(t)~dt \right) &= 
            \left(\begin{array}{cccc}
                a_{11} & a_{12} & \cdots & a_{1d} \\
                a_{21} & a_{22} & \cdots & a_{2d} \\
                \vdots & \vdots & \ddots & \vdots \\
                a_{d1} & a_{d2} & \cdots & a_{dd} \\
            \end{array}\right) 
            \left(\begin{array}{c}
                \displaystyle\int_{a}^{b} f_1(t)~dt  \\
                \displaystyle\int_{a}^{b} f_2(t)~dt  \\
                \vdots \\
                \displaystyle\int_{a}^{b} f_d(t)~dt  
            \end{array}\right) = \left(\begin{array}{c}
                \displaystyle\sum_{j=1}^{d} \left(a_{1j}  \int_{a}^{b} f_{j}(t)~dt \right) \\
                \displaystyle\sum_{j=1}^{d} \left(a_{2j}  \int_{a}^{b} f_{j}(t)~dt \right) \\
                \vdots \\
                \displaystyle\sum_{j=1}^{d} \left(a_{dj}  \int_{a}^{b} f_{j}(t)~dt \right) 
            \end{array}\right) \\
                 &= \left(\begin{array}{c}
                     \displaystyle \int_{a}^{b} \left(\sum_{j=1}^{d}a_{1j} \cdot f_j(t)\right)~dt  \\
                     \displaystyle \int_{a}^{b} \left(\sum_{j=1}^{d}a_{2j} \cdot f_j(t)\right)~dt  \\
                     \vdots \\
                     \displaystyle \int_{a}^{b} \left(\sum_{j=1}^{d}a_{dj} \cdot f_j(t)\right)~dt  
             \end{array}\right) = \int_{a}^{b} [A\cdot f(t)]~dt 
        \end{align*}

    \end{proof}
\end{prop}

\begin{prop}
    Se verifica que:
    \begin{equation*}
        \left\|\int_{a}^{b} f(t)~dt \right\| \leq \int_{a}^{b} \|f(t)\|~dt 
    \end{equation*}
    Para cualquier norma.
    \begin{proof}
        Por comodidad, definimos $\Delta_m = \{1,\ldots,m\}$.

        Sea $P=\{a=x_0<x_1<\ldots<x_m=b\}$ una partición de $[a,b]$ y $\xi_j \in [x_{j-1},x_j]$ $\forall j\in \Delta_m$. Como las componentes de $f$ y la función $\|f\|$ son continuas, son integrables en el sentido de Riemann y podemos considerar las sumas de Riemann asociadas a la partición $P$ con etiquetas $\xi_j$ $\forall j\in \Delta_m$. Entonces, se cumple que:
        \begin{align*}
            \sigma(\|f\|,P) &= \sum_{j=1}^{d} \|f(\xi_j)\| (x_j-x_{j-1}) = \sum_{j=1}^{d}\|f(\xi_j)(x_j-x_{j-1})\| \geq \left\|\sum_{j=1}^{d}(f(\xi_j)(x_j-x_{j-1}))\right\| \\
                            &= \left\|\sum_{j=1}^{d}\left(\sum_{k=1}^{d}(f_k(\xi_j)(x_j-x_{j-1}))l_k\right)\right\|= \left\|\sum_{k=1}^{d}\left(\sum_{j=1}^{d}(f_k(\xi_j)(x_j-x_{j-1}))l_k\right)\right\| \\
                            &= \left\|\sum_{k=1}^{d}(\sigma(f_k,P)l_k)\right\| = \left\| \left(\begin{array}{c}
                                \sigma(f_1,P) \\
                                \sigma(f_2,P) \\
                                \vdots \\
                                \sigma(f_d,P) 
                            \end{array}\right)  \right\|
        \end{align*}
        Por lo que $\sigma(\|f\|,P) \geq \|(\sigma(f_1,P), \ldots, \sigma(f_d,P))\|$ para toda partición $P$ del intervalo $[a,b]$.\\

        Por tanto, si $\{P_n\}$ es una sucesión de particiones de $[a,b]$ tales que $\{\Delta P_n\}\longrightarrow 0$ (donde $\Delta P_n$ es el diámetro de la partición $P_n$), usando que $\|f\|$ y todas las componentes de $f$ son Riemann-integrables, se tiene que:
        \begin{align*}
            \int_{a}^{b} \|f(t)\|~dt &= \lim_{n\to\infty}\sigma(\|f\|, P_n) \geq \lim_{n\to\infty} \left\| \left(\begin{array}{c}
                \sigma(f_1,P_n) \\
                \vdots  \\
                \sigma(f_d,P_n) 
            \end{array}\right) \right\|    = \left\| \left(\begin{array}{c}
                \displaystyle\lim_{n\to\infty} \sigma(f_1,P_n) \\
                \vdots  \\
                \displaystyle\lim_{n\to\infty}  \sigma(f_d,P_n) 
            \end{array}\right) \right\| \\
                                     &=  \left\| \left(\begin{array}{c}
                \int_{a}^{b} f_1(t)~dt  \\
                \vdots \\
                \int_{a}^{b} f_d(t)~dt  
            \end{array}\right) \right\| = \left\|\int_{a}^{b} f(t)~dt \right\|
        \end{align*}

        Es decir:
    \begin{equation*}
        \left\|\int_{a}^{b} f(t)~dt \right\| \leq \int_{a}^{b} \|f(t)\|~dt 
    \end{equation*}
    \end{proof}
\end{prop}

% // TODO: Clase
\subsubsection{Convergencia uniforme}
\noindent
Dada cualquier norma vectorial $\|\cdot \|:\mathbb{R}^d\rightarrow\mathbb{R}$ y fijado un intervalo $I\subseteq \mathbb{R}$, podemos definir\footnote{En este caso, no obtenemos una norma, porque puede tomar el valor $\infty$.} $\|\cdot \|_\infty:\bb{F}(\text{I},\mathbb{R}^d)\rightarrow\mathbb{R}$ dada por:
\begin{equation*}
    \|\varphi\|_\infty = \sup_{t\in I}\|\varphi(t)\| \qquad \forall \varphi \in \bb{F}(\text{I},\mathbb{R}^d)
\end{equation*}

\begin{definicion}[Norma del máximo]
    Recordamos la definición de la norma vectorial del máximo, $\|\cdot \|_{\max}:\mathbb{R}^d\rightarrow\mathbb{R}$ dada por
    \begin{equation*}
        \|x\|_{\max} = \max\{|x_1|,|x_2|\} \qquad \forall x=(x_1,x_2)\in \mathbb{R}^2
    \end{equation*}
    Que en algunos contextos se denota también por $\|\cdot \|_1$.
\end{definicion}

\begin{ejemplo}
    En $(\mathbb{R}^2, \|\cdot \|_{\max})$:
    \begin{enumerate}
        \item Sea $\varphi_1:\left]0,1\right[\rightarrow\mathbb{R}^2$ dada por:
            \begin{equation*}
                \varphi_1(t) = \left(\begin{array}{c}
                    \cos t \\
                    e^t
                \end{array}\right) \qquad t\in \left]0,1\right[
            \end{equation*}
            Tenemos que $\|\varphi\| = e$.
        \item Sea ahora $\varphi_2:\left]0,1\right[\rightarrow\mathbb{R}^2$ dada por:
            \begin{equation*}
                \varphi_2(t) = \left(\begin{array}{c}
                        \nicefrac{1}{t} \\
                        t
                \end{array}\right) \qquad t\in \left]0,1\right[
            \end{equation*}
            Tenemos que $\|\varphi\| = \infty$.
    \end{enumerate}
\end{ejemplo}

\begin{definicion}
    Dada una sucesión de funciones $\{f_n\}$ y una función $f$, todas ellas definidas sobre un mismo intervalo $I$, decimos que $f_n$ convergen uniformemente a $f$ si:
    \begin{equation*}
        \|f_n - f\|_\infty \rightarrow 0
    \end{equation*}
\end{definicion}

\begin{ejemplo}
    Dadas $f,\psi \in \bb{F}(\text{I},\mathbb{R})$ y $\delta\in \mathbb{R}^+$, que:
    \begin{equation*}
        \|f-\psi\|_\infty \leq \delta
    \end{equation*}
    significa que $\psi$ no se puede separar de $f$ más que $\delta$. Podemos observar esto gráficamente en la Figura~\ref{fig:conv_unif}, donde $\psi$ debe estar en la región delimitada por las líneas discontinuas.
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            % Ejes
            \draw[-Stealth] (-0.5, 0) -- (7, 0) node[right] {$x$};
            \draw[-Stealth] (0, -0.5) -- (0, 4.5) node[above] {$y$};

            % Función f(x)
            \draw[thick, gray!90, domain=0:6, smooth, samples=100] 
                plot(\x, {sin(deg(\x)) + 2}) node[right] {$f(x)$};

            % Función f(x) + 1
            \draw[thick, dashed, blue!50, domain=0:6, smooth, samples=100] 
                plot(\x, {sin(deg(\x)) + 2.5}) node[right] {};

            % Función f(x) - 1
            \draw[thick, dashed, blue!50, domain=0:6, smooth, samples=100] 
                plot(\x, {sin(deg(\x)) + 1.5}) node[right] {};
        \end{tikzpicture}
        \caption{Región en la que debe estar $\psi$.}
        \label{fig:conv_unif}
    \end{figure}

\end{ejemplo}

\noindent
Algunas propiedades de la convergencia uniforme\footnote{que ya fueron vistas en Análisis Matemático II} son:
\begin{prop}
    Si $f_n$ son continuas y convergen uniformemente a $f$, entonces $f$ es continua.
\end{prop}

\begin{prop}
    Si $[a,b]\subseteq I$ y tenemos $f_n:I\rightarrow\mathbb{R}^d$ continuas que convergen uniformemente a $f:I\rightarrow\mathbb{R}^d$, entonces:
    \begin{equation*}
        \int_{a}^{b} f_n \rightarrow \int_a^b f
    \end{equation*}
\end{prop}

Sin embargo, si $f_n$ son derivables y convergen uniformemente a $f$, entonces no podemos asegurar que $f$ sea derivable:
\begin{ejemplo}
    Sean $f_n:\mathbb{R}\rightarrow\mathbb{R}$:
    \begin{equation*}
        f_n(t) = \dfrac{\sen(nt)}{n} \qquad n\in \mathbb{N} \quad t\in \mathbb{R}
    \end{equation*}
    Tenemos que $f_n\rightarrow f$ con $f:\mathbb{R}\rightarrow\mathbb{R}$ dada por $f(t) = 0$ $\forall t\in \mathbb{R}$, ya que:
    \begin{equation*}
        \|f_n - f\|_\infty = \sup_{t\in \mathbb{R}} \left|\dfrac{\sen(nt)}{n}\right| \leq \dfrac{1}{n} \longrightarrow 0
    \end{equation*}
    Y tenemos que:
    \begin{equation*}
        f_n'(t) = \cos(nt) \qquad \forall n\in \mathbb{N}, \quad t\in \mathbb{R}
    \end{equation*}
    Que no convergen a $f'$, ya que:
    \begin{equation*}
        f_n'(\pi) = {(-1)}^{n} \not\rightarrow f'(\pi) = 0
    \end{equation*}
\end{ejemplo}

\begin{prop}[Test de Weierstrass]
    Dadas $f_n:I\rightarrow\mathbb{R}^d$, resulta que:
    \begin{equation*}
        \|f_{n+1}(t) - f_n(t)\| \leq M_n \qquad \forall t\in I, \quad \forall n\geq 0
    \end{equation*}
    Si tenemos que $\sum M_n \leq \infty$. Entonces, $f_n$ converge uniformemente en $I$.
\end{prop}
Este Test de Weierstrass nos permite demostrar la existencia del límite de una sucesión de funciones sin conocer la función límite.

\begin{observacion}
    Si queremos estudiar la serie $\sum a_n$, bastar definir las sumas parciales:
    \begin{equation*}
        s_n = \sum_{k=1}^{n}a_k
    \end{equation*}
    Y estudiar el límite $\lim a_n$.\\

    Así mismo, si buscamos $\lim a_n$, podemos estudiar la serie:
    \begin{equation*}
        \sum (a_{n+1}-a_n)
    \end{equation*}
\end{observacion}

\begin{ejemplo}
    Sabemos que:
    \begin{equation*}
        e^t = \sum_{n=0}^{\infty} \dfrac{t^n}{n!} \qquad t\in \mathbb{R}
    \end{equation*}
    Si definimos:
    \begin{equation*}
        S_n(t) = \sum_{k=0}^{n} \dfrac{t^k}{k!} \qquad \forall n\in \mathbb{N}, \quad t\in \mathbb{R}
    \end{equation*}

    \begin{enumerate}
        \item Recordando la teoría que usábamos en Análisis Matemático I sobre el radio de convergencia, vemos que el radio de convergencia de $S_n$ es infinito, por lo que podemos garantizar convergencia uniforme en cada intervalo compacto de $\mathbb{R}$, pero no en todo $\mathbb{R}$.\\

    Mediante el siguiente dibujo, podemos pensar que la convergencia en todo $\mathbb{R}$ no se nos garantiza. % // TODO: Hacer dibujo 
    Pero los polinomios en infinito se van a menos infinito o más infinito.???.

    De esta forma, una serie de polinomios nunca puede converger a una función que está acotada en un intervalo no acotado.
        \item De forma análoga y usando el Test de Weierstrass, podemos desmotrar convergencia en cada intervalo compacto, siendo $a$ el máximo de dicho intervalo:
            \begin{equation*}
                |S_{n+1}(t) - S_n(t)| = \dfrac{t^{n+1}}{(n+1)!} \leq \dfrac{a^{n+1}}{(n+1)!} \longrightarrow 0
            \end{equation*}
    \end{enumerate}
\end{ejemplo}

\begin{teo}[Existencia y unicidad de las soluciones]\label{teo:existencia_unicidad_sistemas}
    Dados $t_0\in I$, $x_0\in \mathbb{R}^d$, existe una única solución del sistema:
    \begin{equation*}
        x' = A(t)x + b(t) \qquad x(t_0) = x_0
    \end{equation*}
    definida en \textbf{todo} el intervalo $I$.
    \begin{proof}
        Inicialmente, demostraremos el teorema en un caso particular y veremos que el caso general se puede reducir al particular:
        \begin{itemize}
            \item Supongamos que $I$ es un intervalo acotado de longitud $l$ y que:
                \begin{equation*}
                    \|A(t)\| \leq \alpha \quad \|b(t)\|\leq \beta \qquad \forall t\in I
                \end{equation*}
                \begin{description}
                    \item [Exitencia.]~\\
                        Queremos llegar a que tenemos una solución, esta cumplirá:
                        \begin{equation*}
                            x'(t) = A(t)x(t) + b(t)
                        \end{equation*}
                        Con lo que la integraremos (vectorialmente) cogiendo $t_0\in I$ (son funciones continuas en un compacto):
                        \begin{equation*}
                            \int_{t_0}^{t} x'(s)~ds  = \int_{t_0}^{t} [A(s)x(s) + b(s)]~ds 
                        \end{equation*}
                        Aplicando coordenada a coordenada la Regla de Barrow y que $x(t_0) = x_0$:
                        \begin{equation*}
                            x(t) - x_0 = x(t) - x(t_0) = \int_{t_0}^{t} x'(s)~ds  = \int_{t_0}^{t} [A(s)x(s) + b(s)]~ds 
                        \end{equation*}
                        Y hemos llegado a una ecuación integral que cumplirá la $x$ buscada:
                        \begin{equation*}
                            x(t) = x_0 + \int_{t_0}^{t} [A(s)x(s) + b(s)]~ds 
                        \end{equation*}
                        Buscamos soluciones aproximadas (buscamos las iterantes de Picard):
                        La primera aproximación la tomamos como la condición inicial:
                            \begin{equation*}
                                x_0(t) = x_0 \qquad t\in I
                            \end{equation*}
                            con lo que:
                            \begin{equation*}
                                x_{n+1}(t) = x_0 + \int_{t_0}^{t} [A(s)x_n(s) + b(s)]~ds 
                            \end{equation*}
                        La idea de la demostración es construir las iterantes de Picard (que están bien definidas y todas de clase $C^1(I)$). Los pasos a seguir son:
                        \begin{enumerate}
                            \item Demostraremos que las iterantes de Picard convergen uniformemente (esto será una función continua).
                            \item Una vez que sabemos que $x_n$ tienden a un límite, haciendo $n$ tender a infinito, vamos a llegar a la ecuación integral, usando para ello la comnutación de integral con convergencia uniforme.

                                El límite de Picard es una solución integral.
                            \item Probar que una solución de la ecuación integral es una solución del problema de valores iniciales.
                        \end{enumerate}
                \end{description}
            \item De vuelta al caso general,
        \end{itemize}
    \end{proof}
\end{teo}

