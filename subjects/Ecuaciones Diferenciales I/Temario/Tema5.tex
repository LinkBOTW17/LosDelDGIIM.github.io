\newpage
\chapter{Sistemas Lineales}

\begin{notacion}
    Por comodidad, a lo largo de la sección notaremos al conjunto de matrices de orden $n\times m$ sobre $\mathbb{R}$ por:
    \begin{equation*}
        \mathbb{R}^{n\times m} = M_{n\times m}(\mathbb{R})
    \end{equation*}
\end{notacion}

\noindent
Estudiaremos sistemas lineales de primer orden, es decir, ecuaciones de la forma:
\begin{equation}\label{eq:sel_1orden}
    x' = A(t) x + b(t)
\end{equation}
Con $A:I\rightarrow\mathbb{R}^{d\times d}$ y $b:I\rightarrow\mathbb{R}^d$, funciones continuas\footnote{Recordemos que esto significa que sean continuas coordenada a coordenada.} en un intervalo abierto $I\subseteq \mathbb{R}$. Si notamos por $A = {(a_{ij})}_{i,j}$, $b = (b_i)$ y $x = (x_i)$ a las correspondientes coordenadas de $A$, $b$ y $x$, podemos reescribir~(\ref{eq:sel_1orden}) en forma de sistema, como:
\begin{equation}\label{eq:sistema_1orden}
    \left\{\begin{array}{ccccccccc}
            x_1' & = & a_{11}(t)x_1 & + & \cdots & + & a_{1d}(t)x_d & + & b_1(t) \\
            x_2' & = & a_{21}(t)x_1 & + & \cdots & + & a_{2d}(t)x_d & + & b_2(t) \\
            \vdots & & \vdots & & \ddots & & \vdots & & \vdots \\
            x_d' & = & a_{d1}(t)x_1 & + & \cdots & + & a_{dd}(t)x_d & + & b_d(t) \\
    \end{array}\right.
\end{equation}

\begin{ejemplo}
    Supongamos que estamos en la situación de la Figura~\ref{fig:muelles}, con dos masas $m_1$ y $m_2$, y dos muelles con constantes elásticas $k_1$ y $k_2$. Supongamos además que a la masa $m_2$ se le aplica una fuerza $F(t)$.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        % Dibuja la pared
        \draw[thick] (0,0.5) -- (0,1.5);

        % Dibuja el primer muelle
        \draw[thick, decoration={aspect=0.3, segment length=4mm, amplitude=3mm, coil}, decorate] (0,1) -- (4,1);
        % Etiqueta para el primer muelle
        \node[above] at (2,1.2) {$k_1$};

        % Dibuja la primera masa (cuadrado)
        \draw[fill=gray!30] (4,0.75) rectangle (4.5,1.25);
        % Etiqueta para la primera masa
        \node[below] at (4.25,0.75) {$m_1$};

        % Dibuja el segundo muelle (más ancho)
        \draw[thick, decoration={aspect=0.3, segment length=4mm, amplitude=4mm, coil}, decorate] (4.5,1) -- (8,1);
        % Etiqueta para el segundo muelle
        \node[above] at (6.25,1.3) {$k_2$};

        % Dibuja la segunda masa (cuadrado)
        \draw[fill=gray!30] (7.9,0.65) rectangle (8.6,1.35);
        % Etiqueta para la segunda masa
        \node[below] at (8.25,0.65) {$m_2$};

        % Dibuja un vector horizontal desde la masa derecha
        \draw[-{Latex[length=3mm, width=2mm]}, thick] (8.6,1) -- (9.6,1);
        % Etiqueta para el vector
        \node[above] at (9.1,1) {$F(t)$};
    \end{tikzpicture}
    \caption{Dos masas conectadas por muelles.}
    \label{fig:muelles}
\end{figure}
    Describiremos este sistema de forma matemática describiendo $x_1$, la distancia de la masa $m_1$ a su posición de equilibrio; y $x_2$, la distancia de la masa $m_2$ a su posición de equilibrio a lo largo del tiempo $t$.\\

    Suponiendo que inicialmente (en el instante $t_0$) el primer muelle está dilatado (es decir, $x_1(t_0) > 0$) y que el segundo muelle está contraído ($x_2(t_0) - x_1(t_0)<0$), aplicando las leyes de Newton, llegamos al sistema:
    \begin{equation*}
        \left\{\begin{array}{rcl}
                m_1 x_1 '' &=& -k_1 x_1 + k_2 (x_2 - x_1) \\
                m_2 x_2 '' &=& -k_2(x_2 - x_1) + F(t)
        \end{array}\right.
    \end{equation*}
    La máquina descrita sigue estas ecuaciones diferenciales, que no están en la categoría que nos interesa, por ser de segundo orden. Sin embargo, un sistema lineal de cualquier orden se puede hacer siempre de primer orden. Para ello, buscamos transformar dos ecuaciones de segundo orden en 4 ecuaciones de primer orden.\\

    \noindent
    El truco para cambiar orden por dimensión es llamar incógnita a las derivadas. Definimos:
    \begin{equation*}
        y_1 = x_1 \qquad y_2 = x_1' \qquad y_3 = x_2 \qquad y_4 = x_2'
    \end{equation*}
    De esta forma:
    \begin{equation*}
        \left\{\begin{array}{rl}
                y_1' &= y_2 \\
            y_2' &= \dfrac{-k_1}{m_1} y_1 + \dfrac{k_2}{m_1} (y_3-y_1) \\
            y_3' &= y_4 \\
            y_4' &= \dfrac{-k_2}{m_2}(y_3-y_1) + \dfrac{F(t)}{m_2}
        \end{array}\right.
    \end{equation*}
    Obtenemos ya un sistema de ecuaciones lineal de primer orden. Los físicos dicen que hemos pasado del espacio de las configuraciones al espacio de estados.\\

    Tenemos:
    \begin{equation*}
        A(t) = \left(\begin{array}{cccc}
                0 & 1 & 0 & 0 \\
                -\left(\frac{k_1+k_2}{m_1}\right) & 0 & \frac{k_2}{m_2} & 0 \\
                0 & 0 & 0 & 1 \\
                \frac{k_2}{m_2} & 0 & \frac{-k_2}{m_2} & 0
        \end{array}\right) \qquad b(t) = \left(\begin{array}{c}
            0 \\
            0 \\
            0 \\
            \dfrac{F(t)}{m_2}
        \end{array}\right)
    \end{equation*}
    Este cambio se puede aplicar siempre que queramos, y esta es la razón por la que en este capítulo solo estudiaremos sistemas de ecuaciones lineales de primer orden, porque sabiendo resolverlo sabemos resolver cualquier sistema lineal de orden superior.
\end{ejemplo}

\begin{teo}[Existencia y unicidad de las soluciones]\label{teo:existencia_unicidad_sistemas}
    Dados $t_0\in I$, $x_0\in \mathbb{R}^d$, existe una única solución del sistema:
    \begin{equation*}
        x' = A(t)x + b(t) \qquad x(t_0) = x_0
    \end{equation*}
    definida en \textbf{todo} el intervalo $I$.
\end{teo}
\noindent
Para su demostración, será necesario repasar varios conceptos ya vistos en otras asignaturas.

\begin{coro}
Ahora, si tenemos una ecuación lineal de orden superior:
\begin{equation*}
    x^{(k)} + a_{k-1}(t) x^{(k-1)} + \cdots + a_1(t)x' + a_0(t)x = b(t)
\end{equation*}
Lo que hacemos es tomar como incógnitas:
\begin{equation*}
    y_1 = x \qquad y_2 = x' \qquad \ldots \qquad y_k = x^{(k-1)}
\end{equation*}
Y plantear el sistema:
\begin{equation*}
    \left\{\begin{array}{rcl}
            y_1' &=& y_2 \\
            y_2' &=& y_3 \\
            \vdots && \vdots \\
            y_{k-1}' &=& y_k \\
            y_k' &=& -a_0(t)y_1 -a_1(t) y_2 - \cdots - a_{k-1}(t)y_k + b(t)
    \end{array}\right.
\end{equation*}
Con lo que el Teorema de existencia y unicidad del Capítulo anterior es un corolario del Teorema~\ref{teo:existencia_unicidad_sistemas}.
\end{coro}

Comenzamos entonces con el recordatorio de los conceptos necesarios para la demostración del Teorema~\ref{teo:existencia_unicidad_sistemas}. En particular, será necesario recordar ormas matriciales, integrales vectoriales y convergencia uniforme.
\subsubsection{Normas matriciales}
Dada cualquier norma\footnote{Recordamos que una norma es cualquier función que cumpla la desigualdad triangular, homogeneidad por homotecias y no degeneración.} (vectorial) $\|\cdot \|:\mathbb{R}^d\rightarrow\mathbb{R}$, esta nos permite definir una norma matricial $\|\cdot \|:\mathbb{R}^{d\times d}\rightarrow\mathbb{R}$, dada por:
\begin{equation*}
    \|A\| = \max\{\|Ax\| \mid \|x\|\leq 1\} \qquad \forall A\in \mathbb{R}^{d\times d}
\end{equation*}
\begin{observacion}
    En el caso de que no se especifique la norma, se sobreentiende que es la norma euclídea.
\end{observacion}
Notemos que está bien definida\footnote{Que en realidad existe un máximo.}, ya que consideramos la función $x\longmapsto \|Ax\|$ (que es continua) definida en la bola unidad junto con su frontera (que es un conjunto compacto), por lo que la imagen de un compacto es un compacto y al estar en $\mathbb{R}$, es un conjunto cerrado y acotado.\\

De forma geométrica, cada $A$ es una transformación del espacio $\mathbb{R}^d$ en sí mismo. Lo que hacemos para calcular su norma es calcular las imágenes de todos los vectores de la bola unidad (junto con su frontera) y quedarnos con la mayor norma de todos ellos. Si consideramos la norma vectorial euclídea, lo que hacemos es coger la mayor distancia al origen.

\begin{ejemplo}
    Considerando el espacio normado $(\mathbb{R}^2, \|\cdot \|_2)$, si tomamos:
    \begin{equation*}
        A = \left(\begin{array}{cc}
                2 & 0 \\
                0 & \nicefrac{1}{2}
        \end{array}\right) \in \mathbb{R}^{2\times 2}
    \end{equation*}
    La aplicación lineal asociada a $A$ transforma $\bb{S}^1$ en una elipse de eje mayor 2 y eje menor $\nicefrac{1}{2}$, tal y como vemos en la Figura~\ref{fig:S1_fA}, con lo que $\|A\| = 2$.

    \begin{figure}[H]
        \centering
\begin{tikzpicture}[scale=1]
% Ejes de coordenadas originales
\draw[-Stealth] (-1.5,0) -- (1.5,0) node[anchor=north] {$x$};
\draw[-Stealth] (0,-1.5) -- (0,1.5) node[anchor=east] {$y$};

% Circunferencia inicial
\draw[thick,gray!90] (0,0) circle(1);

% Flecha de transformación curva (menos arqueada)
\draw[-Stealth,thick] 
    (1.2,0.2) .. controls (2.3,0.8) and (3.5,0.8) .. (4.5,0.5)
    node[midway,above] {$f_A$};

% Ejes de coordenadas transformados
\begin{scope}[shift={(6,0)}] % Aumentar la separación
    \draw[-Stealth] (-2.5,0) -- (2.5,0) node[anchor=north] {$x$};
    \draw[-Stealth] (0,-1.5) -- (0,1.5) node[anchor=east] {$y$};

    % Elipse transformada
    \draw[thick,gray!90] (0,0) ellipse (2 and 0.5);
\end{scope}
\end{tikzpicture}        
\caption{Transformación de $\bb{S}^1$ por la aplicación lineal asociada a $A$.}
\label{fig:S1_fA}
    \end{figure}
\end{ejemplo}

\noindent
Las normas matriciales así definidas tienen más propiedades que las normas vectoriales de las que provienen, que ya fueron vistas en Métodos Numéricos I\@:
\begin{prop}
    Se verifica que:
    \begin{enumerate}
        \item $\|I\| = 1$.
        \item $\|AB\| \leq \|A\|\|B\|$, $\forall A,B\in \mathbb{R}^{d\times d}$.
        \item $\|Ax\| \leq \|A\|\|x\|$, $\forall x\in \mathbb{R}^d, A\in \mathbb{R}^{d\times d}$.
    \end{enumerate}
    \begin{proof}
        Demostramos cada igualdad:
        \begin{enumerate}
            \item Evidente.
            \item A partir de la definición, sean $A,B\in \mathbb{R}^{d\times d}$:
                \begin{align*}
                    \|AB\| &= \max\{\|ABu\| \mid \|u\| \leq 1\} \leq \max\{\|A\|\|Bu\| \mid \|u\| \leq 1\} \\
                        &\leq \max\{\|A\|\|B\|\|u\| \mid \|u\| \leq 1\} = \|A\|\|B\|
                \end{align*}
            \item A partir de la definición de $\|A\|$, sabemos que $\|Au\| \leq \|A\|$ para cualquier $u\in \bb{S}^1$ y $A\in \mathbb{R}^{d\times d}$. De esta forma:
                \begin{equation*}
                    \left\|A \dfrac{x}{\|x\|}\right\| = \dfrac{1}{\|x\|} \cdot \|Ax\| \leq \|A\|
                \end{equation*}
                con lo que:
                \begin{equation*}
                    \|Ax\| \leq \|A\|\|x\|
                \end{equation*}
        \end{enumerate}
    \end{proof}
\end{prop}

\subsubsection{Integrales vectoriales}
Supongamos que tenemos $f:[a,b]\rightarrow \mathbb{R}^d$ una función continua en un intervalo compacto, con lo que $f$ tiene $d$ coordenadas: $f=(f_1,\ldots,f_d)$, todas ellas continuas. De esta forma, podemos definir la integral de $f$ como el vector formado por las integrales de sus componentes
\begin{equation*}
    \int_{a}^{b} f(t)~dt  = \left(\begin{array}{c}
        \displaystyle\int_{a}^{b} f_1(t)~dt  \\
        \displaystyle\int_{a}^{b} f_2(t)~dt  \\
        \vdots \\
        \displaystyle\int_{a}^{b} f_d(t)~dt  
    \end{array}\right)
\end{equation*}

\begin{prop}
    Sea $A\in \mathbb{R}^{d\times d}$ y $f:[a,b]\rightarrow \mathbb{R}^d$ una función continua. Entonces, se verifica:
    \begin{equation*}
        A\cdot \left(\int_{a}^{b} f(t)~dt \right) = \int_{a}^{b} [A\cdot f(t)]~dt
    \end{equation*}
    \begin{proof}
        Si notamos a las coordenadas de $A$ por $A={(a_{ij})}_{i,j}$:

        \begin{align*}
            A \left(\int_{a}^{b} f(t)~dt \right) &= 
            \left(\begin{array}{cccc}
                a_{11} & a_{12} & \cdots & a_{1d} \\
                a_{21} & a_{22} & \cdots & a_{2d} \\
                \vdots & \vdots & \ddots & \vdots \\
                a_{d1} & a_{d2} & \cdots & a_{dd} \\
            \end{array}\right) 
            \left(\begin{array}{c}
                \displaystyle\int_{a}^{b} f_1(t)~dt  \\
                \displaystyle\int_{a}^{b} f_2(t)~dt  \\
                \vdots \\
                \displaystyle\int_{a}^{b} f_d(t)~dt  
            \end{array}\right) = \left(\begin{array}{c}
                \displaystyle\sum_{j=1}^{d} \left(a_{1j}  \int_{a}^{b} f_{j}(t)~dt \right) \\
                \displaystyle\sum_{j=1}^{d} \left(a_{2j}  \int_{a}^{b} f_{j}(t)~dt \right) \\
                \vdots \\
                \displaystyle\sum_{j=1}^{d} \left(a_{dj}  \int_{a}^{b} f_{j}(t)~dt \right) 
            \end{array}\right) \\
                 &= \left(\begin{array}{c}
                     \displaystyle \int_{a}^{b} \left(\sum_{j=1}^{d}a_{1j} \cdot f_j(t)\right)~dt  \\
                     \displaystyle \int_{a}^{b} \left(\sum_{j=1}^{d}a_{2j} \cdot f_j(t)\right)~dt  \\
                     \vdots \\
                     \displaystyle \int_{a}^{b} \left(\sum_{j=1}^{d}a_{dj} \cdot f_j(t)\right)~dt  
             \end{array}\right) = \int_{a}^{b} [A\cdot f(t)]~dt 
        \end{align*}

    \end{proof}
\end{prop}

\begin{prop}
    Sea $f:[a,b]\rightarrow \mathbb{R}^d$ una función continua y $\|\cdot\|$ una norma en $\mathbb{R}^d$. Entonces, se verifica:
    \begin{equation*}
        \left\|\int_{a}^{b} f(t)~dt \right\| \leq \int_{a}^{b} \|f(t)\|~dt 
    \end{equation*}
    \begin{proof}
        Por comodidad, definimos $\Delta_m = \{1,\ldots,m\}$.

        Sea $P=\{a=x_0<x_1<\ldots<x_m=b\}$ una partición de $[a,b]$ y $\xi_j \in [x_{j-1},x_j]$ $\forall j\in \Delta_m$. Como las componentes de $f$ y la función $\|f\|$ son continuas, son integrables en el sentido de Riemann y podemos considerar las sumas de Riemann asociadas a la partición $P$ con etiquetas $\xi_j$ $\forall j\in \Delta_m$. Entonces, se cumple que:
        \begin{align*}
            \sigma(\|f\|,P) &= \sum_{j=1}^{d} \|f(\xi_j)\| (x_j-x_{j-1}) = \sum_{j=1}^{d}\|f(\xi_j)(x_j-x_{j-1})\| \geq \left\|\sum_{j=1}^{d}(f(\xi_j)(x_j-x_{j-1}))\right\| \\
                            &= \left\|\sum_{j=1}^{d}\left(\sum_{k=1}^{d}(f_k(\xi_j)(x_j-x_{j-1}))e_k\right)\right\|= \left\|\sum_{k=1}^{d}\left(\sum_{j=1}^{d}(f_k(\xi_j)(x_j-x_{j-1}))e_k\right)\right\| \\
                            &= \left\|\sum_{k=1}^{d}(\sigma(f_k,P)e_k)\right\| = \left\| \left(\begin{array}{c}
                                \sigma(f_1,P) \\
                                \sigma(f_2,P) \\
                                \vdots \\
                                \sigma(f_d,P) 
                            \end{array}\right)  \right\|
        \end{align*}
        Por lo que $\sigma(\|f\|,P) \geq \|(\sigma(f_1,P), \ldots, \sigma(f_d,P))\|$ para toda partición $P$ del intervalo $[a,b]$.\\

        Por tanto, si $\{P_n\}$ es una sucesión de particiones de $[a,b]$ tales que $\{\Delta P_n\}\longrightarrow 0$ (donde $\Delta P_n$ es el diámetro de la partición $P_n$), usando que $\|f\|$ y todas las componentes de $f$ son Riemann-integrables, se tiene que:
        \begin{align*}
            \int_{a}^{b} \|f(t)\|~dt &= \lim_{n\to\infty}\sigma(\|f\|, P_n) \geq \lim_{n\to\infty} \left\| \left(\begin{array}{c}
                \sigma(f_1,P_n) \\
                \vdots  \\
                \sigma(f_d,P_n) 
            \end{array}\right) \right\|    = \left\| \left(\begin{array}{c}
                \displaystyle\lim_{n\to\infty} \sigma(f_1,P_n) \\
                \vdots  \\
                \displaystyle\lim_{n\to\infty}  \sigma(f_d,P_n) 
            \end{array}\right) \right\| \\
                                     &=  \left\| \left(\begin{array}{c}
                \int_{a}^{b} f_1(t)~dt  \\
                \vdots \\
                \int_{a}^{b} f_d(t)~dt  
            \end{array}\right) \right\| = \left\|\int_{a}^{b} f(t)~dt \right\|
        \end{align*}

        Es decir:
    \begin{equation*}
        \left\|\int_{a}^{b} f(t)~dt \right\| \leq \int_{a}^{b} \|f(t)\|~dt 
    \end{equation*}
    \end{proof}
\end{prop}

\subsubsection{Convergencia uniforme}
\noindent
Dada cualquier norma vectorial $\|\cdot \|:\mathbb{R}^d\rightarrow\mathbb{R}$ y fijado un intervalo $I\subseteq \mathbb{R}$, podemos definir\footnote{En este caso, no obtenemos una norma, porque puede tomar el valor $\infty$.} $\|\cdot \|_\infty:\bb{F}(\text{I},\mathbb{R}^d)\rightarrow\mathbb{R}$ dada por:
\begin{equation*}
    \|\varphi\|_\infty = \sup_{t\in I}\|\varphi(t)\| \qquad \forall \varphi \in \bb{F}(\text{I},\mathbb{R}^d)
\end{equation*}
\begin{observacion}
    En el caso de que no se especifique la norma, se sobreentiende que es la norma euclídea.
\end{observacion}

\begin{definicion}[Norma del máximo]
    Recordamos la definición de la norma vectorial del máximo, $\|\cdot \|_{\max}:\mathbb{R}^d\rightarrow\mathbb{R}$ dada por
    \begin{equation*}
        \|x\|_{\max} = \max\{|x_1|,|x_2|\} \qquad \forall x=(x_1,x_2)\in \mathbb{R}^2
    \end{equation*}
    Que en algunos contextos se denota también por $\|\cdot \|_1$.
\end{definicion}

\begin{ejemplo}
    En $(\mathbb{R}^2, \|\cdot \|_{\max})$:
    \begin{enumerate}
        \item Sea $\varphi_1:\left]0,1\right[\rightarrow\mathbb{R}^2$ dada por:
            \begin{equation*}
                \varphi_1(t) = \left(\begin{array}{c}
                    \cos t \\
                    e^t
                \end{array}\right) \qquad t\in \left]0,1\right[
            \end{equation*}
            Tenemos que $\|\varphi_1\|_{\infty} = e$.
        \item Sea ahora $\varphi_2:\left]0,1\right[\rightarrow\mathbb{R}^2$ dada por:
            \begin{equation*}
                \varphi_2(t) = \left(\begin{array}{c}
                        \nicefrac{1}{t} \\
                        t
                \end{array}\right) \qquad t\in \left]0,1\right[
            \end{equation*}
            Tenemos que $\|\varphi_2\|_{\infty} = \infty$.
    \end{enumerate}
\end{ejemplo}

\begin{definicion}[Convergencia Uniforme]
    Dada una sucesión de funciones $\{f_n\}$ y una función $f$, todas ellas definidas sobre un mismo intervalo $I$ y con imagen en $\bb{R}$, decimos que $\{f_n\}$ converge uniformemente a $f$ si:
    \begin{equation*}
        \|f_n - f\|_\infty \rightarrow 0
    \end{equation*}
\end{definicion}

\begin{ejemplo}
    Dadas $f,\psi \in \bb{F}(\text{I},\mathbb{R})$ y $\delta\in \mathbb{R}^+$, que:
    \begin{equation*}
        \|f-\psi\|_\infty \leq \delta
    \end{equation*}
    significa que $\psi$ no se puede separar de $f$ más que $\delta$. Podemos observar esto gráficamente en la Figura~\ref{fig:conv_unif}, donde $\psi$ debe estar en la región delimitada por las líneas discontinuas.
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            % Ejes
            \draw[-Stealth] (-0.5, 0) -- (7, 0) node[right] {$x$};
            \draw[-Stealth] (0, -0.5) -- (0, 4.5) node[above] {$y$};

            % Función f(x)
            \draw[thick, gray!90, domain=0:6, smooth, samples=100] 
                plot(\x, {sin(deg(\x)) + 2}) node[right] {$f(x)$};

            % Función f(x) + 1
            \draw[thick, dashed, blue!50, domain=0:6, smooth, samples=100] 
                plot(\x, {sin(deg(\x)) + 2.5}) node[right] {};

            % Función f(x) - 1
            \draw[thick, dashed, blue!50, domain=0:6, smooth, samples=100] 
                plot(\x, {sin(deg(\x)) + 1.5}) node[right] {};
        \end{tikzpicture}
        \caption{Región en la que debe estar $\psi$.}
        \label{fig:conv_unif}
    \end{figure}

\end{ejemplo}

\noindent
Algunas propiedades de la convergencia uniforme\footnote{Que ya fueron vistas en Análisis Matemático II.} son:
\begin{prop}
    Sean $\{f_n\}$ una sucesión de funciones que convergen uniformemente a $f$.
    Si $f_n$ son continuas para todo $n\in \bb{N}$, entonces $f$ es continua.
\end{prop}

\begin{prop}
    Si $[a,b]\subseteq I$ y tenemos $f_n:I\rightarrow\mathbb{R}^d$ funciones continuas que convergen uniformemente a $f:I\rightarrow\mathbb{R}^d$, entonces:
    \begin{equation*}
        \int_{a}^{b} f_n \rightarrow \int_a^b f
    \end{equation*}
\end{prop}

Sin embargo, si $f_n$ son derivables y convergen uniformemente a $f$, entonces no podemos asegurar que $f$ sea derivable:
\begin{ejemplo}
    Sean $f_n:\mathbb{R}\rightarrow\mathbb{R}$:
    \begin{equation*}
        f_n(t) = \dfrac{\sen(nt)}{n} \qquad n\in \mathbb{N} \quad t\in \mathbb{R}
    \end{equation*}
    Tenemos que $f_n\rightarrow f$ con $f:\mathbb{R}\rightarrow\mathbb{R}$ dada por $f(t) = 0$ $\forall t\in \mathbb{R}$, ya que:
    \begin{equation*}
        \|f_n - f\|_\infty = \sup_{t\in \mathbb{R}} \left|\dfrac{\sen(nt)}{n}\right| \leq \dfrac{1}{n} \longrightarrow 0
    \end{equation*}
    Y tenemos que:
    \begin{equation*}
        f_n'(t) = \cos(nt) \qquad \forall n\in \mathbb{N}, \quad t\in \mathbb{R}
    \end{equation*}
    Que no convergen a $f'$, ya que:
    \begin{equation*}
        f_n'(\pi) = {(-1)}^{n} \not\rightarrow f'(\pi) = 0
    \end{equation*}
\end{ejemplo}

\begin{prop}[Test de Weierstrass]
    Sean $f_n:I\rightarrow\mathbb{R}^d$ y consideramos una sucesión $M_n$ de números reales tal que:
    \begin{equation*}
        \|f_{n+1}(t) - f_n(t)\| \leq M_n \qquad \forall t\in I, \quad \forall n\geq 0
    \end{equation*}
    Si $\sum M_n < \infty$ converge, entonces $\{f_n\}$ converge uniformemente en $I$.
\end{prop}
Este Test de Weierstrass nos permite demostrar la existencia del límite de una sucesión de funciones sin conocer la función límite.

\begin{ejemplo}
    Sabemos que:
    \begin{equation*}
        e^t = \sum_{n=0}^{\infty} \dfrac{t^n}{n!} \qquad t\in \mathbb{R}
    \end{equation*}

    Si definimos:
    \begin{equation*}
        S_n(t) = \sum_{k=0}^{n} \dfrac{t^k}{k!} \qquad \forall n\in \mathbb{N}, \quad t\in \mathbb{R}
    \end{equation*}

    \begin{enumerate}
        \item Recordando la teoría que usábamos en Análisis Matemático II sobre el radio de convergencia, vemos que el radio de convergencia de $S_n$ es infinito, por lo que podemos garantizar convergencia uniforme en cada intervalo compacto de $\mathbb{R}$, pero no en todo $\mathbb{R}$.\\

            Pensando en que los polinomios siempre divergen en $-\infty$, podemos intuir que la convergencia en todo $\mathbb{R}$ no la tenemos garantizada, ya que la función exponencial tiede a 0 en dicho límite.

    De esta forma, una serie de polinomios nunca puede converger a una función que está acotada en un intervalo no acotado.
\item De forma análoga y usando el Test de Weierstrass, podemos desmotrar la convergencia de la suesión $\{S_n\}$ en cada intervalo compacto $[a,b]$:
            \begin{equation*}
                |S_{n+1}(t) - S_n(t)| = \dfrac{t^{n+1}}{(n+1)!} \leq \dfrac{b^{n+1}}{(n+1)!} \longrightarrow 0 \qquad \forall t\in [a,b]
            \end{equation*}
    \end{enumerate}
\end{ejemplo}

\noindent
Estamos ya listos para realizar la demostración del Teorema~\ref{teo:existencia_unicidad_sistemas}:

\begin{teo}[Existencia y unicidad de las soluciones]\label{teo:existencia_unicidad_sistemas}
    Dados $t_0\in I$, $x_0\in \mathbb{R}^d$, existe una única solución del sistema:
    \begin{equation}\label{eq:teo_pvi}
        x' = A(t)x + b(t) \qquad x(t_0) = x_0
    \end{equation}
    definida en \textbf{todo} el intervalo $I$.
    \begin{proof}
        Inicialmente, demostraremos el teorema en un caso particular y veremos que el caso general se puede reducir al particular:
        \begin{itemize}
            \item Supongamos que $I$ es un intervalo acotado de longitud $l$ y que:
                \begin{equation*}
                    \|A(t)\| \leq \alpha \quad \|b(t)\|\leq \beta \qquad \forall t\in I
                \end{equation*}
                \begin{description}
                    \item [Exitencia.] Queremos llegar a que tenemos una solución $x$ del problema de valores iniciales~(\ref{eq:teo_pvi}), esta cumplirá:
                        \begin{equation*}
                            x'(t) = A(t)x(t) + b(t)
                        \end{equation*}
                        Con lo que la integraremos (vectorialmente) cogiendo $t_0\in I$ (son funciones continuas en un compacto):
                        \begin{equation*}
                            \int_{t_0}^{t} x'(s)~ds  = \int_{t_0}^{t} [A(s)x(s) + b(s)]~ds 
                        \end{equation*}
                        Aplicando coordenada a coordenada la Regla de Barrow y que $x(t_0) = x_0$:
                        \begin{equation*}
                            x(t) - x_0 = x(t) - x(t_0) = \int_{t_0}^{t} x'(s)~ds  = \int_{t_0}^{t} [A(s)x(s) + b(s)]~ds 
                        \end{equation*}
                        Y hemos llegado a una ecuación integral que cumplirá la $x$ buscada:
                        \begin{equation*}
                            x(t) = x_0 + \int_{t_0}^{t} [A(s)x(s) + b(s)]~ds 
                        \end{equation*}
                        Buscaremos soluciones aproximadas (buscamos las iterantes de Picard):
                        La primera aproximación la tomamos como la condición inicial:
                            \begin{equation*}
                                x_0(t) = x_0 \qquad t\in I
                            \end{equation*}
                            con lo que podemos definir:
                            \begin{equation*}
                                x_{n+1}(t) = x_0 + \int_{t_0}^{t} [A(s)x_n(s) + b(s)]~ds \qquad \forall n\in \mathbb{N}
 :                           \end{equation*}
                        La idea de la demostración es construir las iterantes de Picard (que están bien definidas y todas de clase $C^1(I)$). Los pasos a seguir son:
                        \begin{enumerate}
                            \item Demostraremos que las iterantes de Picard convergen uniformemente (esto será una función continua).
                            \item Una vez que sabemos que $x_n$ tienden a un límite, haciendo $n$ tender a infinito, vamos a llegar a la ecuación integral, usando para ello la comnutación de integral con convergencia uniforme.

                                El límite de Picard es una solución integral.
                            \item Probar que una solución de la ecuación integral es una solución del problema de valores iniciales.
                        \end{enumerate}

                        % // Clase 2-12
                        Comenzando ahora con la demostración, definimos las Iterantes de Picard:
                        \begin{align*}
                            x_0(t) &= x_0 \\
                            x_{n+1}(t) &= x_0 + \int_{t_0}^{t} [A(s)x_n(s)+b(s)]~ds 
                        \end{align*}
                        Con $x_n:I\rightarrow\mathbb{R}^d$ bien definidas y continuas $\forall n\in \mathbb{N}$ (hágase por inducción). Además, $x_n\in C^1(I)$ $\forall n\in \mathbb{N}$, gracias al Teorema Fundamental del Cálculo.\\

                        Veamos que $\{x_n\}$ converge uniformemente en $I$, usando para ello el Test de Weierstrass. Comenzamos acotando la primera diferencia:
                        \begin{align*}
                            \|x_1(t) - x_0(t)\| &= \left\|\int_{t_0}^{t} [A(s)x_0+b(s)]~ds \right\| \leq \left|\int_{t_0}^{t}\| A(s)x_0 + b(s)\|~ds \right| \\
                                                &\leq \left|\int_{t_0}^{t}\left[ \|A(s)\|\|x_0\| + \|b(s)\|\right]~ds \right| \leq \left|\int_{t_0}^{t} [\alpha\cdot  \|x_0\| + \beta]~ds \right| \\
                                                &\leq (\alpha\cdot \|x_0\|+\beta)\cdot l = M_0 \in \mathbb{R} \qquad \forall t\in I
                        \end{align*}
                        Ahora:
                        \begin{align*}
                            \|x_2(t) - x_1(t)\| &= \left\|\int_{t_0}^{t} [A(s)(x_2(s)-x_1(s))]~ds \right\| \leq  \left|\int_{t_0}^{t} \|A(s)(x_2(s)-x_1(s))\|~ds \right| \\
                                                &\leq \alpha\left|\int_{t_0}^{t} \|x_1(s)-x_0(s)\|~ds \right| \leq \alpha M_0 \left|\int_{t_0}^{t}~ds \right| \leq \alpha M_0 |t-t_0|
                        \end{align*}
                        Donde hemos mantenido la dependencia de $t$, ya que si ahora decimos que $\alpha M_0 |t-t_0|\leq \alpha M_0 l = M_1$, obtendremos luego una serie $\{\sum M_n\}$ que no converja, con lo que tratamos de mantener la dependencia de $t$ hasta el final:
                        \begin{align*}
                            \|x_3(t)-x_2(t)\| &\leq \alpha \left|\int_{t_0}^{t} \|x_2(s)-x_1(s)\|~ds \right| \leq \alpha^2 M_0 \left|\int_{t_0}^{t} |s-t_0|~ds \right| \\
                                              &= \alpha^2 M_0 \dfrac{{|t-t_0|}^{2}}{2}
                        \end{align*}
                        En definitiva, se puede probar por inducción que:
                        \begin{equation*}
                            \|x_{n+1}(t) - x_n(t)\| \leq M_0 \dfrac{\alpha^n{|t-t_0|}^{n}}{n!} \qquad \forall t\in I
                        \end{equation*}
                        Definimos ahora:
                        \begin{equation*}
                            M_n = M_0\cdot  \dfrac{{(\alpha\cdot l)}^{n}}{n!} \qquad \forall n\in \mathbb{N} \cup \{0\}
                        \end{equation*}
                        Una serie conocida, con lo que:
                        \begin{equation*}
                            \sum_{n=0}^\infty M_n = M_0\cdot  e^{\alpha\cdot l} \in \mathbb{R}
                        \end{equation*}
                        Por el Test de Weierstrass, concluimos que $\{x_n\}$ converge uniformemente a una función $x:I\rightarrow\mathbb{R}^d$, que por ahora solo sabemos que es continua, por ser $x_n$ continua $\forall n\in \mathbb{N}$.

                        Veamos ahora que $x$ es solución al problema de valores iniciales.  Como:
                        \begin{equation*}
                            \|A(t)x_n(t)-A(t)x(t)\| \leq \alpha \|x_n(t)-x(t)\|
                        \end{equation*}
                        Tenemos que $\{Ax_n\}\rightarrow Ax$, con lo que:
                        \begin{equation*}
                            \int_{t_0}^{t} A(s)x_n(s)~ds  \longrightarrow \int_{t_0}^{t} A(s)x(s)~ds 
                        \end{equation*}
                        En definitiva:
                        \begin{equation*}
                            x(t) = x_0 + \int_{t_0}^{t} [A(s)x(s)+b(s)]~ds 
                        \end{equation*}
                        Aplicando el Teorema Fundamental del Cálculo, tenemos que $x\in C^1(I,\mathbb{R}^d)$:
                        \begin{equation*}
                            x'(t) = A(t)x(t) + b(t) \qquad \forall t\in I
                        \end{equation*}
                        Con lo que $x$ es solución de~(\ref{eq:teo_pvi}) y se tiene que:
                        \begin{equation*}
                            x(t_0) = x_0 + \int_{t_0}^{t_0} [A(s)x(s)+b(s)]~ds  = x_0
                        \end{equation*}
                    \item [Unicidad.] Supongamos que $x,y:I\rightarrow\mathbb{R}^d$ son ambas soluciones de~(\ref{eq:teo_pvi}). Como son soluciones, también cumplen la ecuación integral:
                        \begin{align*}
                            x(t) &= x_0 + \int_{t_0}^{t} [A(s)x(s)+b(s)]~ds \\
                            y(t) &= x_0 + \int_{t_0}^{t} [A(s)y(s)+b(s)]~ds 
                        \end{align*}
                        Restando:
                        \begin{equation*}
                            x(t) - y(t) = \int_{t_0}^{t} [A(s)(x(s)-y(s))]~ds 
                        \end{equation*}
                        Con lo que:
                        \begin{equation*}
                            \|x(t) - y(t)\| = \left\|\int_{t_0}^{t} [A(s)(x(s)-y(s))]~ds \right\| \leq \alpha\left|\int_{t_0}^{t} \|x(s)-y(s)\|~ds \right| \quad \forall t\in I
                        \end{equation*}
                        Tomando $f(t) = \|x(t)-y(t)\|$ $\forall t\in I$, tenemos una función continua no negativa que está en las hipótesis del Lema~\ref{lema:unicidad_teo}, concluimos que $f(t) = 0$ $\forall t\in I$, con lo que $x(t) = y(t)$ $\forall t\in I$.
                \end{description}
            \item De vuelta al caso general, buscamos quitar las hipótesis de que $I$ sea un intervalo acotado. Para ello, tomamos una sucesión $\{I_n\}$ de intervalos abiertos y acotados de forma que $t_0\in I_0$, $I_n\subseteq I_{n+1}$, $\overline{I_n}\subseteq I$ $\forall n\in \mathbb{N}$ y:
                \begin{equation*}
                    \bigcup_{n=0}^\infty I_n = I
                \end{equation*}
                Podemos ahora definir: 
                \begin{equation*}
                    \alpha_n = \max_{t\in \overline{I_n}} \|A(t)\| \qquad \beta_n = \max_{t\in \overline{I_n}} \|b(t)\| \qquad \forall n\in \mathbb{N}
                \end{equation*}
                Con lo que la hipótesis extra anterior se verifica en cada intervalo $I_n$.

                \begin{description}
                    \item [Unicidad.] Si $x,y:I\rightarrow\mathbb{R}^d$ son soluciones de~(\ref{eq:teo_pvi}), entonces $x_{|I_n}$ y $y_{|I_n}$ son soluciones de~(\ref{eq:teo_pvi}) en $I_n$, donde sabemos que se verifica $x(t) = y(t)$ $\forall t\in I_n$, para todo $n\in \mathbb{N}$, con lo que $x(t) = y(t)$ $\forall t\in I$.
                    \item [Existencia.] Si ahora llamamos $x_n$ a la solución del problema de valores iniciales en el intervalo $I_n$, definimos $x:I\rightarrow\mathbb{R}^d$ 
                        \begin{equation*}
                            x(t) = x_n(t) \text{\ si\ } t\in I_n
                        \end{equation*}
                        Es una función bien definida gracias a la unicidad en cada $I_n$, es derivable y cumple la ecuación diferencial porque lo es y la cumple en cada $I_n$.
                \end{description}
        \end{itemize}
    \end{proof}
\end{teo}

\begin{lema}\label{lema:unicidad_teo}
    Sea $J$ un intervalo y $f:J\rightarrow \mathbb{R}^+_0$ continua, sean $t_0\in J$, $\alpha >0$:
    \begin{equation*}
        f(t) \leq \alpha\left|\int_{t_0}^{t} f(s)~ds \right| \quad \forall t\in J \Longrightarrow f(t) = 0 \quad \forall t\in J
    \end{equation*}
    \begin{proof}
        Realizando primero la demostración en un caso más específico:
        \begin{itemize}
            \item Si $J$ es compacto, $\exists \max\limits_{t\in J} f(t) = m$, con lo que:
                \begin{equation*}
                    f(t) \leq \alpha \cdot m\cdot  |t-t_0| \qquad \forall t\in J
                \end{equation*}
                \begin{equation*}
                    f(t) \leq \alpha \left|\int_{t_0}^{t} [\alpha \cdot m\cdot  |s-t_0|]~ds \right| \leq m\cdot  \dfrac{\alpha^2 {|t-t_0|}^{2}}{2}\qquad \forall t\in J
                \end{equation*}
                En definitiva, se puede probar por inducción que:
                \begin{equation*}
                    0\leq f(t) \leq m\cdot  \dfrac{\alpha^n {|t-t_0|}^{n}}{n!} \qquad \forall t\in J
                \end{equation*}
                Como sabemos que la serie de dichos términos converge, sabemos que la sucesión tiende a 0, luego tomando límites llegamos a que $0\leq f(t) \leq 0$ $\forall t\in J$, concluimos que $f(t) = 0$ $\forall t\in J$.
            \item Sea ahora $J$ cualquier intervalo, tomamos $J_n$ un intervalo compacto de forma que $J_n \subseteq J_{n+1}$ con $t_0 \in J_0$ y:
                \begin{equation*}
                    \bigcup_{n=0}^\infty J_n = J
                \end{equation*}
                Por el paso anterior, $f(t) = 0$ $\forall t\in J_n$, para todo $n\in \mathbb{N}$, con lo que $f(t) = 0$ $\forall t\in J$.
        \end{itemize}
    \end{proof}
\end{lema}

\section{Sistemas lineales homogéneos}
\noindent
Nos preocupamos ahora por sistemas de la forma
\begin{equation}\label{eq:lin_sup_h}
    x' = A(t) x
\end{equation}
con $A:I\rightarrow\mathbb{R}^{d\times d}$ una función continua. Sean $V = C^1(I,\mathbb{R}^d)$ y $W=C^0(I,\mathbb{R}^d)$.

\begin{definicion}
    Dado un sistema lineal homogéneo de la forma~(\ref{eq:lin_sup_h}), definimos el operador asociado a la ecuación como la aplicación $L:V\rightarrow W$ dado por:
    \begin{equation*}
        L[x] = x' - Ax
    \end{equation*}
\end{definicion}
Se verifica que el operador lineal $L$ asociado a la ecuación~(\ref{eq:lin_sup_h}) es lineal. Más aún, se verifica que $Z=\ker L$ es el espacio vectorial de las soluciones de la ecuación~(\ref{eq:lin_sup_h}). 

\begin{prop} Dado un sistema lineal homogéneo de la forma~(\ref{eq:lin_sup_h}), con $L$ el operador asociado a la ecuación, se verifica que:
    $$\dim Z = d.$$
    \begin{proof}
        La demostración es similar al caso de una ecuación diferencial lineal de orden superior.\\
        Para ello, fijado $t_0\in I$, definimos $\Phi_{t_0}:Z\rightarrow\mathbb{R}^d$ dada por $\Phi_{t_0}(x) = x(t_0)$, que es un isomorfismo entre $Z$ y $\mathbb{R}^d$ gracias al Teorema~\ref{teo:existencia_unicidad_sistemas}, concluimos que $\dim Z = d$.
    \end{proof}
\end{prop}

Dados $\phi_1,\ldots,\phi_d\in Z$ funciones linealmente independientes en $V$, todas las soluciones de~(\ref{eq:lin_sup_h}) las obtendremos mediante combinaciones lineales de dichas funciones:
\begin{equation*}
    x(t) = c_1\phi_1(t) + \ldots + c_d\phi_d(t) \qquad c_1,\ldots,c_d\in \mathbb{R}
\end{equation*}

\begin{prop}
    Dadas $\phi_1,\ldots,\phi_d\in Z$, son equivalentes:
    \begin{enumerate}
        \item[$i)$] $\{\phi_1,\ldots,\phi_d\}$ es una base.
        \item[$ii)$] $\det(\phi_1(t)|\ldots|\phi_d(t)) \neq 0$ $\forall t\in I$.
        \item[$iii)$] $\det(\phi_1(t)|\ldots|\phi_d(t)) \neq 0$ para cierto $t\in I$.
    \end{enumerate}
    \begin{proof}
        Se deja como ejercicio para el lector por su analogía con el caso de ecuaciones diferenciales lineales de orden superior.
    \end{proof}
\end{prop}~\\

\noindent
Sabemos que la ecuación de la forma~(\ref{eq:lin_sup_h}) no se puede resolver de forma explícita para $d\geq 2$. En el siguiente ejemplo, veremos soluciones de ecuaciones de la forma~(\ref{eq:lin_sup_h}) que sí se pueden resolver de forma explícita.
\begin{ejemplo}
Un primer ejemplo de estos son los sistemas triangulares.
    Consideramos el sistema:
    \begin{equation*}
        \left\{\begin{array}{rl}
                x_1' &= x_1 + x_2 \\
            x_2' &= \frac{1}{t}x_2
        \end{array}\right.
    \end{equation*}
    Estamos trabajando con $I = \mathbb{R}^+$, $d=2$ y:
    \begin{equation*}
        x = \left(\begin{array}{c}
            x_1 \\
            x_2
        \end{array}\right) \qquad A(t) = \left(\begin{array}{cc}
            1 & 1 \\
            0 & \nicefrac{1}{t}
        \end{array}\right) \quad \forall t\in I
    \end{equation*}
    Comenzaremos resolviendo la segunda ecuación y luego sustituyendo en la primera:
    \begin{equation*}
        x_2' = \dfrac{1}{t}x_2
    \end{equation*}
    Sabemos que las soluciones de esta ecuación son de la forma $x_2:I\rightarrow\mathbb{R}$
    \begin{equation*}
        x_2(t) = c_2 t \qquad c_2\in \mathbb{R} \quad t\in I
    \end{equation*}
    Trataremos de resolver ahora la ecuación:
    \begin{equation*}
        x_1' = x_1 + c_2 t
    \end{equation*}
    que es una ecuación lineal completa. Para resolverla, haremos uso de su estructura afín: buscaremos una solución a ojo y le sumaremos las soluciones de su ecuación homogénea. Buscamos con una función de la forma:
    \begin{equation*}
        x_1(t) = \alpha t + \beta \qquad t\in I
    \end{equation*}
    Derivando:
    \begin{equation*}
        \alpha = \alpha t + \beta + c_2 t
    \end{equation*}
    Que nos lleva a unas ecuaciones:
    \begin{equation*}
        \left\{\begin{array}{rl}
                \alpha &= \beta \\
                \alpha + c_2 &= 0
        \end{array}\right. 
    \end{equation*}
    Con lo que la solución particular buscada es:
    \begin{equation*}
        x_1(t) = -c_2(t+1) \qquad t\in I
    \end{equation*}
    Finalmente, una solución de la ecuación completa es $x_1:I\rightarrow\mathbb{R}$ dada por:
    \begin{equation*}
        x_1(t) = -c_2(t+1) + c_1 e^t \qquad c_2\in \mathbb{R} \quad  t\in I
    \end{equation*}

    Por tanto, la solución general del sistema es:
    \begin{equation*}
        x(t)=\begin{pmatrix}
            c_1e^t -c_2(t+1)\\
            c_2t
        \end{pmatrix},\qquad c_2,c_2\in \bb{R},\qquad t\in I
    \end{equation*}

    Para buscar una base que nos dé el espacio de soluciones para el sistema, haremos elecciones de $c_1$ y $c_2$ para obtener dos funciones linealmente independientes. De esta forma, una base la obtenemos con:
    \begin{equation*}
        \phi_1(t) = \left(\begin{array}{c}
            e^t \\
            0
        \end{array}\right) \qquad 
        \phi_2(t) = \left(\begin{array}{c}
            -(t+1) \\
            t
        \end{array}\right) \qquad \forall t\in I
    \end{equation*}
    Que son dos funciones $\phi_1,\phi_2:I\rightarrow\mathbb{R}$ linealmente independientes, ya que:
    \begin{equation*}
        \det(\phi_1(t)|\phi_2(t)) = t\cdot e^t \neq 0 \qquad \forall t\in I
    \end{equation*}
\end{ejemplo}

\subsection{Sistemas de coeficientes constantes}\label{sec:sel_coef_ctes}
\noindent
Un tipo de sistemas que también se puede resolver siempre es cuando la función $A$ es constante. Veamos este ejemplo, donde trabajamos con una matriz $A\in \mathbb{R}^{d\times d}$, con lo que $I=\mathbb{R}$.\\

Supongamos que $\lm \in \sigma(A)\cap \mathbb{R}$ es un valor propio no trivial de $A$, y consideramos $v\in \mathbb{R}^d\setminus\{0\}$ un vector propio asociado a $\lm$. En dicho caso, la función $x:I\rightarrow\mathbb{R}$ dada por
\begin{equation*}
    x(t) = e^{\lm t} \cdot v \qquad t\in I
\end{equation*}
Es una solución del sistema, ya que:
\begin{equation*}
    x'(t) = \lm e^{\lm t} v 
\end{equation*}
Y se tiene que:
\begin{equation*}
    Ax(t) = e^{\lm t} Av = \lm e^{\lm t} v = x'(t) \qquad \forall t\in I
\end{equation*}
De esta forma, ante un sistema de coeficientes constantes en el que la matriz $A$ sea diagonalizable, bastará encontrar los valores y vectores propios de la matriz para hayar las soluciones.

\begin{ejemplo}
    Sea el sistema de ecuaciones diferenciales dado por la matriz:
    \begin{equation*}
        A = \left(\begin{array}{cc}
                3 & 1 \\
                1 & 3
        \end{array}\right)
    \end{equation*}
    Tenemos que $\sigma(A) = \{\lm_1, \lm_2\}$, con $\lm_1 = 4$ y $\lm_2 = -2$. Además, sabemos que los vectores $v_1=(1, 1)$ y $v_2 = (1, -1)$ son vectores propios asociados a dichos valores, respectivamente. De esta forma, sabemos que:
    \begin{equation*}
        \phi_1(t) = e^{4t} \left(\begin{array}{c}
            1 \\
            1
        \end{array}\right) \qquad 
        \phi_2(t) = e^{-2t} \left(\begin{array}{c}
            1 \\
            -1
        \end{array}\right) \qquad \forall t\in I
    \end{equation*}
    Son soluciones del sistema, que además son linealmente independientes, ya que:
    \begin{equation*}
        \det(\phi_1(t)|\phi_2(t)) = e^{2t} \det(v_1|v_2) \neq 0 \qquad \forall t\in I
    \end{equation*}
\end{ejemplo}

\subsubsection{Valores propios complejos}
Dada una matriz $A\in \mathbb{R}^{d\times d}$, si tomamos $\lm \in \sigma(A)\cap (\bb{C}\setminus\bb{R})$, con vector propio $w\in \bb{C}^d\setminus \{0\}$. Lo que haremos ahora será buscar soluciones del sistema en los complejos, es decir, buscar una función $x:\mathbb{R}\rightarrow\bb{C}^d$ pensando en $\bb{C}$ como en $\mathbb{R}^2$: $x = u + iv$. Al obtener una solución compleja $x$, notando por $u,v:\mathbb{R}\rightarrow\mathbb{R}^d$ a su parte real e imaginaria, respectivamente, $u=\Re(x)$ y $v=\Im(x)$, tenemos que:
\begin{equation*}
    \left.\begin{array}{r}
        x' = u' + iv' \\
        x' = Ax = Au + iAv
    \end{array}\right\} \Longrightarrow u' + iv' = A\cdot (u+iv)
    \Longrightarrow
    \left\{\begin{array}{r}
        u' = Au \\
        v' = Av
    \end{array}\right.
\end{equation*}
Por tanto, tenemos que $u$ y $v$ son soluciones \emph{reales} del sistema. Es decir, la parte real e imaginaria de una solución compleja de un sistema de ecuaciones diferenciales lineales homogéneas con coeficientes constantes es solución de dicho sistema.

\begin{ejemplo}
    Si ahora tomamos la matriz:
    \begin{equation*}
        \left(\begin{array}{cc}
                0 & 1 \\
                -1 & 0
        \end{array}\right) \in \mathbb{R}^{2\times 2}
    \end{equation*}
    Es la matriz asociada a la rotación de 90º, que no tiene valores propios reales, sino complejos:
    \begin{equation*}
        \sigma(A) = \{\lm_1 = i, \lm_2 = -i\}
    \end{equation*}
    Con vectores propios asociados $v_1=(1,i)$, $v_2=(1,-i)$ linealmente independientes. Podemos construir una solución compleja:
    \begin{equation*}
        \psi(t) = e^{it} 
        \left(\begin{array}{c}
                1\\
                i
        \end{array}\right) = 
        \left(\begin{array}{c}
                e^{it} \\
                ie^{it}
        \end{array}\right) = 
        \left(\begin{array}{rcl}
                \cos t &+& i\sen t \\
                -\sen t &+& i\cos t
        \end{array}\right) \qquad \forall t\in I
    \end{equation*}
    De donde podemos obtener dos soluciones reales:
    \begin{equation*}
        \phi_1(t) = \left(\begin{array}{c}
            \cos t\\
            -\sen t
        \end{array}\right) \qquad 
        \phi_2(t) = \left(\begin{array}{c}
            \sen t\\
            \cos t
        \end{array}\right) \qquad t\in I
    \end{equation*}
    Que son linealmente independientes, por ser $\det(\phi_1(t)|\phi_2(t))\neq 0$ $\forall t\in I$.\\


    Consideramos ahora otra forma de resolver el mismo sistema:
    \begin{equation*}
        \left\{\begin{array}{rcl}
                x_1' &=& x_2 \\
                x_2' &=& -x_1
        \end{array}\right.
    \end{equation*}
    Para resolverlo, lo pasamos a una ecuación de orden superior. Para ello, derivamos la primera ecuación:
    \begin{equation*}
        x_1'' = x_2' = -x_1
    \end{equation*}

    Despejando, tenemos la siguiente ecuación diferencial lineal de segundo orden, que sabemos resolver por el Capítulo anterior:
    \begin{equation*}
        x_1'' + x_1 = 0
    \end{equation*}
    Su polinomio característico  es $p(\lm)=\lm^2 + 1$, el cual tiene soluciones complejas $\pm i$. Por tanto, dos soluciones linealmente independientes son:
    \begin{align*}
        e^{it}&=\cos t + i\sen t\\
        e^{-it}&=\cos t - i\sen t
    \end{align*}
    Tomando las partes reales e imaginarias como base de $Z$, obtenemos la solución general para $x_1$, y así para $x_2$.
    \begin{align*}
        x_1(t) &= c_1 \cos t + c_2\sen t\\
        x_2(t) &= -c_1\sen t + c_2\cos t
    \end{align*}
\end{ejemplo}

\subsection{Matriz solución y matriz fundamental}
\noindent
Hemos ya trabajado con los sistemas lineales homogéneos, los que tenían la forma~(\ref{eq:lin_sup_h}) con $A:I\rightarrow\mathbb{R}^{d\times d}$ una función continua. Seguiremos ahora trabajando con una ecuación de la misma forma, pero ahora no estaremos interesados en buscar soluciones $x:I\rightarrow\mathbb{R}^d$, sino en buscar \textit{matrices solución}.

\begin{definicion}[Matriz solución]
    Dada una ecuación de la forma~(\ref{eq:lin_sup_h}) siendo $A:I\rightarrow\mathbb{R}^{d\times d}$ una función continua, diremos que una función $\Phi:I\rightarrow\mathbb{R}^{d\times d}$ es una matriz solución de la ecuación si es derivable\footnote{Recordamos que esto es equivalente a que cada componente sea derivable.} y cumple que:
    \begin{equation*}
        \Phi'(t) = A(t)\Phi(t) \qquad \forall t\in I
    \end{equation*}
\end{definicion}

\begin{ejemplo}
    Antes teníamos el sistema $x' = Ax$ dado por:
    \begin{equation*}
        A = \left(\begin{array}{cc}
                0 & 1 \\
                -1 & 0
        \end{array}\right)
    \end{equation*}
    Puede comprobarse que $\Phi:\mathbb{R}\rightarrow\mathbb{R}^{2\times 2}$ dada por:
    \begin{equation*}
        \Phi(t) = \left(\begin{array}{rc}
                \cos t & \sen t \\
                -\sen t & \cos t
        \end{array}\right) \qquad t\in I
    \end{equation*}
    es una matriz solución del sistema. También lo es:
    \begin{equation*}
        \Phi_1(t) = \left(\begin{array}{rc}
                2\cos t & 3\sen t \\
                -2\sen t & 3\cos t
        \end{array}\right) \qquad t\in I
    \end{equation*}
\end{ejemplo}

\begin{observacion}
    Sea $\Phi:I\rightarrow\mathbb{R}^{d\times d}$, si notamos a sus columnas por $\phi_1,\ldots,\phi_d$:
    \begin{equation*}
        \Phi = (\phi_1|\ldots|\phi_d)
    \end{equation*}

    $\Phi$ es una matriz solución de~(\ref{eq:lin_sup_h}) $\Longleftrightarrow $ $\phi_1,\ldots,\phi_d \in Z$.\\

    Esto se debe a que si tenemos una matriz $B=(b_1|\ldots|b_d)$, entonces:
    \begin{equation*}
        A\cdot B = (Ab_1|\ldots|Ab_d)
    \end{equation*}

    Con lo que si se cumple $\Phi'(t) = A(t)\Phi(t)$, entonces:
    \begin{equation*}
        (\phi_1'|\ldots|\phi_d') = A(\phi_1|\ldots|\phi_d)
    \end{equation*}

    y viceversa.
\end{observacion}
\noindent
A partir de esta observación, una matriz solución no es nada más que una matriz cuyas columnas son soluciones.

\begin{definicion}[Matriz fundamental]
    Sea $\Phi$ una matriz solución de un sistema de la forma~(\ref{eq:lin_sup_h}) con $A:I\rightarrow\mathbb{R}^{d\times d}$ una función continua. Si existe\footnote{Esto es equivalente a que el determinante sea distinto de 0 para todo $t\in I$, gracias a la teoría desarrollada.} $t\in I$ tal que $\det(\Phi(t)) \neq 0$, diremos que $\Phi$ es una matriz fundamental.
\end{definicion}

\begin{observacion}
    Notemos que una matriz fundamental es una matriz cuyas columnas forman un sistema fundamental: Si $\Phi=(\phi_1|\ldots|\phi_d)$ es una matriz fundamental, entonces $\{\phi_1,\ldots,\phi_d\}$ es un sistema fundamental.\\

    \noindent
    Como realizar combinaciones lineales de $\phi_1,\ldots,\phi_d$ es equivalente a multiplicar la matriz $\Phi$ por un vector:
    \begin{equation*}
        \alpha_1 \phi_1 + \ldots + \alpha_d \phi_d = \Phi\cdot 
        \left(\begin{array}{c}
            \alpha_1 \\
            \vdots \\
            \alpha_d
        \end{array}\right)
    \end{equation*}
    Obtentemos la identidad:
    \begin{equation*}
        Z = \{\Phi\cdot c \mid c\in \mathbb{R}^d\}
    \end{equation*}
    Notemos que si exigimos solo que $\Phi$ sea una matriz solución (no necesariamente fundamental), solo obtenemos la inclusión:
    \begin{equation*}
        \{\Phi\cdot c \mid c\in \mathbb{R}^d\} \subseteq Z
    \end{equation*}
\end{observacion}

\subsubsection{Derivación del producto de dos matrices}
Con vistas a demostrar una proposición, aprenderemos ahora a derivar un producto de matrices.
\begin{lema}\label{lema:prop_deriv}
    Sean $\Phi,\Psi:I\rightarrow\mathbb{R}^{d\times d}$ funciones derivables, entonces $\Phi\cdot \Psi$ es derivable, con\footnote{Recordemos que el producto de matrices no es conmutativo, luego debemos mantener el orden en la fórmula.}:
    \begin{equation*}
        (\Phi\cdot \Psi)' = \Phi'\cdot \Psi + \Phi\cdot \Psi'
    \end{equation*}
    \begin{proof}
        Si $\Phi={(\phi_{ij})}_{i,j}$ y $\Psi={(\psi_{ij})}_{i,j}$, tenemos que $\phi_{ij},\psi_{ij}$ son derivables, para todo $i,j\in \{1,\ldots,d\}$. Si definimos:
        \begin{equation*}
            \xi_{ij} = \sum_{k=1}^d \phi_{ik}\cdot  \psi_{kj} \qquad \forall i,j \in \{1,\ldots,d\}
        \end{equation*}
        Entonces, tenemos que $\Phi\cdot \Psi = {(\xi_{ij})}_{i,j}$, con cada $\xi_{ij}$ derivable por ser suma de productos de funciones derivables. Ahora, si escribimos el cociente incremental de la función $\Phi\cdot \Psi$:
        \begin{multline*}
            \dfrac{1}{h}[\Phi(t+h)\cdot \Psi(t+h) - \Phi(t)\Psi(t)] =\\= \dfrac{1}{h} [\Phi(t+h)\cdot \Psi(t+h)-\Phi(t+h)\cdot \Psi(t)] + \dfrac{1}{h} [\Phi(t+h)\Psi(t)-\Phi(t)\Psi(t)]
        \end{multline*}
        Sacando factor común:
        \begin{multline*}
            \dfrac{1}{h}[\Phi(t+h)\cdot \Psi(t+h) - \Phi(t)\Psi(t)] =\\= \dfrac{\Phi(t+h)}{h} [\Psi(t+h)-\Psi(t)] + \dfrac{1}{h} [\Phi(t+h)-\Phi(t)] \Psi(t)
        \end{multline*}
        Y si ahora hacemos $h\rightarrow 0$:
        \begin{equation*}
            (\Phi\cdot \Psi)'(t) = \Phi(t)\cdot \Psi'(t) + \Phi'(t) \cdot \Psi(t) =  \Phi'(t)\cdot \Psi(t) +  \Phi(t) \cdot \Psi'(t)  \qquad \forall t\in I
        \end{equation*}
    \end{proof}
\end{lema}


\begin{prop}
    Supongamos que $\Phi$ es una matriz solución y que $C\in \mathbb{R}^{d\times d}$. Entonces $\Phi\cdot C$ es una matriz solución.
    \begin{proof}
        Que $\Phi$ sea una matriz solución significa que es derivable y que:
        \begin{equation*}
            \Phi'(t) = A(t)\cdot \Phi(t) \qquad \forall t\in I
        \end{equation*}
        Por el Lema~\ref{lema:prop_deriv}:
        \begin{equation*}
            (\Phi\cdot C)' = \Phi'\cdot C = (A\cdot \Phi)\cdot C = A\cdot (\Phi \cdot C)
        \end{equation*}
        Con lo que $\Phi\cdot C$ es una matriz solución.
    \end{proof}
\end{prop}

\begin{coro}\label{cor:matriz_fundamental}
    Si $\Phi$ es una matriz fundamental y $C\in \mathbb{R}^{d\times d}$ con $\det(C)\neq 0$. Entonces $\Phi\cdot C$ es una matriz fundamental.
\end{coro}

\begin{ejercicio*}
    Si tenemos una matriz fundamental $\Phi$, podemos obtener todas las bases de soluciones si multiplicamos por cada matriz $C\in \mathbb{R}^{d\times d}$ con $\det(C)\neq 0$.

    La resolución de este caso es parte del Ejercicio~\ref{ej:5.7}, pero se insta al lector a intentar resolverlo por su cuenta.
\end{ejercicio*}

\begin{definicion}[Matriz fundamental principal en un punto]
    Dado $t_0\in I$, decimos que $\Phi$ es una matriz fundamental principal en $t_0$ si $\Phi$ es una matriz fundamental y se verifica que
    \begin{equation*}
        \Phi(t_0) = Id_{d}
    \end{equation*}
\end{definicion}

\begin{ejemplo}
    Si consideramos el sistema anterior $x' = Ax$ dado por la matriz:
    \begin{equation*}
        A = \left(\begin{array}{cc}
                0 & 1 \\
                -1 & 0
        \end{array}\right)
    \end{equation*}
    Sabemos que la matriz $\Phi:I\rightarrow\mathbb{R}$ dada por:
    \begin{equation*}
        \Phi(t) = \left(\begin{array}{cc}
                \cos t & \sen t \\
                -\sen t & \cos t
        \end{array}\right) \qquad t\in I
    \end{equation*}
    Es fundamental. Además, es principal en cualquier punto de la forma $2k\pi$, con $k\in \mathbb{Z}$.
\end{ejemplo}

\begin{prop}
    Dado un sistema lineal homogéneo de la forma~(\ref{eq:lin_sup_h}) y $t_0\in I$, entonces existe una única matriz fundamental principal en $t_0$.
    \begin{proof}
        Demostramos tanto la existencia como la unicidad de la matriz fundamental principal en $t_0$.
        \begin{description}
            \item[Existencia:] Dado un sistema lineal homogéneo de la forma~(\ref{eq:lin_sup_h}), consideramos $Z$, que sabemos que tiene $\dim Z = d$. De esta forma, cogemos $\phi_1,\ldots,\phi_d \in Z$ funciones linealmente independientes y definimos $\Phi:I\rightarrow\mathbb{R}^{d\times d}$ dada por:
            \begin{equation*}
                \Phi(t) = (\phi(t)_1|\ldots|\phi(t)_d)  \qquad t\in I
            \end{equation*}
            De esta forma, $\Phi$ es una matriz fundamental para~(\ref{eq:lin_sup_h}), con lo que su determinante será no nulo: $\det(\Phi(t)) \neq 0$ $\forall t\in I$. Consideramos $\Phi^{-1}(t_0) \in \mathbb{R}^{d\times d}$, con $\det(\Phi^{-1}(t_0)) \neq 0$, por lo que aplicando el Corolario~\ref{cor:matriz_fundamental}, tenemos que la función $\Phi\cdot \Phi^{-1}(t_0):I\rightarrow\mathbb{R}^{d\times d}$ es una matriz fundamental, que verifica que:
            \begin{equation*}
                (\Phi\cdot \Phi^{-1}(t_0))(t_0) = \Phi(t_0)\cdot \Phi^{-1}(t_0) = Id_d
            \end{equation*}
            Por tanto, $\Phi\cdot \Phi^{-1}(t_0)$ es una matriz fundamental principal en $t_0$.

            \item[Unicidad:] Supongamos ahora que $\Phi,\Psi:I\rightarrow\mathbb{R}^{d\times d}$ son dos matrices fundamentales principales en $t_0\in I$. Si notamos a sus columnas por:
            \begin{equation*}
                \Phi = (\phi_1|\ldots|\phi_d) \qquad \Psi = (\psi_1|\ldots|\psi_d)
            \end{equation*}
            con $\phi_i,\psi_i:I\rightarrow\mathbb{R}^d$ $\forall i \in \{1,\ldots,d\}$. Como $\Phi(t_0) = Id_d = \Psi(t_0)$, tenemos que tanto $\phi_i$ como $\psi_i$ son ambas soluciones de la ecuación~(\ref{eq:lin_sup_h}) para la condición inicial $t_0\in I$, $\alpha_i =(0,\ldots,0,\stackrel{i)}{1},0,\ldots,0)\in \mathbb{R}$, para cada $i \in \{1,\ldots,d\}$. Sin embargo, el Teorema~\ref{teo:existencia_unicidad_sistemas} nos garantiza la unicidad de dichas soluciones, con lo que $\phi_i = \psi_i$ $\forall i \in \{1,\ldots,d\}$.
    
            Luego $\Phi = \Psi$.
        \end{description}
    \end{proof}
\end{prop}

\begin{ejercicio*}
    Existe una Fórmula de Jacobi-Liouville para sistemas:

    Dada una matriz solución $\Phi(t)$ de~(\ref{eq:lin_sup_h}), tomamos $t_0\in I$. Resulta que:
    \begin{equation*}
        \det\Phi(t) = \det\Phi(t_0)\cdot e^{\displaystyle \int_{t_0}^{t} \operatorname{tr} (A(s))~ds } \qquad \forall t\in I
    \end{equation*}
    Donde notamos por $\operatorname{tr}(A(s))$ a la traza de la matriz $A$ en el punto $s\in \mathbb{R}$. Se pide:
    \begin{itemize}
        \item Demostrar la fórmula.
        \item Ver que la fórmula del Capítulo anterior es un caso particular de esta.
    \end{itemize}
    (\textbf{Pista}: derivar la función $\det\Phi(t)$, sacar una ecuación diferencial de primer orden de la que es solución y comprobar que la expresión de la derecha también es solución del mismo problema de valores iniciales.)
\end{ejercicio*}

\section{Exponencial de una matriz}
\noindent
Como motivación, volvemos a la ecuación diferencial del inicio del curso:
\begin{equation*}
    x' = \lm x
\end{equation*}
Tenemos que una solución suya viene dada por $x:\mathbb{R}\rightarrow\mathbb{R}$
\begin{equation*}
    x(t) = e^{\lm t} x_0 \qquad t\in \mathbb{R}
\end{equation*}
para cierto $x_0\in \mathbb{R}$. Si ahora consideramos el sistema de ecuaciones homogéneo siguiente
\begin{equation*}
    x' = Ax,
\end{equation*}
sería lógico pensar que las soluciones del sistema serán de la forma:
\begin{equation*}
    x(t) = e^{tA} x_0
\end{equation*}
Pero, ¿qué es la exponencial de una matriz?

\subsection{Definición de exponencial de una matriz}
Recordando la definición de la exponencial, dado $\lm \in \mathbb{R}$, $e^\lm$ se define como:
\begin{equation*}
    e^\lm = \sum_{n=0}^{\infty}\dfrac{\lm^n}{n!}
\end{equation*}
Es decir, el límite de una serie de potencias, pero ¿cómo podemos generalizar este límite a las matrices? Pues bien, podemos pensar intuitivamente en este límite como en un ``polinomio de grado infinito'', y en asignaturas pasadas\footnote{Como en Geometría II.} aprendimos ya que dado un polinomio, por ejemplo $p(\lm)=\lm^3 - \lm + 3$, podemos cambiar su dominio de definición (usualmente $\mathbb{R}$) a $\mathbb{R}^{d\times d}$, a partir de la fórmula:
\begin{equation*}
    P(A) = A^3 - A + 3I \qquad A\in \mathbb{R}^{d\times d}
\end{equation*}
Por lo que ya sabemos evaluar polinomios en matrices. Antes de definir qué es la exponencial de una función, es necesario antes ver ciertos resultados, para poder realizar dicha definición.

\begin{lema}\label{lema:1_exp}
    Sea $\{A_n\}$ con $A_n\in \mathbb{R}^{d\times d}$ $\forall n\in \mathbb{N}$ una sucesión de matrices y $\{M_n\}$ con $M_n\in \mathbb{R}$ $\forall n\in \mathbb{N}$ una sucesión de números reales de forma que $\|A_n\|\leq M_n$ $\forall n\in \mathbb{N}$. Entonces:
    \begin{equation*}
        \sum_{n=0}^{\infty}M_n < \infty
        \Longrightarrow
        \sum_{n=0}^{\infty}A_n < \infty
    \end{equation*}
\end{lema}

\begin{lema}\label{lema:2_exp}
    La siguiente serie de matrices es convergente: 
    \begin{equation*}
        \left\{\sum_{n=0}^{k}\dfrac{1}{n!}A^n\right\}
    \end{equation*}
    \begin{proof}
        Si consideramos la sucesión $\left\{\dfrac{1}{n!}A^n\right\}$, tenemos que:
        \begin{equation}\label{eq:desigualdad_exp}
            \left\|\dfrac{1}{n!}A^n\right\| = \dfrac{1}{n!} \|A^n\| \stackrel{(\ast)}{\leq }\dfrac{1}{n!}\|A\|^n \qquad \forall n\in \mathbb{N}
        \end{equation}
        Donde en $(\ast)$ hemos usado que $\|\cdot \|$ es una norma matricial, con lo que se da que $\|A\cdot B\| \leq \|A\|\|B\|$ para cualesquiera matrices $A$ y $B$, que puede generalizarse para todo $n\in \mathbb{N}$ fácilmente por inducción. Si ahora definimos la sucesión $\{M_n\}$ de forma que:
        \begin{equation*}
            M_n = \dfrac{1}{n!}\|A\|^n \qquad \forall n\in \mathbb{N}
        \end{equation*}
        la desigualdad~(\ref{eq:desigualdad_exp}) nos da que $\|A_n\|\leq M_n$ $\forall n\in \mathbb{N}$. Por tanto, de la definición de exponencial, tenemos que:
        \begin{equation*}
            \sum_{n=0}^{\infty} M_n = e^{\|A\|} < \infty
        \end{equation*}
        Por el Lema~\ref{lema:1_exp}, tenemos que la serie anterior es convergente, como queríamos demostrar.
    \end{proof}
\end{lema}

\begin{definicion}[Exponencial de una matriz cuadrada]
    Sea $A\in \mathbb{R}^{d\times d}$ una matriz cuadrada, definimos:
    \begin{equation*}
        e^A = \sum_{n=0}^{\infty} \dfrac{1}{n!} A^n
    \end{equation*}
    que sabemos que es convergente por el Lema~\ref{lema:2_exp}.
\end{definicion}

\begin{observacion}
    Notemos que en el Lema~\ref{lema:2_exp}, además de probar que la serie que nos da la definición de la exponencial de una matriz es convergente, habíamos conseguido probar que:
    \begin{equation*}
        \|e^A\| \leq e^{\|A\|} \qquad \forall A\in \mathbb{R}^{d\times d}
    \end{equation*}
\end{observacion}

\noindent
Una vez definida la exponencial de una matriz, la única forma que tenemos de calcularla para una matriz dada es a partir de la definición, por lo que tendremos que calcular el límite de una serie de potencias matriciales.

Parece lógico pensar que las matrices para las cuales es fácil calcular su exponencial son aquellas para las que es fácil calcular sus potencias, tal y como pondremos de manifiesto en el siguiente ejemplo, donde aprendemos a calcular la exponencial de las matrices más sencillas.
\begin{ejemplo}
    Calcularemos a continuación la exponencial de varias matrices, de forma que para estas es fácil calcular su exponencial.
    
    \begin{enumerate}
        \item Calculemos $e^0$, con $0\in \mathbb{R}^{d\times d}$:
            \begin{equation*}
                e^0 = \sum_{n=0}^{\infty} \dfrac{1}{n!}0^n = Id_d
            \end{equation*}
        donde hemos hecho uso de que, en $\bb{R}$, $0^0=1$.

        \item Si ahora tratamos de calcular la exponencial de cualquier matriz diagonal:
            \begin{equation*}
                A = \left(\begin{array}{cccc}
                        \lm_1 & 0 & \cdots & 0 \\
                        0 & \lm_2 & \cdots & 0 \\
                        \vdots & \vdots & \ddots & \vdots \\
                        0 & 0 & \cdots & \lm_d
                \end{array}\right) \qquad \lm_1,\lm_2,\ldots,\lm_d\in \mathbb{R}
            \end{equation*}
            Sabemos que:
            \begin{equation*}
                A^n = \left(\begin{array}{cccc}
                        \lm_1^n & 0 & \cdots & 0 \\
                        0 & \lm_2^n & \cdots & 0 \\
                        \vdots & \vdots & \ddots & \vdots \\
                        0 & 0 & \cdots & \lm_d^n
                \end{array}\right)  \qquad \forall n\in \mathbb{N}
            \end{equation*}
            De esta forma:
            \begin{multline*}
                e^A = \frac{1}{0!}\left(\begin{array}{cccc}
                        1 & 0 & \cdots & 0 \\
                        0 & 1 & \cdots & 0 \\
                        \vdots & \vdots & \ddots & \vdots \\
                        0 & 0 & \cdots & 1
                \end{array}\right) + 
                \frac{1}{1!}
                \left(\begin{array}{cccc}
                     \lm_1 & 0 & \cdots & 0 \\
                     0 & \lm_2 & \cdots & 0 \\
                    \vdots & \vdots & \ddots & \vdots \\
                     0 & 0  & \cdots & \lm_d
                \end{array}\right) + \\ + \ldots +
                \frac{1}{k!}\left(\begin{array}{cccc}
                     \lm_1^k & 0 & \cdots & 0 \\
                     0 & \lm_2^k & \cdots & 0 \\
                    \vdots & \vdots & \ddots & \vdots \\
                     0 & 0  & \cdots & \lm_d^k
                \end{array}\right) + \ldots
            \end{multline*}
            con lo que obtenemos la matriz diagonal de forma que en la posición $i,i$ de la matriz (con $i \in \{1,\ldots,d\}$) tenemos:
            \begin{equation*}
                \sum_{n=0}^{\infty} \dfrac{1}{n!}\lm_i^n = e^{\lm_i}
            \end{equation*}
            Es decir, en cada componente de la diagonal tenemos el desarrollo en serie de la exponencial de cada $\lm_i$, con lo que:
            \begin{equation*}
                e^A = \left(\begin{array}{cccc}
                        e^{\lm_1} & 0 & \cdots & 0 \\
                        0 & e^{\lm_2} & \cdots & 0 \\
                        \vdots & \vdots & \ddots & \vdots \\
                        0 & 0 & \cdots & e^{\lm_d}
                \end{array}\right)
            \end{equation*}
        \item Si ahora calculamos la exponencial de cualquier matriz nilpotente (cualquier matriz que tenga una potencia nula), sucederá algo parecido a lo que nos sucedía con la exponencial, y es que la serie se convierte en una suma finita.

            Un ejemplo muy representativo de esto es la matriz:
            \begin{equation*}
                A = \left(\begin{array}{ccccc}
                        0 & 1 & 0 & \cdots & 0 \\
                        0 & 0 & 1 & \cdots & 0 \\
                        \vdots & \vdots & \ddots & \ddots & \vdots \\
                        0 & 0 & 0 & \ddots & 1 \\
                        0 & 0 & 0 & \cdots & 0
                \end{array}\right)
            \end{equation*}
            Es decir, la matriz cuyas componentes son todo ceros salvo la diagonal que se encuentra por encima de la diagonal princpal, cuyos componentes son todo unos. Esta matriz cuenta con una propiedad especial, y es que en cada potencia de la matriz la diagonal de los unos asciende un nivel (compruébese), con lo que si seguimos calculando potencias, obtenemos finalmente que:
            \begin{equation*}
                A^{d-1} = \left(\begin{array}{cccc}
                        0 & 0 & \cdots & 1 \\
                        0 & 0 & \cdots & 0 \\
                        \vdots & \vdots & \ddots & \vdots \\
                        0 & 0  & \cdots & 0
                \end{array}\right) \qquad A^d = \left(\begin{array}{cccc}
                     0 & 0 & \cdots & 0\\
                     0& 0 & \cdots & 0\\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & \cdots &0 
                \end{array}\right) = 0
            \end{equation*}
            Por lo que $A^k = 0$ para cualquier $k\geq d$. De esta forma, solo tenemos que calcular una suma finita para calcular la exponencial de una matriz, que a su vez es fácil de calcular:
            \begin{equation*}
                e^A = \left(\begin{array}{ccccc}
                        1 & 1 & \nicefrac{1}{2!} & \cdots & \nicefrac{1}{(d-1)!} \\
                        0 & 1 & \ddots & \ddots & \vdots \\
                        0 & 0 & \ddots & \ddots & \nicefrac{1}{2!} \\
                        \vdots & \vdots & \ddots & \ddots & 1 \\
                        0 & 0 & \cdots & \cdots & 1 
                \end{array}\right)
            \end{equation*}
    \end{enumerate}
    Y estas son las matrices más fáciles para las que se puede obtener la exponencial de una matriz.
\end{ejemplo}

Hay métodos alternativos que nos permiten calcular la exponencial de cualquier matriz, entre los que distinguimos:
\begin{itemize}
    \item Un método algebraico con muchos cálculos que no nos interesará.
    \item Un método basado en ecuaciones diferenciales, que justificará por qué nos interesan las exponenciales de las matrices y que veremos a continuación.
\end{itemize}

\subsection{Formas de cálculo de exponenciales de matrices}
\begin{prop}\label{prop:equiv_exp_ec}
    Dado un sistema de la forma:
    \begin{equation}\label{eq:matriz_cte}
        x' = Ax
    \end{equation}
    con $A\in \mathbb{R}^{d\times d}$. La función $\Phi:\mathbb{R}\rightarrow\mathbb{R}^{d\times d}$ dada por:
    \begin{equation*}
        \Phi(t) = e^{tA} \qquad t\in \mathbb{R}
    \end{equation*}
    es la matriz fundamental de~(\ref{eq:matriz_cte}) principal en $t_0 = 0$.
    \begin{proof}
        La demostración se podría hacer si previamente aprendemos a derivar series de potencias matriciales, algo que no haremos, por lo que optamos por esta otra demostración, que podemos realizar tras la demostración del Teorema~\ref{teo:existencia_unicidad_sistemas}.
        Calculemos las iterantes de Picard para la ecuación~(\ref{eq:matriz_cte}) con cualquiera condición inicial $x(0) = x_0\in \mathbb{R}^d$, viendo cómo es la sucesión $\{x_n(t)\}$:
        \begin{align*}
            x_0(t) &= x_0 \\
            x_{n+1}(t) &= x_0 + \int_{t_0}^{t} Ax_n(s)~ds 
        \end{align*}
        Por la teoría vista en la demostración del Teorema~\ref{teo:existencia_unicidad_sistemas}, sabemos que $\{x_n(t)\}$ converge uniformemente a una función $x$ en $I\subseteq \mathbb{R}$, con $I$ un intervalo acotado y que $x$ era solución del problema con las condiciones iniciales. Calculemos una expresión para dicha $x$:
        \begin{equation*}
            x_1(t) = x_0 + \int_{0}^{t} Ax_0~ds  = x_0 + tAx_0 = (I+tA)x_0
        \end{equation*}
        Que es un polinomio de primer grado en $t$.
        \begin{equation*}
            x_2(t) = x_0 + \int_{0}^{t} A(I+sA)x_0~ds = x_0 + tAx_0 + \dfrac{s^2}{2}A^2 x_0 = \left(I+tA+\dfrac{t^2}{2}A^2\right)x_0
        \end{equation*}
        Que es un polinomio de segundo grado en $t$. Por inducción se podría probar que:
        \begin{equation*}
            x_n(t) = \left(I + tA + \dfrac{t^2}{2}A^2 + \cdots + \dfrac{t^n}{n!}A^n\right) x_0 \qquad \forall n\in \mathbb{N}
        \end{equation*}
        Por una parte, sabemos que el paréntesis converge a $e^{tA}$ y que $x_0$ es un vector constante. Como el producto de matrices por vectores es una operación continua, tenemos que $\{x_n(t)\}\rightarrow e^{tA}x_0$. Por otra parte, sabemos que las iterantes de Picard convergen absolutamente a la solución del problema de valor inicial, $x(t)$. Por tanto, deducimos que:
        \begin{equation*}
            x(t) = e^{tA} x_0
        \end{equation*}
        Para cualquier $x_0\in \mathbb{R}^d$.\\

        Para finalizar la demostración, hemos de probar tres cosas:
        \begin{itemize}
            \item Que $\Phi$ es una matriz solución de~(\ref{eq:matriz_cte}).
            \item Que $\Phi$ es una matriz fundamental.
            \item Que $\Phi(0) = I$, para tener que es principal en 0.
        \end{itemize}

        \begin{enumerate}
            \item Ver que $\Phi$ es una matriz solución de~(\ref{eq:matriz_cte}) es equivalente a ver que sus columnas son soluciones vectoriales de la misma ecuación. Para ello, escribimos cómo son las columnas de $\Phi$:
                \begin{equation*}
                    \Phi(t) = (\phi_1(t)|\ldots|\phi_d(t)) = (\Phi(t)e_1|\ldots|\Phi(t)e_d)
                \end{equation*}
                donde hemos notado por $e_i$ al $i$-ésimo vector de la base canónica de $\mathbb{R}^d$. Sin embargo, antes vimos que cualquier función definida de la forma:
                \begin{equation*}
                    x_v(t) = e^{tA} v \qquad v\in \mathbb{R}^d
                \end{equation*}
                Es una solución de~(\ref{eq:matriz_cte}), por lo que $\Phi(t)e_i$ es solución de la ecuación, para cualquier $e_i$ vector de la base canónica, de donde deducimos que $\Phi$ es una matriz solución de~(\ref{eq:matriz_cte}).
            \item[3.] Tenemos que:
                \begin{equation*}
                    \Phi(0) = e^0 = I
                \end{equation*}
            \item[2.] Como $\Phi(0) = I$, tenemos que $\det(\Phi(0))=1\neq 0$, por lo que $\Phi$ es matriz fundamental de~(\ref{eq:matriz_cte}), y en el punto 3 vimos que es principal en 0.
        \end{enumerate}
    \end{proof}
\end{prop}
\begin{observacion}
    Notemos que la Proposición~\ref{prop:equiv_exp_ec} nos da una equivalencia entre los sistemas de ecuaciones lineales y el cálculo de exponenciales de una matriz:
    \begin{itemize}
        \item Si sabemos calcular $e^{tA}$, sabemos ya resolver el sistema $x'=Ax$, ya que cualquier función vectorial de la forma $e^{tA}\cdot v$ será solución, independientemente del $v\in \mathbb{R}^d$ escogido.
        \item Si ahora tenemos un sistema $x'=Ax$ y queremos calcular $e^{tA}$, si resolvemos el sistema, obtenemos una matriz fundamental $\Phi:\mathbb{R}\rightarrow\mathbb{R}^{d\times d}$. Sin embargo, esta no tiene por qué ser la matriz fundamental principal en 0. A pesar de ello, anteriormente vimos en el Corolario~\ref{cor:matriz_fundamental}, que nos permite realizar el cálculo ($\det(\Phi(t))\neq 0$ $\forall t\in \mathbb{R}$):
            \begin{equation*}
                \Phi(t)\Phi(0)^{-1}
            \end{equation*}
            Obteniendo una matriz fundamental que además es principal en $0$, ya que:
            \begin{equation*}
                \Phi(0)\Phi(0)^{-1} = I
            \end{equation*}
            Por lo que dada cualquier matriz fundamental de~(\ref{eq:matriz_cte}), ya sabemos calcular la exponencial de la matriz $A$:
            \begin{equation*}
                e^{At} = \Phi(t)\Phi(0)^{-1}
            \end{equation*}
    \end{itemize}
\end{observacion}

\subsection{Casos de cálculo de la exponencial de una matriz}
Dada una matriz $A\in \mathbb{R}^{d\times d}$, veamos ahora varios casos de cálculo de $e^A$. Cada nuevo caso de cálculo engloba a los anteriores, pero el proceso para conseguir $e^A$ es más difícil cuanto más general sea el caso.

\subsubsection{Si $A$ es diagonalizable en $\bb{R}$}
\noindent
En dicho caso, tendremos $\lm_1,\ldots,\lm_d\in \mathbb{R}$ valores propios reales de forma que podamos encontrar $v_1,\ldots,v_d\in \mathbb{R}^d$ vectores linealmente independientes\footnote{Luego forman una base.} teniendo que $v_i$ sea un vector propio del valor propio $\lm_i$, $\forall i \in \{1,\ldots,d\}$. Vimos en la Sección~\ref{sec:sel_coef_ctes} que si teníamos un sistema de la forma $x'=Ax$, entonces las funciones:
\begin{equation*}
    x_{v_i}(t) = e^{\lm_i t} \cdot v_i \qquad i \in \{1,\ldots,d\}
\end{equation*}
eran solución del sistema. En esta situación, podemos producir una matriz solución, $\Phi:\mathbb{R}\rightarrow\mathbb{R}^{d\times d}$ de la forma:
\begin{equation*}
    \Phi(t) = (x_{v_1}(t)|\ldots|x_{v_d}(t)) \qquad t\in \mathbb{R}
\end{equation*}
Que es una matriz fundamental, ya que:
\begin{equation*}
    \det(\Phi(0)) = \det(v_1 | \ldots | v_d) \neq 0
\end{equation*}
Por ser $\cc{B}=\{v_1,\ldots,v_d\}$ una base de $\mathbb{R}^d$, con lo que finalmente podemos aplicar la siguiente fórmula:
\begin{equation*}
    e^{tA} = \Phi(t)\cdot \Phi(0)^{-1} \qquad \forall t\in \mathbb{R}
\end{equation*}
Para obtener $e^{tA}$.
\begin{ejemplo}
    Buscamos calcular la exponencial de la matriz:
    \begin{equation*}
        A = \left(\begin{array}{cc}
                1 & 3 \\
                3 & 1 \\
        \end{array}\right)
    \end{equation*}
    $A$ tiene valores propios $\lm_1=4$ y $\lm_2=-2$, de forma que los vectores $v_1 = (1,1)$ y $v_2=(1,-1)$ forman una base de $\mathbb{R}^2$ de vectores propios. Por tanto, la matriz:
    \begin{equation*}
        \Phi(t) = \left(\begin{array}{cc}
                e^{4t} & e^{-2t} \\
                e^{4t} & -e^{-2t} 
        \end{array}\right)
    \end{equation*}

    es una matriz fundamental de $x' = Ax$, con:
    \begin{equation*}
        \Phi(0) = \left(\begin{array}{cc}
                1 & 1 \\
                1 & -1
        \end{array}\right)
    \end{equation*}
    Y bastaría calcular $\Phi(t)\cdot \Phi(0)^{-1}$ para obtener $e^{tA}$.
\end{ejemplo}

\subsubsection{Si $A$ es diagonalizable en $\bb{C}$}
En dicho caso, tendremos $r$ valores propios reales: $\lm_1,\ldots,\lm_r\in \mathbb{R}$ y $r$ vectores propios reales linealmente independientes, $v_1,\ldots,v_r\in \mathbb{R}^d$. Además, tendremos $d-r$ valores propios complejos, de forma que cuando tengamos un valor propio complejo, su conjugado también será un valor propio complejo, por lo que tendremos los valores propios complejos $\mu_1,\ldots,\mu_s,\overline{\mu_1},\ldots,\overline{\mu_s}\in \bb{C}$ de forma que $2s = d-r$. Para cada valor propio complejo, tendremos un vector propio que sea linealmente independiente del resto, por lo que tendremos como vectores propios $w_1,\ldots,w_s,\overline{w_1},\ldots,\overline{w_s}\in \bb{C}^d$ de forma que:
\begin{equation*}
    \cc{B} = \{v_1,\ldots,v_r,w_1,\overline{w_1},\ldots,w_s,\overline{w_s}\}
\end{equation*}
Sea una base de vectores propios de $\mathbb{R}^d$. Construiremos las funciones:
\begin{align*}
    x_{v_i}:\mathbb{R}\rightarrow\mathbb{R}^d &\qquad i \in \{1,\ldots,r\} \\
    x_{w_j}:\mathbb{R}\rightarrow\mathbb{C}^d &\qquad j \in \{1,\ldots,s\} 
\end{align*}

dadas por:
\begin{align*}
    x_{v_i}(t) &= e^{\lm_i t} \cdot v_i \qquad t\in \mathbb{R}, \quad  i \in \{1,\ldots,r\} \\
    x_{w_j}(t) &= e^{\mu_j t}\cdot w_j \qquad t\in \mathbb{R},\quad  j\in \{1,\ldots,s\}
\end{align*}
Notemos que hemos usado solo los vectores propios $w_j$ y que no hemos usado los vectores $\overline{w_j}$, ya que cuando tengamos $w_j\in \bb{C}^d$, entonces los vectores:
\begin{align*}
    Re(z) &= \dfrac{1}{2}w_j + \dfrac{1}{2}\overline{w_j} \\
    Im(z) &= \dfrac{1}{2i}w_j - \dfrac{1}{2i} \overline{w_j}
\end{align*}
Son linealmente independientes, y estos serán los que nos interesen\footnote{Por lo que a pesar de no considerar las funciones $x_{\overline{w_j}}$, las consideramos de forma implícita al considerar las funciones $x_{w_j}$ y luego considerar sus partes real e imaginaria.}. Definimos ahora:
\begin{equation*}
    \psi_j(t) = Re(x_{w_j}(t)) \qquad \tilde{\psi_j}(t) = Im(x_{w_j}(t)) \qquad \forall t\in \mathbb{R}
\end{equation*}
Anteriormente vimos que $x_{w_j}$, $\psi_j$ y $\tilde{\psi_j}$ son soluciones de $x'=Ax$, por lo que la matriz $\Phi:\mathbb{R}\rightarrow\mathbb{R}^{d\times d}$ dada por:
\begin{equation*}
    \Phi(t) = \left(x_{v_1}(t)|\ldots|x_{v_r}(t)|\psi_1(t)|\tilde{\psi_1}(t)|\ldots|\psi_s(t)|\tilde{\psi_s}(t)\right) \qquad t\in \mathbb{R}
\end{equation*}
Es una matriz solución, que además es fundamental por ser $\cc{B}$ una base. Finalmente, aplicamos la fórmula:
\begin{equation*}
    e^{tA} = \Phi(t)\Phi(0)^{-1}
\end{equation*}

\begin{ejemplo}
    Buscamos ahora calcular la exponencial de matriz:
    \begin{equation*}
        \left(\begin{array}{cc}
                0 & 1 \\
                -1 & 0
        \end{array}\right)
    \end{equation*}
    Esta tiene como valores propios $\mu_1 = i$ y $\mu_2 = -i$, de forma que los vectores $w_1=(1,i)$ y $w_2=(1,-i)$ forman una base de $\mathbb{C}^2$ de vectores propios. Anteriormente vimos que las funciones:
    \begin{equation*}
        e^{it}\cdot w_1 \qquad e^{-it} \cdot w_2
    \end{equation*}
    eran solución de $x'=Ax$. Con la primera (la segunda se obtiene con el conjugado) obtenemos las funciones:
    \begin{align*}
        \psi_1(t) &= Re(e^{it}, ie^{it}) = (\cos t, -\sen t) \\
        \wt{\psi_1}(t) &= Im(e^{it}, ie^{it})  = (\sen t, \cos t)
    \end{align*}
    Con lo que podemos construir la matriz $\Phi:\mathbb{R}\rightarrow\mathbb{R}^{2\times 2}$:
    \begin{equation*}
        \Phi(t) = \left(\begin{array}{cc}
                \cos t & \sen t \\
                -\sen t & \cos t 
        \end{array}\right)
    \end{equation*}
    que es solución y fundamental por ser $\{(1,i),(1,-i)\}$ una base de $\mathbb{C}^2$. Adicionalmente, hemos tenido la suerte de que:
    \begin{equation*}
        \Phi(0) = \left(\begin{array}{cc}
                1 & 0 \\
                0 & 1
        \end{array}\right) = I
    \end{equation*}
    Con lo que $e^{tA} = \Phi(t)$ $\forall t\in \mathbb{R}$.\\

    Como curiosidad, vemos que obtenemos la generalización de la fórmula:
    \begin{equation*}
        e^{i\pi} +1 = 0
    \end{equation*}

    ya que:
    \begin{equation*}
        e^{\begin{pmatrix}
        0 & \pi \\
        -\pi & 0
        \end{pmatrix}} +I=0
    \end{equation*}
\end{ejemplo}

\begin{ejercicio*}
    Se deja como ejercicio obtener la fórmula de $e^{tA}$ a partir de su propia definición. Resulta un buen ejercicio para saber manejar bien las series.
\end{ejercicio*}

\subsubsection{Cualquier matriz $A$. Forma canónica de Jordan}
Para conseguir todas las casuísticas de cálculo de la exponencial de una matriz, es necesario tener en cuenta las matrices no diagonalizables. Para ello, es necesario antes estudiar la forma canónica de Jordan de una matriz cualquiera $A\in \mathbb{R}^{d\times d}$.\\

\noindent
Dada una matriz $A\in \mathbb{R}^{d\times d}$, existen $P,J\in \mathbb{R}^{d\times d}$ de forma que:
\begin{equation*}
    A = PJP^{-1}
\end{equation*}
Con $J$ una matriz diagonal por bloques, a la que llamaremos forma de Jordan de\footnote{Esta es única salvo conmutaciones de los bloques.} la matriz $A$. Es decir, $J$ es de la forma:
\begin{equation*}
    J = \left(\begin{array}{cccc}
         J_1 &  &  & \\
         & J_2 &   & \\
         &  & \ddots &  \\
         &  &  &  J_r
    \end{array}\right)
\end{equation*}

con:
\begin{equation*}
    J_i = \left(\begin{array}{ccccc}
            \lm_i & 1 & 0 & \cdots & 0 \\
            0 & \lm_i & 1 & \cdots & 0 \\
         \vdots & \vdots & \ddots & \ddots & \vdots \\
        0& 0 &   &\ddots & 1 \\
        0& 0 & 0 &\cdots & \lm_i 
    \end{array}\right) \qquad \lm_i \in \mathbb{R}, \quad \forall i \in \{1,\ldots,r\}
\end{equation*}
De esta forma, se verifica que una matriz es diagonalizable si y solo si tiene tantas cajas de Jordan como dimensiones ($r=d$), es decir, si todas sus cajas de Jordan son diagonalizables (y por tanto, de dimensión 1). 

\begin{ejemplo}
    Sea $A\in \mathbb{R}^{3\times 3}$ una matriz de forma que su polinomio característico es de la forma $p(\lm) = {(\lm-3)}^{3}$. Es decir, que tiene como valor propio 3 con multiplicidad algebraica 3, las posibes formas de Jordan para esta matriz $A$ son:
    \begin{equation*}
        \left(\begin{array}{ccc}
                3 & & \\
                  &3  & \\
                  &  &3 
        \end{array}\right) \qquad 
        \left(\begin{array}{ccc}
                3 & 1& \\
                  &3  & \\
                  &  &3 
        \end{array}\right)\qquad 
        \left(\begin{array}{ccc}
                3 & 1& \\
                  &3  & 1 \\
                  &  &3 
        \end{array}\right)
    \end{equation*}
    Que se corresponden con que la matriz $A$ tenga multiplicidad geométrica 3 (y por tanto sea diagonalizable), 2 y 1, respectivamente.\\

    Una vez que sepamos cual es la forma de Jordan de una matriz dada (por ejemplo, razonando por la multiplicidad geométrica de los valores propios), hallar una matriz $P$ de cambio será sencillo, ya que bastará con resolver el sistema:
    \begin{equation*}
        AP = PJ
    \end{equation*}
    imponiendo que $|P|\neq 0$ para que $P$ sea invertible.
\end{ejemplo}

Veremos ahora cómo podemos calcular la exponencial de cualquier matriz si previamente conocemos su forma canónica de Jordan. Para ello, será necesario primero ver un par de resultados.

\begin{lema}\label{lema:potencias_semejantes}
    Si dos matrices son semejantes, entonces sus potencias son semejantes. Dicho de otra forma, si $A,B,P\in \mathbb{R}^{d\times d}$ de forma que:
    \begin{equation*}
        A = PBP^{-1}
    \end{equation*}
    
    Entonces:
    \begin{equation*}
        A^n = PB^n P^{-1} \qquad \forall n\in \mathbb{N}
    \end{equation*}
    \begin{proof}
        Para $n\in \{0,1\}$ la demostración es trivial. Procedemos por inducción sobre $n$:
        \begin{description}
            \item [Para $n=2$:]~\\
                \begin{equation*}
                    A^2 = A\cdot A = (PBP^{-1})(PBP^{-1}) \AstIg PB^2P^{-1}
                \end{equation*}
                Donde en $(\ast)$ hemos aplicado que el producto de matrices es asociativo.
            \item [Supuesto para $n$, veámoslo para $n+1$:] 
                \begin{equation*}
                    A^{n+1} = A^n \cdot A = (PB^{n}P^{-1})(PBP^{-1}) \AstIg PB^{n+1}P^{-1}
                \end{equation*}
                Donde en $(\ast)$ hemos vuelto a aplicar que el producto de matrices es asociativo.
        \end{description}
    \end{proof}
\end{lema}

\begin{prop}\label{prop:jordan}
    Si dos matrices son semejantes, entonces sus exponenciales son semejantes. Dicho de otra manera, si $A,B,P\in \mathbb{R}^{d\times d}$ de forma que:
    \begin{equation*}
        A = PBP^{-1}
    \end{equation*}

    Entonces:
    \begin{equation*}
        e^A = P e^B P^{-1}
    \end{equation*}
    \begin{proof}
        Si consideramos las sumas parciales de la serie que nos permite calcular $e^A$:
        \begin{align*}
            S_n^A &= I + A + \dfrac{1}{2}A^2 + \ldots + \dfrac{1}{n!}A^n \\
                &\AstIg  I + PBP^{-1} + \dfrac{1}{2} PB^2 P^{-1} + \ldots + \dfrac{1}{n!} PB^nP^{-1} \\
                &\stackrel{(\ast\ast)}{=} P\left( I + B + \dfrac{1}{2}B^2 + \ldots + \dfrac{1}{n!}B^n \right)P^{-1} = PS_n^BP^{-1}
        \end{align*}
        Donde en $(\ast)$ hemos usado el Lema~\ref{lema:potencias_semejantes} y en $(\ast\ast)$ la propiedad distributiva del producto de matrices\footnote{Teniendo en cuenta que el producto no es conmutativo.}. Si ahora tomamos límites cuando $n\rightarrow\infty$ y aplicamos que el producto de matrices es una operación continua, llegamos a la igualdad que queríamos probar.
    \end{proof}
\end{prop}

A partir de la Proposición~\ref{prop:jordan}, calcular $e^A$ se reduce a hallar la forma canónica de Jordan de la matriz $A$ junto con la matriz $P$ de cambio de base y por último saber calcular $e^J$.\\

\noindent
Nos interesaremos por tanto, en ver cómo calcular $e^J$, siendo $J$ una matriz en forma canónica de Jordan. Sin embargo, notemos que las matrices diagonales por bloques tienen una propiedad especial (demostrarla se deja como ejercicio), y es que:
\begin{equation*}
    J^n = \left(\begin{array}{cccc}
         J_1^n &  &  & \\
         & J_2^n &  & \\
         &  & \ddots &  \\
         &  &  & J_r^n 
    \end{array}\right) \qquad \forall n\in \mathbb{N}
\end{equation*}
Por tanto, al igual que pasaba con las matrices diagonales, tenemos que:
\begin{equation*}
    e^J = \left(\begin{array}{cccc}
         e^{J_1^n} &  &  & \\
         & e^{J_2^n} &  & \\
         &  & \ddots &  \\
         &  &  & e^{J_r^n }
    \end{array}\right) 
\end{equation*}
De esta forma, calcular $e^A$ para cualquier matriz $A$ se reduce a saber cómo calcular $e^H$, siendo $H$ una matriz de la forma:
\begin{equation*}
    H = \left(\begin{array}{ccccc}
            \lm & 1 & 0 & \cdots & 0 \\
            0 & \lm & 1 & \cdots & 0 \\
         \vdots & \vdots & \ddots & \ddots & \vdots \\
        0& 0 &   &\ddots & 1 \\
        0& 0 & 0 &\cdots & \lm 
    \end{array}\right) \qquad \lm \in \mathbb{R}
\end{equation*}
Y esta exponencial vamos a calcularla a partir del sistema de ecuaciones diferenciales lineal dado por $x' = Hx$, es decir, el sistema:
\begin{equation*}
    \left\{\begin{array}{ccccc}
            x_1' &=& \lm x_1 & + & x_2 \\
            x_2' &=& \lm x_2 &+& x_3 \\
            \vdots & & \vdots && \vdots \\
            x_{r-1}' &=& \lm x_{r-1} &+& x_r \\
            x_r' &=& \lm x_r &&
    \end{array}\right.
\end{equation*}
Para ello, vamos a aplicar el cambio de variable:
\begin{equation*}
    y_i(t) = e^{-\lm t}x_i(t) \qquad \forall i \in \{1,\ldots,r\}
\end{equation*}
Tenemos que:
\begin{align*}
    y_i' &= e^{-\lm t} x_i' - \lm e^{-\lm t} x_i = e^{-\lm t}(x_i' - \lm x_i) = e^{-\lm t} x_{i+1} = y_{i+1} \qquad \forall i \in \{1,\ldots,r-1\}\\
    y_r' &= e^{-\lm t} x_r' - \lm e^{-\lm t} x_r = e^{-\lm t}(x_r' - \lm x_r) = e^{-\lm t} \cdot 0 = 0
\end{align*}
Por lo que nos queda el sistema:
\begin{equation*}
    \left\{\begin{array}{ccc}
            y_1' &=& y_2 \\
            y_2' &=& y_3 \\
                 \vdots &&\vdots \\
            y_{r-1}' &=& y_r \\
            y_r' &=& 0
    \end{array}\right.
\end{equation*}
Que ya se puede resolver de forma fácil en escalera, con:
\begin{align*}
    y_r(t) &= c_r \\
    y_{r-1}(t) &= c_{r-1} + c_r t \\
    y_{r-2}(t) &= c_{r-2} + c_{r-1}t + c_r \dfrac{t^2}{2} \\
               & \vdots \\
    y_1(t) &= c_1 + c_2t + c_3 \dfrac{t^2}{2} + \ldots + c_r \dfrac{t^{r-1}}{(r-1)!} \\
           &c_1,c_2,\ldots,c_r \in \mathbb{R}
\end{align*}
Deshaciendo el cambio, tenemos:
\begin{align*}
    x_r(t) &= e^{\lm t} c_r \\
    x_{r-1}(t) &= e^{\lm t} \left(c_{r-1} + c_r t\right) \\
    x_{r-2}(t) &= e^{\lm t} \left(c_{r-2} + c_{r-1}t + c_r \dfrac{t^2}{2}\right) \\
              & \vdots \\
    x_1(t) &= e^{\lm t} \left(c_1 + c_2t + c_3 \dfrac{t^2}{2} + \ldots + c_r \dfrac{t^{r-1}}{(r-1)!}\right)
\end{align*}
Y para construir una matriz fundamental de $x' = Hx$, vamos dando valores a las constantes. Si les damos a las constantes los valores de las componentes de los vectores de la base canónica de $\mathbb{R}^d$, obtenemos la matriz:
\begin{equation*}
    \Phi(t) = e^{\lm t} \left(\begin{array}{ccccc}
            1 & t & \nicefrac{t^2}{2} & \cdots & \nicefrac{t^{r-1}}{(r-1)!} \\
            0 & 1 & t & \cdots & \nicefrac{t^{r-2}}{(r-2)!}\\
            0 & 0 & 1 & \dots &  \nicefrac{t^{r-3}}{(r-3)!}\\
            \vdots & \vdots & \vdots & \ddots & \vdots\\
            0 & 0 & 0 & \cdots & 1
    \end{array}\right)
\end{equation*}
Que es matriz fundamental, por tener determinante 1. De esta forma:
\begin{equation*}
    e^{tJ} = \Phi(t)\cdot \Phi(0)^{-1} = \Phi(t)\cdot I = e^{\lm t}\left(\begin{array}{ccccc}
            1 & t & \nicefrac{t^2}{2} & \cdots & \nicefrac{t^{r-1}}{(r-1)!} \\
            0 & 1 & t & \cdots & \nicefrac{t^{r-2}}{(r-2)!}\\
            0 & 0 & 1 & \dots &  \nicefrac{t^{r-3}}{(r-3)!}\\
            \vdots & \vdots & \vdots & \ddots & \vdots\\
            0 & 0 & 0 & \cdots & 1
    \end{array}\right)
\end{equation*}

\begin{ejemplo}
    Consideramos:
    \begin{equation*}
        A = \left(\begin{array}{cc}
                2 & 0 \\
                -1 & 2
        \end{array}\right)
    \end{equation*}
    Que solo tiene el valor propio $\lm_1=2$ (raíz doble), con espacio vectorial asociado a dicho valor de dimensión 1, generado por el vector $v_1=(0,1)$. En este caso, la forma canónica de Jordan podría ser:
    \begin{equation*}
        J = \left(\begin{array}{cc}
                2 & \\
                   & 2
        \end{array}\right) \qquad \text{ó} \qquad 
        J = \left(\begin{array}{cc}
                2 & 1\\
                   & 2
        \end{array}\right) 
    \end{equation*}
    Y la primera es imposible, ya que sería diagonalizable. Finalmente, calculamos una matriz $P$ tal que:
    \begin{equation*}
        A = P J P^{-1}
    \end{equation*}
    Que podemos calcular gracias al sistema:
    \begin{equation*}
        AP = PJ
    \end{equation*}
    imponiendo que $|P|\neq 0$ para que $P$ sea invertible. Una matriz $P$ admisible (hay muchas), es:
    \begin{equation*}
        P = \left(\begin{array}{cc}
                0 & 1 \\
                -1 & 0
        \end{array}\right)
    \end{equation*}
    Con lo que:
    \begin{equation*}
        e^A = Pe^J P^{-1}
    \end{equation*}
    Y usando la fórmula anteriormente obtenida para el caso $r=2$:
    \begin{equation*}
        e^J = e^2 \left(\begin{array}{cc}
                1 & 1 \\
                0 & 1
        \end{array}\right)
    \end{equation*}
    Con lo que llegamos a que (háganse las cuentas):
    \begin{equation*}
        e^A = P e^J P^{-1}
    \end{equation*}
\end{ejemplo}

% // TODO: Pasar esto a final del T4
% \section{Fórmula de variación de constantes}
% Falta por ver

% % Esto es de la clase del de prácticas

% \begin{ejemplo}
%     Si tenemos (hágase la cuenta para sacar la ecuación):
%     \begin{equation*}
%         p(\lm) = {(\lm-3)}^{3}
%     \end{equation*}
%     Tres soluciones linealmente inedependientes de la ecuación serían:
%     \begin{equation*}
%         e^{3t} \qquad te^{3t} \qquad t^2e^{3t}
%     \end{equation*}
%     Si ahora consideramos:
%     \begin{equation*}
%         p(\lm) = {(\lm-3)}^{3}{(\lm^2 + 1)}^{2}
%     \end{equation*}
%     Las soluciones serían (las que faltan):
%     \begin{equation*}
%         \cos t \qquad \sen t \qquad t\cos t \qquad t\sen t
%     \end{equation*}
% \end{ejemplo}

% % TODO: El caso de raices multiples es:
% \begin{equation*}
%     L(e^{\lm t}) = p(\lm) e^{\lm t}
% \end{equation*}
% La derivamos respecto a $\lm$ (pensando que es funcion de dos variables, $\lm$ y $t$) (como se justifica la derivabilidad respecto a $\lm$?):
% \begin{equation*}
%     L(te^{\lm t}) = p'(\lm) e^{\lm t} + p(\lm) t e^{\lm t}
% \end{equation*}
% Y si tenemos una raíz doble (es raíz de $p$ y de $p'$), tenemos que $te^{\lm t}$ es solución.

% Esto también vale para exponenciales complejas.
