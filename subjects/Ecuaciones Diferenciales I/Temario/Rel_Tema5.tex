\section{Sistemas Lineales}

\begin{ejercicio}\label{ej:5.1}
    Calcula la solución general del sistema lineal homogéneo $x' = Ax$ para las siguientes matrices
    \[
        A_1 =
        \begin{pmatrix}
            1 & 0 & 1 \\
            0 & 1 & 0 \\
            1 & 0 & 1
        \end{pmatrix},
        \quad
        A_2 =
        \begin{pmatrix}
            1 & -1 & 2 \\
            -1 & 1 & 0 \\
            -1 & 0 & 1
        \end{pmatrix}.
    \]

    \begin{enumerate}
        \item $A_1$.
        
        Calculamos el polinomio característico de $A_1$:
        \begin{align*}
            p_{A_1}(\lambda) &= \det(A_1 - \lambda I)
            = \begin{vmatrix}
                1 - \lambda & 0 & 1\\
                0 & 1 - \lambda & 0\\
                1 & 0 & 1 - \lambda
            \end{vmatrix}
            =\\&= (1 - \lambda)\cdot \begin{vmatrix}
                1 - \lambda & 1\\
                1 & 1 - \lambda
            \end{vmatrix}
            = (1 - \lambda)\left((1 - \lambda)^2 - 1\right)
            =\\&= (1 - \lambda)(1-\lambda - 1)(1 - \lambda + 1)
            = (1 - \lambda)(-\lambda)(2 - \lambda)
        \end{align*}

        Por tanto, tenemos que:
        \begin{equation*}
            \sigma(A_1) = \{1, 0, 2\}.
        \end{equation*}

        Calculamos los vectores propios asociados a cada valor propio:
        \begin{align*}
            V_1 &= \ker(A_1 - I) = \left\{
                \begin{pmatrix}
                    x\\
                    y\\
                    z
                \end{pmatrix}
                \in \bb{R}^3
                \mid
                \begin{pmatrix}
                    0 & 0 & 1\\
                    0 & 0 & 0\\
                    1 & 0 & 0
                \end{pmatrix}
                \begin{pmatrix}
                    x\\
                    y\\
                    z
                \end{pmatrix}
                =
                \begin{pmatrix}
                    0\\
                    0\\
                    0
                \end{pmatrix}
            \right\}
            = \cc{L}\left\{
                \begin{pmatrix}
                    0\\
                    1\\
                    0
                \end{pmatrix}
            \right\},\\
            V_0 &= \ker(A_1) = \left\{
                \begin{pmatrix}
                    x\\
                    y\\
                    z
                \end{pmatrix}
                \in \bb{R}^3
                \mid
                \begin{pmatrix}
                    1 & 0 & 1\\
                    0 & 1 & 0\\
                    1 & 0 & 1
                \end{pmatrix}
                \begin{pmatrix}
                    x\\
                    y\\
                    z
                \end{pmatrix}
                =
                \begin{pmatrix}
                    0\\
                    0\\
                    0
                \end{pmatrix}
            \right\}
            = \cc{L}\left\{
                \begin{pmatrix}
                    1\\
                    0\\
                    -1
                \end{pmatrix}
            \right\},\\
            V_2 &= \ker(A_1 - 2I) = \left\{
                \begin{pmatrix}
                    x\\
                    y\\
                    z
                \end{pmatrix}
                \in \bb{R}^3
                \mid
                \begin{pmatrix}
                    -1 & 0 & 1\\
                    0 & -1 & 0\\
                    1 & 0 & -1
                \end{pmatrix}
                \begin{pmatrix}
                    x\\
                    y\\
                    z
                \end{pmatrix}
                =
                \begin{pmatrix}
                    0\\
                    0\\
                    0
                \end{pmatrix}
            \right\}
            = \cc{L}\left\{
                \begin{pmatrix}
                    1\\
                    0\\
                    1
                \end{pmatrix}
            \right\}.
        \end{align*}

        Por tanto, tres soluciones del sistema son:
        \begin{align*}
            x_1(t) = e^t\begin{pmatrix}
                0\\
                1\\
                0
            \end{pmatrix},\qquad
            x_0(t) = \begin{pmatrix}
                1\\
                0\\
                -1
            \end{pmatrix},\qquad
            x_2(t) = e^{2t}\begin{pmatrix}
                1\\
                0\\
                1
            \end{pmatrix}.
        \end{align*}

        Además, estas son linealmente independientes, ya que:
        \begin{equation*}
            \det(x_1\mid x_0\mid x_2) = e^{1+0+2}\det\begin{pmatrix}
                0 & 1 & 1\\
                1 & 0 & 0\\
                0 & -1 & 1
            \end{pmatrix}\neq 0
        \end{equation*}
        donde el segundo determinante no es nulo porque vectores asociados a valores propios distintos son linealmente independientes. Por tanto, tenemos que la solución general del sistema es:
        \begin{equation*}
            x(t) = c_1e^t\begin{pmatrix}
                0\\
                1\\
                0
            \end{pmatrix} + c_2\begin{pmatrix}
                1\\
                0\\
                -1
            \end{pmatrix} + c_3e^{2t}\begin{pmatrix}
                1\\
                0\\
                1
            \end{pmatrix}\qquad t\in \bb{R}, c_1, c_2, c_3\in \bb{R}.
        \end{equation*}

        \item $A_2$.
        
        Calculamos el polinomio característico de $A_2$:
        \begin{align*}
            p_{A_2}(\lambda) &= \det(A_2 - \lambda I)
            = \begin{vmatrix}
                1 - \lambda & -1 & 2\\
                -1 & 1 - \lambda & 0\\
                -1 & 0 & 1 - \lambda
            \end{vmatrix}
            =\\&= (1 - \lambda)\cdot \begin{vmatrix}
                1 - \lambda & -1\\
                -1 & 1 - \lambda
            \end{vmatrix}
            + 2\cdot \begin{vmatrix}
                -1 & 1 - \lambda\\
                -1 & 0
            \end{vmatrix}
            =\\&= (1 - \lambda)\left((1 - \lambda)^2 - 1\right) + 2(0 + (1 - \lambda))
            =\\&= (1-\lm)(-\lm)(2-\lm) +2(1-\lm)
            = (1-\lm)\left(-\lm(2-\lm) +2\right)
            = (1-\lm)(\lm^2 -2\lm +2)
        \end{align*}

        Por tanto, tenemos que:
        \begin{equation*}
            \sigma(A_2) = \{1, 1+i, 1-i\}.
        \end{equation*}

        Calculamos los vectores propios asociados a cada valor propio:
        \begin{align*}
            V_1 &= \ker(A_2 - I) = \left\{
                \begin{pmatrix}
                    x\\
                    y\\
                    z
                \end{pmatrix}
                \in \bb{R}^3
                \mid
                \begin{pmatrix}
                    0 & -1 & 2\\
                    -1 & 0 & 0\\
                    -1 & 0 & 0
                \end{pmatrix}
                \begin{pmatrix}
                    x\\
                    y\\
                    z
                \end{pmatrix}
                =
                \begin{pmatrix}
                    0\\
                    0\\
                    0
                \end{pmatrix}
            \right\}
            = \cc{L}\left\{
                \begin{pmatrix}
                    0\\
                    2\\
                    1
                \end{pmatrix}
            \right\},\\
            V_{1+1} &= \ker(A_2 - (1+i)I) = \left\{
                \begin{pmatrix}
                    x\\
                    y\\
                    z
                \end{pmatrix}
                \in \bb{R}^3
                \mid
                \begin{pmatrix}
                    -i & -1 & 2\\
                    -1 & -i & 0\\
                    -1 & 0 & -i
                \end{pmatrix}
                \begin{pmatrix}
                    x\\
                    y\\
                    z
                \end{pmatrix}
                =
                \begin{pmatrix}
                    0\\
                    0\\
                    0
                \end{pmatrix}
            \right\}
            = \cc{L}\left\{
                \begin{pmatrix}
                    i\\
                    -1\\
                    -1
                \end{pmatrix}
            \right\}
        \end{align*}

        Por tanto, tenemos que dos soluciones son:
        \begin{align*}
            x_1(t) &= e^t\begin{pmatrix}
                0\\
                2\\
                1
            \end{pmatrix},\\
            x_{1+i}(t) &= e^{(1+i)t}\begin{pmatrix}
                i\\
                -1\\
                -1
            \end{pmatrix}
            = e^t\left(\cos t + i\sen t\right)\begin{pmatrix}
                i\\
                -1\\
                -1
            \end{pmatrix}
            = e^t\begin{pmatrix}
                -\sen t + i\cos t\\
                -\cos t - i\sen t\\
                -\cos t - i\sen t
            \end{pmatrix}.
        \end{align*}

        Por tanto, como $Re(x_{1+i}), Im(x_{1+i})$ son dos soluciones de $x' = Ax$, tenemos que tres soluciones reales son:
        \begin{equation*}
            x_1(t) = e^t\begin{pmatrix}
                0\\
                2\\
                1
            \end{pmatrix},\qquad
            x_2(t) = e^t\begin{pmatrix}
                -\sen t\\
                -\cos t\\
                -\cos t
            \end{pmatrix},\qquad
            x_3(t) = e^t\begin{pmatrix}
                \cos t\\
                -\sen t\\
                -\sen t
            \end{pmatrix}.
        \end{equation*}

        Veamos ahora si son linealmente independientes:
        \begin{equation*}
            \det(x_1\mid x_2\mid x_3) = e^{3t}\det\begin{pmatrix}
                0 & -\sen t & \cos t\\
                2 & -\cos t & -\sen t\\
                1 & -\cos t & -\sen t
            \end{pmatrix}\neq 0
        \end{equation*}

        Por tanto, la solución general del sistema es:
        \begin{equation*}
            x(t) = c_1x_1(t) + c_2x_2(t) + c_3x_3(t)\qquad t\in \bb{R}, c_1, c_2, c_3\in \bb{R}.
        \end{equation*}
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}\label{ej:5.2}
    Se considera el problema de valores iniciales para el sistema de ecuaciones de segundo orden
    \[
        \begin{cases}
            x'' = A(t)x + b(t),\\
            x(t_0) = x_0,\\
            x'(t_0) = v_0,
        \end{cases}
    \]
    con $A \in C(I, \bb{R}^{N\times N})$, $b \in C(I, \bb{R}^N)$, $I$ intervalo abierto, $t_0 \in I$, $x_0, v_0 \in \bb{R}^N$. Demuestra que la sucesión de iterantes
    \[
        x_{n+1}(t) = \int_{t_0}^t (t - s)[A(s)x_n(s) + b(s)]ds + x_0 + v_0(t - t_0)
    \]
    con inicialización $x_0(t) = x_0$ converge uniformemente en compactos de $I$ a una solución del problema. Demuestra que esta solución es única.\\

    Sea $J\subset I$ un intervalo compacto, y sea $\lm(J)$ su longitud (finito, por ser un intervalo cerrado). Como $A,b$ y la norma son continuas en $I$, existen $\alpha,\beta\in \bb{R}^+_0$ de forma que:
    \begin{equation*}
        \|A(t)\|\leq \alpha,\quad \|b(t)\|\leq \beta,\quad \forall t\in J.
    \end{equation*}

    Probaremos la convergencia uniforme mediante el Test de Weierstrass. Estudiemos la diferencia entre dos iterantes consecutivos:
    \begin{align*}
        \|x_1(t) - x_0(t)\| &= \left\|\int_{t_0}^t (t - s)[A(s)x_0(s) + b(s)]ds  + v_0(t - t_0)\right\|
        \leq\\ &\leq 
        \left\|\int_{t_0}^t (t - s)[A(s)x_0 + b(s)]ds\right\|  + \left\|v_0(t - t_0)\right\|
        \leq\\ &\leq 
        \left|\int_{t_0}^t \left\|(t - s)[A(s)x_0 + b(s)]\right\|ds\right|  + |t - t_0|\left\|v_0\right\|
        \leq\\ &\leq 
        \left|\int_{t_0}^t |t-s|[\left\|A(s)x_0\right\| + \left\|b(s)\right\|]ds\right|  + \lm(J)\left\|v_0\right\|
        \leq\\ &\leq 
        \left|\int_{t_0}^t \lm(J)[\left\|A(s)\right\|\left\|x_0\right\| + \left\|b(s)\right\|]ds\right|  + \lm(J)\left\|v_0\right\|
        \leq\\ &\leq 
        \left|\lm(J)(\alpha\|x_0\|+\beta)(t-t_0)\right|  + \lm(J)\left\|v_0\right\|
        \leq \lm^2(J)(\alpha\|x_0\|+\beta)+ \lm(J)\left\|v_0\right\|.
    \end{align*}

    Definimos la siguiente constante:
    \begin{equation*}
        M_0:= \lm^2(J)(\alpha\|x_0\|+\beta)+ \lm(J)\left\|v_0\right\|.
    \end{equation*}

    Veamos ahora para $i=1,2$:
    \begin{align*}
        \|x_2(t) - x_1(t)\| &= \left\|\int_{t_0}^t (t - s)A(s)\left(x_1(s) - x_0(s)\right)ds\right\|
        \leq \\&\leq
        \lm(J)\alpha M_0 |t-t_0|\\
        \|x_3(t) - x_2(t)\| &= \left\|\int_{t_0}^t (t - s)A(s)\left(x_2(s) - x_1(s)\right)ds\right\|
        \leq \\&\leq
        \lm^2(J)\alpha^2 M_0 \left|\int_{t_0}^t |s-t_0|ds\right|
        = \lm^2(J)\alpha^2 M_0 \frac{|t-t_0|^2}{2}
    \end{align*}

    Es fácil probar por inducción que:
    \begin{equation*}
        \|x_{n+1}(t) - x_{n}(t)\| \leq \lm^{n}(J)\alpha^{n}M_0\frac{|t-t_0|^{n}}{n!}\leq \lm^{n}(J)\alpha^{n}M_0\frac{\lm^{n}(J)}{n!}.
    \end{equation*}

    Definimos por tanto la sucesión de constantes:
    \begin{equation*}
        M_{n} = \alpha^{n}M_0\cdot \frac{\lm^{2n}(J)}{n!}
        = M_0\cdot \frac{(\alpha\lm^2(J))^n}{n!}
    \end{equation*}

    Por el desarrollo de Taylor de la exponencial, tenemos que:
    \begin{equation*}
        \sum_{n=0}^{\infty}M_n = M_0\sum_{n=0}^{\infty}\frac{(\alpha\lm^2(J))^n}{n!} = M_0e^{\alpha\lm^2(J)}<\infty.
    \end{equation*}

    Por tanto, la serie $\sum\limits_{n=0}^{\infty}M_n$ converge, y por tanto la sucesión de iterantes $x_n(t)$ converge uniformemente en $J$ a una función continua $x(t)$. Como $J$ era un intervalo compacto arbitrario, la sucesión converge uniformemente en compactos de $I$. Veamos ahora qué función es.\\
    
    Por inducción, es fácil probar que $x_n\in C(I, \bb{R}^N)$ para cada $n\in \bb{N}$, por lo que el integrando es continuo para todo $n\in \bb{N}$. Por tanto, podemos intercambiar el límite y la integral:
    \begin{align*}
        x(t) &= \lim_{n\to \infty}x_n(t)
        = \lim_{n\to \infty}\int_{t_0}^t (t - s)[A(s)x_{n-1}(s) + b(s)]ds + x_0 + v_0(t - t_0)
        =\\&=
        \int_{t_0}^t \lim_{n\to \infty}(t - s)[A(s)x_{n-1}(s) + b(s)]ds + x_0 + v_0(t - t_0)
        =\\&=
        \int_{t_0}^t (t - s)[A(s)x(s) + b(s)]ds + x_0 + v_0(t - t_0)
        =\\&= 
        t\cdot \int_{t_0}^t[A(s)x(s) + b(s)]ds -\int_{t_0}^t[A(s)x(s) + b(s)]ds + x_0 + v_0(t - t_0)
    \end{align*}

    Una vez obtenida la función límite $x$, veamos que es solución del problema de valores iniciales. Derivamos $x$ usando el Teorema Fundamental del Cálculo:
    \begin{align*}
        x'(t) &= \int_{t_0}^t[A(s)x(s) + b(s)]ds+v_0\\
        x''(t) &= A(t)x(t) + b(t).
    \end{align*}

    Por tanto, vemos que la ecuación la cumple. Veamos si cumple las condiciones iniciales:
    \begin{align*}
        x(t_0) &= \int_{t_0}^{t_0}[A(s)x(s) + b(s)]ds + x_0 + v_0(t_0 - t_0) = x_0,\\
        x'(t_0) &= \int_{t_0}^{t_0}[A(s)x(s) + b(s)]ds + v_0 = v_0.
    \end{align*}

    Por tanto, la función $x$ es solución del problema de valores iniciales con dominio $J$. Por la unicidad de la solución del problema de valores iniciales, tenemos que $x$ es la única solución del problema de valores iniciales en $J$.
\end{ejercicio}

\begin{ejercicio}\label{ej:5.3}
    Construye un sistema de ecuaciones lineal y homogéneo que tenga como soluciones linealmente independientes
    \[
        x_1 = (1, \sen t)^t, \quad x_2 = (\sen t, 1)^t.
    \]
    Justifica razonadamente si los coeficientes de tal sistema pueden estar o no definidos en toda la recta real.\\

    Sea la matriz $A(t)=(a_{ij}(t))_{i,j}\in \bb{R}^{2\times 2}$, de forma que:
    \begin{equation*}
        x' = A(t)x
    \end{equation*}

    Calculemos $x'$ para cada una de las soluciones dadas:
    \begin{align*}
        x_1' &= \begin{pmatrix}
            0\\
            \cos t
        \end{pmatrix}
        = A(t)x_1 = \begin{pmatrix}
            a_{11}(t) & a_{12}(t)\\
            a_{21}(t) & a_{22}(t)
        \end{pmatrix}
        \begin{pmatrix}
            1\\
            \sen t
        \end{pmatrix}
        = \begin{pmatrix}
            a_{11}(t) + a_{12}(t)\sen t\\
            a_{21}(t) + a_{22}(t)\sen t
        \end{pmatrix},\\
        x_2' &= \begin{pmatrix}
            \cos t\\
            0
        \end{pmatrix}
        = A(t)x_2 = \begin{pmatrix}
            a_{11}(t) & a_{12}(t)\\
            a_{21}(t) & a_{22}(t)
        \end{pmatrix}
        \begin{pmatrix}
            \sen t\\
            1
        \end{pmatrix}
        = \begin{pmatrix}
            a_{11}(t)\sen t + a_{12}(t)\\
            a_{21}(t)\sen t + a_{22}(t)
        \end{pmatrix}.
    \end{align*}

    Por tanto, tenemos el siguiente sistema de ecuaciones:
    \begin{align*}
        \begin{cases}
            a_{11}(t) + a_{12}(t)\sen t = 0,\\
            a_{11}(t)\sen t + a_{12}(t) = \cos t,\\
            a_{21}(t) + a_{22}(t)\sen t = \cos t,\\
            a_{21}(t)\sen t + a_{22}(t) = 0.
        \end{cases}
    \end{align*}

    Despuejando $a_{11}(t)$ y $a_{22}(t)$, tenemos que el sistema queda:
    \begin{align*}
        \begin{cases}
            a_{11}(t)= -a_{12}(t)\sen t,\\
            -a_{12}(t)\sen^2 t + a_{12}(t) = \cos t,\\
            a_{21}(t) - a_{21}(t)\sen^2 t = \cos t,\\
            a_{22}(t) = -a_{21}(t)\sen t.
        \end{cases}
        \Longrightarrow
        \begin{cases}
            a_{11}(t)= -a_{12}(t)\sen t,\\
            a_{12}(t)(1-\sen^2 t) = \cos t,\\
            a_{21}(t)(1-\sen^2 t) = \cos t,\\
            a_{22}(t) = -a_{21}(t)\sen t.
        \end{cases}
        \Longrightarrow
        \begin{cases}
            a_{11}(t)= -a_{12}(t)\sen t,\\
            a_{12}(t)\cos^2t = \cos t,\\
            a_{21}(t)\cos^2t = \cos t,\\
            a_{22}(t) = -a_{21}(t)\sen t.
        \end{cases}
    \end{align*}

    Por tanto, en los puntos de $t$ en los que el coseno se anula, el sistema es compatible indeterminado, ya que cualquier valor de $a_{12}(t)$ y $a_{21}(t)$ satisface el sistema. Por tanto, en los valores que no se anula el coseno, tenemos:
    \begin{equation*}
        \begin{cases}
            a_{11}(t)= -\tg t,\\
            a_{12}(t) = \frac{1}{\cos t},\\
            a_{21}(t) = \frac{1}{\cos t},\\
            a_{22}(t) = -\tg t.
        \end{cases}
        \Longrightarrow
        A(t) = \begin{pmatrix}
            -\tg t & \frac{1}{\cos t}\\
            \frac{1}{\cos t} & -\tg t
        \end{pmatrix}.
    \end{equation*}

    Como vemos, dichos coeficientes no están definidos en toda la recta real, ya que el coseno se anula en ciertos puntos. Por tanto, tan solo es posible definirlo en los intervalos de la forma:
    \begin{equation*}
        I_k = \left]\left(\frac{1}{2} + k\right)\pi, \left(\frac{3}{2} + k\right)\pi\right[,\quad k\in \bb{Z}.
    \end{equation*}
\end{ejercicio}

\begin{ejercicio}\label{ej:5.4}
    Consideremos el sistema
    \[
        \begin{cases}
            x' = Ax + b(t),
        \end{cases}
    \]
    donde $A$ es una matriz cuadrada de orden $N$ y $b(t) = e^{\mu t}v$, con $v \in \bb{R}^N$. Si $\mu$ no es valor propio de $A$, prueba que existe una solución particular de la forma $x(t) = e^{\mu t}w$. Usa esta idea y el principio de superposición para encontrar una solución particular del sistema
    \[
        \begin{cases}
            x_1' = 2x_1 - 3x_2 + 3e^{2t},\\
            x_2' = x_1 - 2x_2 - 8e^{-3t}.
        \end{cases}
    \]

    Para encontrar una solución particular de la forma $x(t) = e^{\mu t}w$, derivamos $x$:
    \begin{align*}
        x'(t)= \mu e^{\mu t}w
        = A e^{\mu t}w + e^{\mu t}v
        \Longrightarrow
        -\cancel{e^{\mu t}}v = (A - \mu I)\cancel{e^{\mu t}}w
        \Longrightarrow
        w = -(A - \mu I)^{-1}v.
    \end{align*}
    donde $\exists (A - \mu I)^{-1}$ porque $\mu$ no es valor propio de $A$, por o que $|A - \mu I|\neq 0$. Por tanto, una solución particular del sistema es:
    \begin{align*}
        x(t) = -e^{\mu t} (A - \mu I)^{-1}v.
    \end{align*}

    Consideramos ahora la matriz $A$ y el vector $b(t)$ siguientes:
    \begin{align*}
        A = \begin{pmatrix}
            2 & -3\\
            1 & -2
        \end{pmatrix},\quad
        b(t) = \begin{pmatrix}
            3e^{2t}\\
            -8e^{-3t}
        \end{pmatrix}.
    \end{align*}

    Calculamos el polinomio característico de $A$:
    \begin{align*}
        p_A(\lambda) &= \lm^2 - \operatorname{tr}(A)\lm + |A|
        = \lambda^2 -1
        \Longrightarrow
        \sigma(A) = \{1, -1\}.
    \end{align*}

    Como $b(t)$ no es de la forma $e^{\mu t}v$, no podemos aplicar el resultado anterior de forma directa. No obstante, consideramos:
    \begin{equation*}
        b_1(t) = e^{2t}\begin{pmatrix}
            3\\
            0
        \end{pmatrix},\quad
        b_2(t) = e^{-3t}\begin{pmatrix}
            0\\
            -8
        \end{pmatrix}.
    \end{equation*}

    Como $2, -3\notin \sigma(A)$, y usando el resultado que acabamos de demostrar, podemos encontrar, respectivamente, una solución particular del sistema $x'=Ax+b_1(t)$ y $x'=Ax+b_2(t)$. Sean estas, respectivamente, $x_1(t)$ y $x_2(t)$. Tenemos que:
    \begin{align*}
        x_1(t) &= -e^{2t}\begin{pmatrix}
            0 & -3\\
            1 & -4
        \end{pmatrix}^{-1}\begin{pmatrix}
            3\\
            0
        \end{pmatrix}
        = -e^{2t}\begin{pmatrix}
            -4 & 3\\
            -1 & 0
        \end{pmatrix}\begin{pmatrix}
            1\\
            0
        \end{pmatrix}
        = -e^{2t}\begin{pmatrix}
            -4\\
            -1
        \end{pmatrix}
        = e^{2t}\begin{pmatrix}
            4\\
            1
        \end{pmatrix},\\
        x_2(t) &= -e^{-3t}\begin{pmatrix}
            5 & -3\\
            1 & 1
        \end{pmatrix}^{-1}\begin{pmatrix}
            0\\
            -8
        \end{pmatrix}
        = -e^{-3t}\begin{pmatrix}
            1 & 3\\
            -1 &5\\
        \end{pmatrix}\begin{pmatrix}
            0\\
            -1
        \end{pmatrix}
        = -e^{-3t}\begin{pmatrix}
            -3\\
            -5
        \end{pmatrix}
        = e^{-3t}\begin{pmatrix}
            3\\
            5
        \end{pmatrix}.
    \end{align*}

    Por tanto, la solución particular del sistema $x'=Ax+b(t)$, usando el principio de superposición, es:
    \begin{align*}
        x(t) = x_1(t) + x_2(t) = e^{2t}\begin{pmatrix}
            4\\
            1
        \end{pmatrix} + e^{-3t}\begin{pmatrix}
            3\\
            5
        \end{pmatrix}.
    \end{align*}
\end{ejercicio}

\begin{ejercicio}\label{ej:5.5}
    Se considera el sistema
    \[
        \begin{pmatrix}
            x'\\
            y'
        \end{pmatrix}
        =
        \begin{pmatrix}
            a(t) & b(t)\\
            -b(t) & a(t)
        \end{pmatrix}
        \begin{pmatrix}
            x\\
            y
        \end{pmatrix},
    \]
    donde $a, b : I \to \bb{R}$ son continuas. Demuestra que este sistema se puede reformular como una ecuación escalar compleja del tipo $z' = \alpha(t)z$ donde la incógnita $z = z(t)$ puede tomar valores complejos y la función $\alpha : I \to \bb{C}$ se determina a partir de $a(t)$ y $b(t)$. Utiliza este hecho para resolver el sistema original.
\end{ejercicio}

\begin{ejercicio}\label{ej:5.6}
    Se definen las funciones:
    \[
        x(t) = \int_0^\infty \frac{e^{-\sigma}}{\sqrt{\sigma}}\sen(t\sigma)~d\sigma, \quad y(t) = \int_0^\infty \frac{e^{-\sigma}}{\sqrt{\sigma}}\cos(t\sigma)~d\sigma,
    \]
    Demuestra que $(x, y)$ es solución de un sistema lineal homogéneo. Resuelve el sistema con las condiciones iniciales adecuadas para calcular las integrales.
    \begin{observacion}
        Tenga en cuenta que
        $\displaystyle \int_0^\infty e^{-t^2}~dt = \frac{\sqrt{\pi}}{2}$.
    \end{observacion}

\begin{comment}
    El integrando de ambas funciones es una función de clase $C^1$ (puesto que el denominador no se anula). Por la derivación de las integrales dependientes de un parámetro, tenemos que:
    \begin{align*}
        x'(t) &= \int_0^\infty \frac{\partial}{\partial t}\left(\frac{e^{-\sigma}}{\sqrt{\sigma}}\sen(t\sigma)\right)~d\sigma
        = \int_0^\infty \frac{-\sigma e^{-\sigma}}{\sqrt{\sigma}}\cos(t\sigma)~d\sigma
        = -\int_0^\infty \sqrt{\sigma}e^{-\sigma}\cos(t\sigma)~d\sigma
    \end{align*}
\end{comment}
\end{ejercicio}

\begin{ejercicio}\label{ej:5.7}
    Dada una matriz fundamental $\Phi(t)$ del sistema $x' = A(t)x$, con $A : I \to \bb{R}^{N\times N}$ continua y una función $\Psi : I \to \bb{R}^{N\times N}$ de clase $C^1$, demuestra que:
    \[
        \Psi(t) \text{ es matriz fundamental} \Longleftrightarrow \exists C \in \bb{R}^{N\times N} \text{ constante, con } |C|\neq 0 \text{, tal que } \Psi(t) = \Phi(t)C.
    \]

    Demostramos mediante doble implicación:
    \begin{description}
        \item[$\Longrightarrow$] Supongamos que $\Psi(t)$ es matriz fundamental. Entononces, sean:
        \begin{align*}
            \Psi=(\psi_1, \ldots, \psi_N),\quad \Phi=(\phi_1, \ldots, \phi_N).
        \end{align*}

        Entonces, para cada $i\in \{1, \ldots, N\}$, como $\Phi$ es matriz fundamental, podemos expresar $\psi_i$ como combinación lineal de las columnas de $\Phi$:
        \begin{equation*}
            \psi_i(t) = \sum_{j=1}^N c_{ij}\phi_j(t),
        \end{equation*}
        donde $c_{ij}$ son constantes. Por tanto, definiendo $C=(c_{ij})_{i,j}$, tenemos que $\Psi(t)=\Phi(t)C$. Además, como $\Psi$ es matriz fundamental, tenemos que $|\Psi(t)|\neq 0$, por lo que $|C|\neq 0$.

        Notemos que dicha $C$ es la matriz de cambio de base entre ambas bases (la de $\Phi$ y la de $\Psi$).

        \item[$\Longleftarrow$] Supongamos que existe $C\in \bb{R}^{N\times N}$ constante, con $|C|\neq 0$, tal que $\Psi(t)=\Phi(t)C$. Veamos que $\Psi(t)$ es matriz solución. Como $\Phi$ es matriz solución, tenemos que:
        \begin{equation*}
            \Phi' = A\Phi.
        \end{equation*}

        Por tanto, usando la derivada del producto de matrices, tenemos que:
        \begin{align*}
            \Psi' &= (\Phi C)' = \Phi'C + \Phi C' = \Phi'C = A\Phi C = A\Psi.
        \end{align*}

        Por tanto, es matriz solución. Además, tenemos que:
        \begin{equation*}
            |\Psi| = |\Phi C| = |\Phi|\cdot |C|\neq 0,
        \end{equation*}
        Por tanto, $\Psi$ es matriz fundamental.
    \end{description}
\end{ejercicio}

\begin{ejercicio}\label{ej:5.8}
    Dos tanques del mismo volumen $V$ contienen inicialmente agua salada con concentración $C_1$, $C_2$ respectivamente
    Se produce un transvase de agua entre los dos tanques a una velocidad fija de $\unitfrac[k]{l}{min}$ y en ambas direcciones, de manera que el volumen $V$ de cada tanque se mantiene constante. Plantea y resuelve el sistema de ecuaciones diferenciales que rige la cantidad de sal $Q_1$, $Q_2$ en cada tanque. Explica el comportamiento a largo plazo.
\end{ejercicio}

\begin{ejercicio}\label{ej:5.9}
    Dada $A \in C(\bb{R}, \bb{R}^{N\times N})$, demuestra que si la matriz $A(t)$ conmuta con $B(t) = \displaystyle \int_0^t A(s)~ds$, entonces $\Phi(t) = e^{B(t)}$ es matriz fundamental del sistema $x' = A(t)x$.
\end{ejercicio}

\begin{ejercicio}\label{ej:5.10}
    Por el método de variación de constantes, encuentra la solución general del sistema completo $x' = Ax + b(t)$ en los siguientes casos
    \begin{enumerate}
        \item $A\in \bb{R}^{2\times 2}$, $b\in \bb{R}^{2\times 1}$, donde:
        \[
            A =
            \begin{pmatrix}
                0 & 1\\
                -2 & 3
            \end{pmatrix},
            \quad
            b(t) =
            \begin{pmatrix}
                e^t \\ 2
            \end{pmatrix}.
        \]
        \item $A\in \bb{R}^{2\times 2}$, $b\in \bb{R}^{2\times 1}$, donde:
        \[
            A =
            \begin{pmatrix}
                2 & 1\\
                -4 & 2
            \end{pmatrix},
            \quad
            b(t) =
            \begin{pmatrix}
                te^{2t} \\ -e^{2t}
            \end{pmatrix}.
        \]
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}\label{ej:5.11}
    En este ejercicio probaremos un resultado sobre independencia lineal para funciones que son productos de polinomios y exponenciales. Lo haremos en tres pasos:
    \begin{enumerate}
        \item Demuestra que si $p(t)$ es un polinomio no nulo, $\alpha\in \bb{R}^*$ y $m = 1, 2, \ldots$, entonces
        \[
            \frac{d^m}{dt^m}\left[p(t)e^{\alpha t}\right] = q(t)e^{\alpha t},
        \]
        donde $q(t)$ es otro polinomio no nulo.
        \item Se supone que $p_1, \ldots, p_r$ son polinomios y $\alpha_1, \ldots, \alpha_r$ números distintos entre sí ($\alpha_i \neq \alpha_j$ si $i \neq j$). Entonces si la identidad
        \[
            p_1(t)e^{\alpha_1t} + \ldots + p_r(t)e^{\alpha_rt} = 0
        \]
        es válida en algún intervalo $I$ se cumplirá
        \[
            p_1 \equiv p_2 \equiv \ldots \equiv p_r \equiv 0.
        \]
        \item Dados números naturales $n_1, \ldots, n_r$ las funciones
        \[
            e^{\alpha_1t}, te^{\alpha_1t}, \ldots, t^{n_1}e^{\alpha_1t}, \ldots, e^{\alpha_rt}, te^{\alpha_rt}, \ldots, t^{n_r}e^{\alpha_rt}
        \]
        son linealmente independientes en $I$.
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}\label{ej:5.12}
    Se considera el operador diferencial
    \[
        L[y] = y^{(k)} + a_{k-1}y^{(k-1)} + \ldots + a_1y' + a_0y
    \]
    donde $a_0, a_1, \ldots, a_{k-1}$ son números reales.
    \begin{enumerate}
        \item Demuestra, para cada $m \geq 0$, la identidad
        \[
            L\left[t^me^{\lambda t}\right] = \sum_{h = 0}^m \binom{m}{h}t^{m-h}p^{(h)}(\lambda)e^{\lambda t}
        \]
        donde $p(\lambda) = \lambda^k + a_{k-1}\lambda^{k-1} + \ldots + a_1\lambda + a_0$.
        \item Utiliza esta identidad y el ejercicio anterior para obtener un sistema fundamental de la ecuación $L[y] = 0$. Se distinguirá el caso de raíces complejas.
        \item Resuelve la ecuación
        \[
            y^{(5)} - y^{(4)} + 2y''' - 2y'' + y' - y = 0.
        \]
        \item Se pasa la ecuación del apartado anterior a un sistema $x' = Ax$, con $x \in \bb{R}^5$, por el cambio $x_1 = y$, $x_2 = y'$, $x_3 = y''$, $x_4 = y'''$, $x_5 = y^{(4)}$. Diseña dos posibles estrategias para calcular $e^{At}$. ¿Cuál sería más conveniente?
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}\label{ej:5.13}
    ¿Es cierta la identidad $e^Ae^B = e^{A+B}$ para matrices arbitrarias $A, B \in \bb{R}^{N\times N}$?
\end{ejercicio}

\begin{ejercicio}\label{ej:5.14}
    Calcula $e^A$ para las matrices
    \[
        A =
        \begin{pmatrix}
            a & b\\
            -b & a
        \end{pmatrix},
        \quad
        A =
        \begin{pmatrix}
            0 & b_1 & 0 & \ldots & 0\\
            0 & 0 & b_2 & \ldots & 0\\
            \vdots & \vdots & \vdots & \ddots & \vdots\\
            0 & 0 & 0 & \ldots & b_{N-1}\\
            0 & 0 & 0 & \ldots & 0
        \end{pmatrix}.
    \]
\end{ejercicio}

\begin{ejercicio}\label{ej:5.15}
    Dada $A \in \bb{R}^{N\times N}$ se considera el sistema $x' = Ax$
    \begin{enumerate}
        \item Prueba que $e^{(t-t_0)A}$ es matriz fundamental principal en $t = t_0$.
        \item $(e^{tA})^{-1} = e^{-tA}$.
        \item Se considera ahora el problema de valores iniciales $x' = Ax+b(t)$, $x(t_0) = x_0$ donde $b : I \to \bb{R}^N$ es una función continua. Prueba que la solución está dada por la fórmula
        \[
            x(t) = e^{tA}x_0 + \int_{t_0}^t e^{(t-s)A}b(s)~ds.
        \]
    \end{enumerate}
\end{ejercicio}

\begin{ejercicio}\label{ej:5.16}
    Dada una matriz $A \in \bb{R}^{N\times N}$ define $\sen(A)$ y $\cos(A)$. Calcula: $$\cos\left(\begin{pmatrix} t & 1\\ 0 & t \end{pmatrix}\right)$$
\end{ejercicio}



\begin{comment}
1 En este ejercicio probaremos un resultado sobre independencia lineal para funciones que son productos de polinomios
y exponenciales. Lo haremos en tres pasos:
a) Demuestra que si p(t) es un polinomio no nulo, α 6= 0 es un n´umero y m = 1, 2, . . . , entonces
d
m
dtm

p(t)e
αt
= q(t)e
αt
,
donde q(t) es otro polinomio no nulo.
b) Se supone que p1, . . . , pr son polinomios y α1, . . . , αr n´umeros distintos entre s´ı(αi 6= αj si i 6= j). Entonces si
la identidad
p1(t)e
α1t + · · · + pr(t)e
αrt = 0
es v´alida en alg´un intervalo I se cumplir´a
p1 ≡ p2 ≡ · · · ≡ pr ≡ 0.
c) Dados n´umeros naturales n1, . . . , nr las funciones
e
α1t
, teα1t
, . . . , tn1 e
α1t
, . . . , eαrt
, teαrt
, . . . , tnr e
αrt
son linealmente independientes en I.
2 Se considera el operador diferencial
L[y] = y
(k) + ak−1y
(k−1) + · · · + a1y
0 + a0y
donde a0, a1, . . . , ak−1 son n´umeros reales.
a) Demuestra, para cada m ≥ 0, la identidad
L

t
me
λt
=
"Xm
h=0

m
h

t
m−h
p
(h)
(λ)
#
e
λt
donde p(λ) = λ
k + ak−1λ
k−1 + · · · + a1λ + a0.
b) Utiliza esta identidad y el ejercicio anterior para obtener un sistema fundamental de la ecuaci´on L[y] = 0. Se
distinguir´a el caso de ra´ıces complejas.
c) Resuelve la ecuaci´on
y
(5) − y
(4) + 2y
000 − 2y
00 + y
0 − y = 0.
d) Se pasa la ecuaci´on del apartado anterior a un sistema x
0 = Ax, con x ∈ R
5
, por el cambio x1 = y, x2 = y
0
, x3 =
y
00, x4 = y
000, x5 = y
(4). Dise˜na dos posibles estrategias para calcular e
At. ¿Cu´al ser´ıa m´as conveniente?.
3 ¿Es cierta la identidad e
Ae
B = e
A+B para matrices arbitrarias A, B ∈ R
N×N ?
4 Calcula e
A para las matrices A =

a b
−b a 
y A =


0 b1 0 · · · 0
0 0 b2 · · · 0
· · · · · · ·
0 0 0 · · · bN−1
0 0 0 · · · 0


.
5 Dada A ∈ R
N×N se considera el sistema x
0 = Ax
a) Prueba que e
(t−t0)A es matriz fundamental principal en t = t0.
b) (e
tA)
−1 = e
−tA.
c) Se considera ahora el problema de valores iniciales x
0 = Ax+b(t), x(t0) = x0 donde b : I → R
N es una funci´on
continua. Prueba que la soluci´on est´a dada por la f´ormula
x(t) = e
tAx0 +
Z t
t0
e
(t−s)Ab(s)ds.
6 Dada una matriz A ∈ R
N×N define sen(A) y cos(A). Calcula cos 
t 1
0 t

\end{comment}