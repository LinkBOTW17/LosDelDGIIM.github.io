\chapter{Algunos Modelos Multivariantes}

En el presente Capítulo, y al igual que hemos hecho en el caso unidimensional, vamos a estudiar ahora ciertos modelos multivariantes que nos permitirán trabajar con más de una variable aleatoria.
En concreto, vamos a generalizar la distribución binomial y la normal, ya estudiadas previamente.

\section{Distribución Multinomial}

Como ya hemos adeñantado, esta distribución es una generalización de la distribución binomial.
Fijamos $k\in \bb{N}$, y consideramos un experimento aleatorio con $k+1$ posibles resultados asociados a otros tantos sucesos $A_1, A_2, \ldots, A_{k+1}$ exhaustivos y mutuamente excluyentes, por lo tanto constituyen una partición del espacio muestral, con probabilidad de ocurrencia $p_i \in \left]0,1\right[$, $i=1, \ldots, k+1$. Es decir:
\begin{align*}
    \Omega&=\bigcup_{i=1}^{k+1}A_i,\\
    A_i\cap A_j&=\emptyset, \quad i,j=1, \ldots, k+1,\qquad i\neq j,\\
    P(A_i)&=p_i, \quad i=1, \ldots, k+1
\end{align*}

Tenemos entonces que:
\begin{align*}
    P(A_{k+1})&=1-P\left(\bigcup_{i=1}^k A_i\right)=1-\sum_{i=1}^k P(A_i)=1-\sum_{i=1}^k p_i
\end{align*}

Supongamos que realizamos $n$ repeticiones independientes del experimento en las mismas condiciones, por tanto las probabilidades $p_i$, $i=1, 2, \ldots, k+1$ se mantienen constantes a lo largo de las repeticiones, y sean $X_1, \ldots, X_{k+1}$ variables aleatorias tales que $X_i$ contabiliza el número de veces que ocurre el suceso $A_i$ en las $n$ repeticiones del experimento, $i=1, 2, \ldots, k+1$. Tenemos que:
\begin{equation*}
    X_1+X_2+\ldots+X_{k+1}=n
\end{equation*}

En estas condiciones, podemos definir la distribución multinomial.
\begin{definicion}[Distribución Multinomial]
    En las condiciones anteriores, se define la distribución multinomial $k-$dimensional con parámetros $n$ y $p_1, \ldots, p_k$ como la distribución del vector aleatorio $X=(X_1, \ldots, X_k)$ en el cual cada componente $X_i$ contabiliza el número de ocurrencias del suceso $A_i$, $i=1, \ldots, k$. La notaremos por:
    \begin{align*}
        X\sim M_k(n, p_1, \ldots, p_k)
    \end{align*}
\end{definicion}
\begin{observacion}
    Notemos que la distribución binomial es un caso particular de la distribución multinomial, en concreto, la distribución multinomial con $k=1$.
\end{observacion}

Razonemos ahora la función de masa de probabilidad de la distribución multinomial. Consideramos el vector $x=(x_1, \ldots, x_k)$ tal que $x_i\in \bb{N}_0$, $i=1, \ldots, k$. Como se realizan $n$ repeticiones del experimento, tenemos que:
\begin{equation*}
    \sum_{i=1}^k x_i\leq n
\end{equation*}
donde la igualdad solo se dará si no se produce el suceso $A_{k+1}$ en ninguna de las repeticiones.
Una de las posibles ordenaciones para que $x$ tome dicho valor es que primero ocurran $x_1$ veces el suceso $A_1$, luego $x_2$ veces el suceso $A_2$, y así sucesivamente, hasta que ocurran $x_k$ veces el suceso $A_k$ y finalmente ocurran $n-\sum_{i=1}^k x_i$ veces el suceso $A_{k+1}$. Es decir, el suceso sería (notando $A_i^{x_i}$ al suceso $A_i$ ocurrido $x_i$ veces):
\begin{align*}
    A_1^{x_1}\cap A_2^{x_2}\cap \ldots \cap A_k^{x_k}\cap A_{k+1}^{n-\sum\limits_{i=1}^k x_i}
\end{align*}

La probabilidad de que ocurra este suceso, usando la independencia de las repeticiones del experimento, es:
\begin{align*}
    P\left[\bigcap_{i=1}^{k}\left(\bigcap_{j=1}^{x_i}A_i\right)\cap \left(\bigcap_{j=1}^{n-\sum\limits_{i=1}^k x_i}A_{k+1}\right)\right]=\prod_{i=1}^k p_i^{x_i}\cdot p_{k+1}^{n-\sum\limits_{i=1}^k x_i}=\prod_{i=1}^{k} p_i^{x_i} \cdot \left(1-\sum_{i=1}^k p_i\right)^{n-\sum\limits_{i=1}^k x_i}
\end{align*}


Veamos ahora cuántas ordenaciones distintas nos pueden dar dicho vector $x$. Estas ordenaciones son permutaciones con repetición de $n$ elementos clasificados en $k+1$ grupos, habiendo $x_i$ del tipo $i-$ésimo, $i=1, \ldots, k$ y $n-\sum_{i=1}^k x_i$ del grupo $k+1-$ésimo. Por tanto, el número de ordenaciones posibles es\footnote{Ver sección de Combinatoria en los apuntes de EDIP.}:
\begin{equation*}
    PR_{n}^{x_1, \ldots, x_k, n-\sum_{i=1}^k x_i}=\dfrac{n!}{\left(\prod\limits_{i=1}^k x_i!\right) \cdot \left(n-\sum\limits_{i=1}^k x_i\right)!}
\end{equation*}

Por tanto, tenemos que:
\begin{equation*}
    P[X=(x_1,\dots,x_k)]=\dfrac{n!}{\left(\prod\limits_{i=1}^k x_i!\right)\cdot \left(n-\sum\limits_{i=1}^k x_i\right)!}\cdot \prod_{i=1}^{k} p_i^{x_i} \cdot \left(1-\sum_{i=1}^k p_i\right)^{n-\sum\limits_{i=1}^k x_i}
\end{equation*}

Comprobemos que, efectivamente, esta función de masa de probabilidad es tal.
Para ello, es necesario introducir el siguiente lema técnico, descubierto por Leibniz.
\begin{lema}[Fórmula de Leibniz]
    Sean $p_1, \ldots, p_j$ números reales y $n\in \bb{N}$. Entonces, se cumple que:
    \begin{equation*}
        \left(\sum_{i=1}^j p_i\right)^n=\sum_{\sum\limits_{i=1}^j x_i=n}\dfrac{n!}{\prod\limits_{i=1}^j x_i!}\cdot \prod_{i=1}^j p_i^{x_i}
    \end{equation*}
\end{lema}
\begin{prop}
    Sea $X\sim M_k(n, p_1, \ldots, p_k)$. Sean $x_1, \ldots, x_k$ tales que $x_i\in \{0,\dots,n\}$, $i=1, \ldots, k$ y $\sum\limits_{i=1}^k x_i\leq n$. Entonces, la función de masa de probabilidad de $X$ es:
    \begin{equation*}
        P[X=(x_1,\dots,x_k)]=\dfrac{n!}{\left(\prod\limits_{i=1}^k x_i!\right)\cdot \left(n-\sum\limits_{i=1}^k x_i\right)!}\cdot \prod_{i=1}^{k} p_i^{x_i} \cdot \left(1-\sum\limits_{i=1}^k p_i\right)^{n-\sum\limits_{i=1}^k x_i}
    \end{equation*}
\end{prop}
\begin{proof}
    Usaremos para ello la Fórmula de Leibniz identificando $k+1=j$ y definiendo $p_{k+1}=1-\sum\limits_{i=1}^k p_i$ y $x_{k+1}=n-\sum\limits_{i=1}^k x_i$:
    \begin{align*}
        \sum_{\sum\limits_{i=1}^k x_i\leq n} &\dfrac{n!}{\left(\prod\limits_{i=1}^k x_i!\right)\cdot \left(n-\sum\limits_{i=1}^k x_i\right)!}\cdot \prod_{i=1}^{k} p_i^{x_i} \cdot \left(1-\sum\limits_{i=1}^k p_i\right)^{n-\sum\limits_{i=1}^k x_i}
        =\\&=
        \sum_{\sum\limits_{i=1}^{k+1} x_i=n}\dfrac{n!}{\prod\limits_{i=1}^{k+1} x_i!}\cdot \prod_{i=1}^{k+1} p_i^{x_i}
        = \left(\sum_{i=1}^{k+1} p_i\right)^n = \left(\sum_{i=1}^{k} p_i+1-\sum_{i=1}^{k} p_i\right)^n=1^n=1
    \end{align*}

    Además, como todos los términos de la suma son no negativos, tenemos efectivamente que se trata de una función de masa de probabilidad.
\end{proof}

Calculemos ahora la función generatriz de momentos de la distribución multinomial.
\begin{prop}
    Sea $X\sim M_k(n, p_1, \ldots, p_k)$. Entonces, la función generatriz de momentos de $X$ es:
    \begin{equation*}
        M_X(t_1, \ldots, t_k)=\left[\left(\sum_{i=1}^k p_i e^{t_i}\right) + \left(1-\sum_{i=1}^k p_i\right)\right]^n \qquad \forall t_1, \ldots, t_k\in \bb{R}
    \end{equation*}
\end{prop}
\begin{proof}
    Usaremos la definición de función generatriz de momentos:
    \begin{align*}
        M_X(t_1&, \ldots, t_k)=E[e^{t_1 X_1+\ldots+t_k X_k}]
        =\\&=
        \sum_{\sum\limits_{i=1}^k x_i\leq n} e^{t_1 x_1+\ldots+t_k x_k}\cdot \dfrac{n!}{\left(\prod\limits_{i=1}^k x_i!\right)\cdot \left(n-\sum\limits_{i=1}^k x_i\right)!}\cdot \prod_{i=1}^{k} p_i^{x_i} \cdot \left(1-\sum\limits_{i=1}^k p_i\right)^{n-\sum\limits_{i=1}^k x_i}
        =\\&=
        \sum_{\sum\limits_{i=1}^k x_i\leq n} \prod_{i=1}^k (e^{t_i})^{x_i}\cdot \dfrac{n!}{\left(\prod\limits_{i=1}^k x_i!\right)\cdot \left(n-\sum\limits_{i=1}^k x_i\right)!}\cdot \prod_{i=1}^{k} p_i^{x_i} \cdot \left(1-\sum\limits_{i=1}^k p_i\right)^{n-\sum\limits_{i=1}^k x_i}
        =\\&=
        \sum_{\sum\limits_{i=1}^k x_i\leq n} \dfrac{n!}{\left(\prod\limits_{i=1}^k x_i!\right)\cdot \left(n-\sum\limits_{i=1}^k x_i\right)!}\cdot \prod_{i=1}^{k} (p_ie^{t_i})^{x_i} \cdot \left(1-\sum\limits_{i=1}^k p_i\right)^{n-\sum\limits_{i=1}^k x_i}
        \AstIg\\&\AstIg
        \left[\left(\sum_{i=1}^k p_i e^{t_i}\right)+\left(1-\sum_{i=1}^k p_i\right)\right]^n
    \end{align*}
    donde en $(\ast)$ hemos usado la Fórmula de Leibniz.
\end{proof}

Calculamos ahora las marginales de la distribución multinomial.
\begin{prop}
    Dado el vector aleatorio $X\sim M_k(n, p_1, \ldots, p_k)$, consideramos un subvector de $X$ $(X_{i_1}, \ldots, X_{i_l})$, con $i_1, \ldots, i_l\in \{1, \ldots, k\}$, siendo $i_p\neq i_q$ si $p\neq q$. Entonces,
    $$(X_{i_1}, \ldots, X_{i_l})\sim M_l(n, p_{i_1}, \ldots, p_{i_l}).$$
\end{prop}
\begin{observacion}
    Notemos que esta notación no es del todo precisa, pero se emplea para no complicar la notación. La distribución marginal de un subvector aleatorio de $l$ componentes sigue una distribución multinomial con los parámetros $n$ y las probabilidades asociadas a las componentes del subvector.
\end{observacion}
\begin{proof}
    Esto se deduce directamente de la función generatiz de momentos de un subvector de $X$, que es:
    \begin{align*}
        M_{(X_{i_1}, \ldots, X_{i_l})}(t_{i_1}, \ldots, t_{i_l})&=M_X(0,\dots,0,t_{i_1},0,\dots,0,t_{i_l},0,\dots,0)
        =\\&=
        \left[\left(\sum_{i=1}^k p_i e^{t_i}\right)+\left(1-\sum_{i=1}^k p_i\right)\right]^n
        =\\&=
        \left[\left(\sum_{j=1}^l p_{i_j} e^{t_{i_j}}\right)+\left(1-\sum_{j=1}^l p_{i_j}\right)\right]^n
    \end{align*}
    Como esta función generatriz de momentos coincide con la de una distribución multinomial con los parámetros $n$ y las probabilidades asociadas a las componentes del subvector, se tiene que la distribución marginal de un subvector de $l$ componentes de $X$ sigue una distribución multinomial con los parámetros $n$ y las probabilidades asociadas a las componentes del subvector.
\end{proof}

Tenemos que, en particular, para $j\in \{1,\dots,k\}$, tenemos que:
\begin{equation*}
    X_j\sim M_1(n, p_j)
\end{equation*}
Su función generatriz de momentos entonces es:
\begin{equation*}
    M_{X_j}(t)=\left[p_j e^t+(1-p_j)\right]^n
\end{equation*}
Por la unicidad de la función generatriz de momentos, tenemos que $X_j\sim B(n, p_j)$, como introducimos ya anteriormente.\\

Calculamos ahora las distribuciones condicionadas de la distribución multinomial.
\begin{prop}
    Consideramos el vector aleatorio $X\sim M_k(n, p_1, \ldots, p_k)$. Sean los conjuntos \emph{disjuntos} $\{i_1, \ldots, i_q\}, \{j_1, \ldots, j_p\}\subset \{1, \ldots, k\}$, con $i_p\neq i_q$ y $j_p\neq j_q$ si $p\neq q$. Entonces:
    \begin{equation*}
        (X_{i_1}, \ldots, X_{i_q})\mid (X_{j_1}=x_{j_1}, \ldots, X_{j_p}=x_{j_p})\sim M_q\left(n-\sum_{l=1}^p x_{j_l}, \dfrac{p_{i_1}}{1-\sum\limits_{l=1}^p p_{j_l}}, \ldots, \dfrac{p_{i_q}}{1-\sum\limits_{l=1}^p p_{j_l}}\right)
    \end{equation*}
\end{prop}
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T09_CondicionadasMultinomial.pdf#[0,{%22name%22:%22Fit%22}]
\begin{observacion}
    De nuevo, la notación no es del todo precisa, pero se emplea para no complicar la notación.

    La distribución condicionada de un subvector de $q$ componentes de $X$ dado que otro subvector de $p$ componentes de $X$ toma unos valores concretos, sigue una distribución multinomial con los parámetros $n$ menos la suma de los valores que toman las componentes del subvector condicionante, y las probabilidades asociadas a las componentes del subvector condicionado divididas por la probabilidad $1$ menos la suma de las probabilidades asociadas a las componentes del subvector condicionante.
\end{observacion}

En el caso de que condicionemos una componente $X_i$ de $X$ a otra componente $X_j$ de $X$ ($i\neq j$), tenemos que:
\begin{equation*}
    X_i \mid X_j=x_j\sim M_1 \left(n-x_j, \dfrac{p_i}{1-p_j}\right) = B\left(n-x_j, \dfrac{p_i}{1-p_j}\right)
\end{equation*}

Veamos ahora que se trata de una distribución reproductiva.
\begin{prop}[Distribución Reproductiva - Multinomial]
    Fijado $p\in \bb{N}$, consideramos $p$ vectores aleatorios $X_1, \ldots, X_p$ \emph{independientes} tales que $X_i\sim M_k(n_i, p_1, \ldots, p_k)$, $i=1, \ldots, p$. Se tiene entonces que:
    \begin{equation*}
        \sum_{i=1}^p X_i \sim M_k\left(\sum_{i=1}^p n_i, p_1, \ldots, p_k\right)
    \end{equation*}
\end{prop}
\begin{proof}
    Por ser independientes, tenemos que:
    \begin{align*}
        M_{\sum\limits_{i=1}^p X_i}(t_1, \ldots, t_k)&=\prod_{i=1}^p M_{X_i}(t_1, \ldots, t_k)=\prod_{i=1}^p \left[\left(\sum_{j=1}^k p_j e^{t_j}\right)+\left(1-\sum_{j=1}^k p_j\right)\right]^{n_i}
        =\\&= \sum_{j=1}^p \left[\left(\sum_{j=1}^k p_j e^{t_j}\right)+\left(1-\sum_{j=1}^k p_j\right)\right]^{\sum\limits_{i=1}^p n_i}
    \end{align*}
    Por tanto, la función generatriz de momentos de $\sum_{i=1}^p X_i$ coincide con la de una distribución multinomial con los parámetros $\sum_{i=1}^p n_i$ y las probabilidades asociadas a las componentes de $X_i$.
\end{proof}

Veamos ahora la esperanza de la distribución multinomial.
\begin{prop}
    Sea $X\sim M_k(n, p_1, \ldots, p_k)$. Entonces, la esperanza de $X$ es:
    \begin{equation*}
        E[X]=n\left(p_1, \ldots, p_k\right)
    \end{equation*}
\end{prop}
\begin{proof}
    De la definición de esperanza, si $X=(X_1, \ldots, X_k)$, tenemos que:
    \begin{equation*}
        E[X]=\left(E[X_1], \ldots, E[X_k]\right)
    \end{equation*}

    Como hemos visto anteriormente, para cada $j\in \{1, \ldots, k\}$, tenemos que $X_j\sim B(n, p_j)$, por lo que $E[X_j]=n\cdot p_j$. Por tanto, tenemos que:
    \begin{equation*}
        E[X]=\left(n\cdot p_1, \ldots, n\cdot p_k\right)=n\left(p_1, \ldots, p_k\right)
    \end{equation*}
\end{proof}

Veamos ahora la covarianza de cada par de componentes de la distribución multinomial.
\begin{prop}
    Sea $X\sim M_k(n, p_1, \ldots, p_k)$. Entonces, si $X=(X_1, \ldots, X_k)$, se tiene que:
    \begin{equation*}
        \Cov(X_i, X_j)=-n\cdot p_i p_j \qquad \forall i,j\in \{1, \ldots, k\}, i\neq j
    \end{equation*}
\end{prop}
\begin{proof}
    De la definición de covarianza, tenemos que:
    \begin{equation*}
        \Cov(X_i, X_j)=E[X_i\cdot X_j]-E[X_i]\cdot E[X_j]
    \end{equation*}

    Para calcular $E[X_i\cdot X_j]$, usamos la función generatriz de momentos de $X$:
    \begin{align*}
        E[X_i\cdot X_j]&=\dfrac{\partial^2 M_X(t_1, \ldots, t_k)}{\partial t_i\partial t_j}\Bigg|_{t_1=\ldots=t_k=0}
        =\\&= n(n-1)p_ip_je^{t_i}e^{t_j}\left[\left(\sum_{i=1}^k p_i e^{t_i}\right)+\left(1-\sum_{i=1}^k p_i\right)\right]^{n-2}\Bigg|_{t_1=\ldots=t_k=0}
        =\\&= n(n-1)p_ip_j\left[\left(\sum_{i=1}^k p_i\right)+\left(1-\sum_{i=1}^k p_i\right)\right]^{n-2}
        = n(n-1)p_ip_j
    \end{align*}

    Por tanto, como $E[X_i]=n\cdot p_i$, $E[X_j]=n\cdot p_j$, tenemos que:
    \begin{equation*}
        \Cov(X_i, X_j)=n(n-1)p_ip_j-n^2p_ip_j=-n\cdot p_i\cdot p_j
    \end{equation*}
\end{proof}