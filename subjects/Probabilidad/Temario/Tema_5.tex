\chapter{Algunos Modelos Multivariantes}

En el presente Capítulo, y al igual que hemos hecho en el caso unidimensional, vamos a estudiar ahora ciertos modelos multivariantes que nos permitirán trabajar con más de una variable aleatoria.
En concreto, vamos a generalizar la distribución binomial y la normal, ya estudiadas previamente.

\section{Distribución Multinomial}

Como ya hemos adeñantado, esta distribución es una generalización de la distribución binomial.
Fijamos $k\in \bb{N}$, y consideramos un experimento aleatorio con $k+1$ posibles resultados asociados a otros tantos sucesos $A_1, A_2, \ldots, A_{k+1}$ exhaustivos y mutuamente excluyentes, por lo tanto constituyen una partición del espacio muestral, con probabilidad de ocurrencia $p_i \in \left]0,1\right[$, $i=1, \ldots, k+1$. Es decir:
\begin{align*}
    \Omega&=\bigcup_{i=1}^{k+1}A_i,\\
    A_i\cap A_j&=\emptyset, \quad i,j=1, \ldots, k+1,\qquad i\neq j,\\
    P(A_i)&=p_i, \quad i=1, \ldots, k+1
\end{align*}

Tenemos entonces que:
\begin{align*}
    P(A_{k+1})&=1-P\left(\bigcup_{i=1}^k A_i\right)=1-\sum_{i=1}^k P(A_i)=1-\sum_{i=1}^k p_i
\end{align*}

Supongamos que realizamos $n$ repeticiones independientes del experimento en las mismas condiciones, por tanto las probabilidades $p_i$, $i=1, 2, \ldots, k+1$ se mantienen constantes a lo largo de las repeticiones, y sean $X_1, \ldots, X_{k+1}$ variables aleatorias tales que $X_i$ contabiliza el número de veces que ocurre el suceso $A_i$ en las $n$ repeticiones del experimento, $i=1, 2, \ldots, k+1$. Tenemos que:
\begin{equation*}
    X_1+X_2+\ldots+X_{k+1}=n
\end{equation*}

En estas condiciones, podemos definir la distribución multinomial.
\begin{definicion}[Distribución Multinomial]
    En las condiciones anteriores, se define la distribución multinomial $k-$dimensional con parámetros $n$ y $p_1, \ldots, p_k$ como la distribución del vector aleatorio $X=(X_1, \ldots, X_k)$ en el cual cada componente $X_i$ contabiliza el número de ocurrencias del suceso $A_i$, $i=1, \ldots, k$. La notaremos por:
    \begin{align*}
        X\sim M_k(n, p_1, \ldots, p_k)
    \end{align*}
\end{definicion}
\begin{observacion}
    Notemos que la distribución binomial es un caso particular de la distribución multinomial, en concreto, la distribución multinomial con $k=1$.
\end{observacion}

Razonemos ahora la función de masa de probabilidad de la distribución multinomial. Consideramos el vector $x=(x_1, \ldots, x_k)$ tal que $x_i\in \bb{N}_0$, $i=1, \ldots, k$. Como se realizan $n$ repeticiones del experimento, tenemos que:
\begin{equation*}
    \sum_{i=1}^k x_i\leq n
\end{equation*}
donde la igualdad solo se dará si no se produce el suceso $A_{k+1}$ en ninguna de las repeticiones.
Una de las posibles ordenaciones para que $x$ tome dicho valor es que primero ocurran $x_1$ veces el suceso $A_1$, luego $x_2$ veces el suceso $A_2$, y así sucesivamente, hasta que ocurran $x_k$ veces el suceso $A_k$ y finalmente ocurran $n-\sum_{i=1}^k x_i$ veces el suceso $A_{k+1}$. Es decir, el suceso sería (notando $A_i^{x_i}$ al suceso $A_i$ ocurrido $x_i$ veces):
\begin{align*}
    A_1^{x_1}\cap A_2^{x_2}\cap \ldots \cap A_k^{x_k}\cap A_{k+1}^{n-\sum\limits_{i=1}^k x_i}
\end{align*}

La probabilidad de que ocurra este suceso, usando la independencia de las repeticiones del experimento, es:
\begin{align*}
    P\left[\bigcap_{i=1}^{k}\left(\bigcap_{j=1}^{x_i}A_i\right)\cap \left(\bigcap_{j=1}^{n-\sum\limits_{i=1}^k x_i}A_{k+1}\right)\right]=\prod_{i=1}^k p_i^{x_i}\cdot p_{k+1}^{n-\sum\limits_{i=1}^k x_i}=\prod_{i=1}^{k} p_i^{x_i} \cdot \left(1-\sum_{i=1}^k p_i\right)^{n-\sum\limits_{i=1}^k x_i}
\end{align*}


Veamos ahora cuántas ordenaciones distintas nos pueden dar dicho vector $x$. Estas ordenaciones son permutaciones con repetición de $n$ elementos clasificados en $k+1$ grupos, habiendo $x_i$ del tipo $i-$ésimo, $i=1, \ldots, k$ y $n-\sum_{i=1}^k x_i$ del grupo $k+1-$ésimo. Por tanto, el número de ordenaciones posibles es\footnote{Ver sección de Combinatoria en los apuntes de EDIP.}:
\begin{equation*}
    PR_{n}^{x_1, \ldots, x_k, n-\sum_{i=1}^k x_i}=\dfrac{n!}{\left(\prod\limits_{i=1}^k x_i!\right) \cdot \left(n-\sum\limits_{i=1}^k x_i\right)!}
\end{equation*}

Por tanto, tenemos que:
\begin{equation*}
    P[X=(x_1,\dots,x_k)]=\dfrac{n!}{\left(\prod\limits_{i=1}^k x_i!\right)\cdot \left(n-\sum\limits_{i=1}^k x_i\right)!}\cdot \prod_{i=1}^{k} p_i^{x_i} \cdot \left(1-\sum_{i=1}^k p_i\right)^{n-\sum\limits_{i=1}^k x_i}
\end{equation*}

Comprobemos que, efectivamente, esta función de masa de probabilidad es tal.
Para ello, es necesario introducir el siguiente lema técnico, descubierto por Leibniz.
\begin{lema}[Fórmula de Leibniz]
    Sean $p_1, \ldots, p_j$ números reales y $n\in \bb{N}$. Entonces, se cumple que:
    \begin{equation*}
        \left(\sum_{i=1}^j p_i\right)^n=\sum_{\sum\limits_{i=1}^j x_i=n}\dfrac{n!}{\prod\limits_{i=1}^j x_i!}\cdot \prod_{i=1}^j p_i^{x_i}
    \end{equation*}
\end{lema}
\begin{prop}
    Sea $X\sim M_k(n, p_1, \ldots, p_k)$. Sean $x_1, \ldots, x_k$ tales que $x_i\in \{0,\dots,n\}$, $i=1, \ldots, k$ y $\sum\limits_{i=1}^k x_i\leq n$. Entonces, la función de masa de probabilidad de $X$ es:
    \begin{equation*}
        P[X=(x_1,\dots,x_k)]=\dfrac{n!}{\left(\prod\limits_{i=1}^k x_i!\right)\cdot \left(n-\sum\limits_{i=1}^k x_i\right)!}\cdot \prod_{i=1}^{k} p_i^{x_i} \cdot \left(1-\sum\limits_{i=1}^k p_i\right)^{n-\sum\limits_{i=1}^k x_i}
    \end{equation*}
\end{prop}
\begin{proof}
    Usaremos para ello la Fórmula de Leibniz identificando $k+1=j$ y definiendo $p_{k+1}=1-\sum\limits_{i=1}^k p_i$ y $x_{k+1}=n-\sum\limits_{i=1}^k x_i$:
    \begin{align*}
        \sum_{\sum\limits_{i=1}^k x_i\leq n} &\dfrac{n!}{\left(\prod\limits_{i=1}^k x_i!\right)\cdot \left(n-\sum\limits_{i=1}^k x_i\right)!}\cdot \prod_{i=1}^{k} p_i^{x_i} \cdot \left(1-\sum\limits_{i=1}^k p_i\right)^{n-\sum\limits_{i=1}^k x_i}
        =\\&=
        \sum_{\sum\limits_{i=1}^{k+1} x_i=n}\dfrac{n!}{\prod\limits_{i=1}^{k+1} x_i!}\cdot \prod_{i=1}^{k+1} p_i^{x_i}
        = \left(\sum_{i=1}^{k+1} p_i\right)^n = \left(\sum_{i=1}^{k} p_i+1-\sum_{i=1}^{k} p_i\right)^n=1^n=1
    \end{align*}

    Además, como todos los términos de la suma son no negativos, tenemos efectivamente que se trata de una función de masa de probabilidad.
\end{proof}

Calculemos ahora la función generatriz de momentos de la distribución multinomial.
\begin{prop}
    Sea $X\sim M_k(n, p_1, \ldots, p_k)$. Entonces, la función generatriz de momentos de $X$ es:
    \begin{equation*}
        M_X(t_1, \ldots, t_k)=\left[\left(\sum_{i=1}^k p_i e^{t_i}\right) + \left(1-\sum_{i=1}^k p_i\right)\right]^n \qquad \forall t_1, \ldots, t_k\in \bb{R}
    \end{equation*}
\end{prop}
\begin{proof}
    Usaremos la definición de función generatriz de momentos:
    \begin{align*}
        M_X(t_1&, \ldots, t_k)=E[e^{t_1 X_1+\ldots+t_k X_k}]
        =\\&=
        \sum_{\sum\limits_{i=1}^k x_i\leq n} e^{t_1 x_1+\ldots+t_k x_k}\cdot \dfrac{n!}{\left(\prod\limits_{i=1}^k x_i!\right)\cdot \left(n-\sum\limits_{i=1}^k x_i\right)!}\cdot \prod_{i=1}^{k} p_i^{x_i} \cdot \left(1-\sum\limits_{i=1}^k p_i\right)^{n-\sum\limits_{i=1}^k x_i}
        =\\&=
        \sum_{\sum\limits_{i=1}^k x_i\leq n} \prod_{i=1}^k (e^{t_i})^{x_i}\cdot \dfrac{n!}{\left(\prod\limits_{i=1}^k x_i!\right)\cdot \left(n-\sum\limits_{i=1}^k x_i\right)!}\cdot \prod_{i=1}^{k} p_i^{x_i} \cdot \left(1-\sum\limits_{i=1}^k p_i\right)^{n-\sum\limits_{i=1}^k x_i}
        =\\&=
        \sum_{\sum\limits_{i=1}^k x_i\leq n} \dfrac{n!}{\left(\prod\limits_{i=1}^k x_i!\right)\cdot \left(n-\sum\limits_{i=1}^k x_i\right)!}\cdot \prod_{i=1}^{k} (p_ie^{t_i})^{x_i} \cdot \left(1-\sum\limits_{i=1}^k p_i\right)^{n-\sum\limits_{i=1}^k x_i}
        \AstIg\\&\AstIg
        \left[\left(\sum_{i=1}^k p_i e^{t_i}\right)+\left(1-\sum_{i=1}^k p_i\right)\right]^n
    \end{align*}
    donde en $(\ast)$ hemos usado la Fórmula de Leibniz.
\end{proof}

Calculamos ahora las marginales de la distribución multinomial.
\begin{prop}
    Dado el vector aleatorio $X\sim M_k(n, p_1, \ldots, p_k)$, consideramos un subvector de $X$ $(X_{i_1}, \ldots, X_{i_l})$, con $i_1, \ldots, i_l\in \{1, \ldots, k\}$, siendo $i_p\neq i_q$ si $p\neq q$. Entonces,
    $$(X_{i_1}, \ldots, X_{i_l})\sim M_l(n, p_{i_1}, \ldots, p_{i_l}).$$
\end{prop}
\begin{observacion}
    Notemos que esta notación no es del todo precisa, pero se emplea para no complicar la notación. La distribución marginal de un subvector aleatorio de $l$ componentes sigue una distribución multinomial con los parámetros $n$ y las probabilidades asociadas a las componentes del subvector.
\end{observacion}
\begin{proof}
    Esto se deduce directamente de la función generatiz de momentos de un subvector de $X$, que es:
    \begin{align*}
        M_{(X_{i_1}, \ldots, X_{i_l})}(t_{i_1}, \ldots, t_{i_l})&=M_X(0,\dots,0,t_{i_1},0,\dots,0,t_{i_l},0,\dots,0)
        =\\&=
        \left[\left(\sum_{i=1}^k p_i e^{t_i}\right)+\left(1-\sum_{i=1}^k p_i\right)\right]^n
        =\\&=
        \left[\left(\sum_{j=1}^l p_{i_j} e^{t_{i_j}}\right)+\left(1-\sum_{j=1}^l p_{i_j}\right)\right]^n
    \end{align*}
    Como esta función generatriz de momentos coincide con la de una distribución multinomial con los parámetros $n$ y las probabilidades asociadas a las componentes del subvector, se tiene que la distribución marginal de un subvector de $l$ componentes de $X$ sigue una distribución multinomial con los parámetros $n$ y las probabilidades asociadas a las componentes del subvector.
\end{proof}

Tenemos que, en particular, para $j\in \{1,\dots,k\}$, tenemos que:
\begin{equation*}
    X_j\sim M_1(n, p_j)
\end{equation*}
Su función generatriz de momentos entonces es:
\begin{equation*}
    M_{X_j}(t)=\left[p_j e^t+(1-p_j)\right]^n
\end{equation*}
Por la unicidad de la función generatriz de momentos, tenemos que $X_j\sim B(n, p_j)$, como introducimos ya anteriormente.\\

Calculamos ahora las distribuciones condicionadas de la distribución multinomial.
\begin{prop}
    Consideramos el vector aleatorio $X\sim M_k(n, p_1, \ldots, p_k)$. Sean los conjuntos \emph{disjuntos} $\{i_1, \ldots, i_q\}, \{j_1, \ldots, j_p\}\subset \{1, \ldots, k\}$, con $i_p\neq i_q$ y $j_p\neq j_q$ si $p\neq q$. Entonces:
    \begin{equation*}
        (X_{i_1}, \ldots, X_{i_q})\mid (X_{j_1}=x_{j_1}, \ldots, X_{j_p}=x_{j_p})\sim M_q\left(n-\sum_{l=1}^p x_{j_l}, \dfrac{p_{i_1}}{1-\sum\limits_{l=1}^p p_{j_l}}, \ldots, \dfrac{p_{i_q}}{1-\sum\limits_{l=1}^p p_{j_l}}\right)
    \end{equation*}
\end{prop}
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T09_CondicionadasMultinomial.pdf#[0,{%22name%22:%22Fit%22}]
\begin{observacion}
    De nuevo, la notación no es del todo precisa, pero se emplea para no complicar la notación.

    La distribución condicionada de un subvector de $q$ componentes de $X$ dado que otro subvector de $p$ componentes de $X$ toma unos valores concretos, sigue una distribución multinomial con los parámetros $n$ menos la suma de los valores que toman las componentes del subvector condicionante, y las probabilidades asociadas a las componentes del subvector condicionado divididas por la probabilidad $1$ menos la suma de las probabilidades asociadas a las componentes del subvector condicionante.
\end{observacion}

En el caso de que condicionemos una componente $X_i$ de $X$ a otra componente $X_j$ de $X$ ($i\neq j$), tenemos que:
\begin{equation*}
    X_i \mid X_j=x_j\sim M_1 \left(n-x_j, \dfrac{p_i}{1-p_j}\right) = B\left(n-x_j, \dfrac{p_i}{1-p_j}\right)
\end{equation*}

Veamos ahora que se trata de una distribución reproductiva.
\begin{prop}[Distribución Reproductiva - Multinomial]
    Fijado $p\in \bb{N}$, consideramos $p$ vectores aleatorios $X_1, \ldots, X_p$ \emph{independientes} tales que $X_i\sim M_k(n_i, p_1, \ldots, p_k)$, $i=1, \ldots, p$. Se tiene entonces que:
    \begin{equation*}
        \sum_{i=1}^p X_i \sim M_k\left(\sum_{i=1}^p n_i, p_1, \ldots, p_k\right)
    \end{equation*}
\end{prop}
\begin{proof}
    Por ser independientes, tenemos que:
    \begin{align*}
        M_{\sum\limits_{i=1}^p X_i}(t_1, \ldots, t_k)&=\prod_{i=1}^p M_{X_i}(t_1, \ldots, t_k)=\prod_{i=1}^p \left[\left(\sum_{j=1}^k p_j e^{t_j}\right)+\left(1-\sum_{j=1}^k p_j\right)\right]^{n_i}
        =\\&= \sum_{j=1}^p \left[\left(\sum_{j=1}^k p_j e^{t_j}\right)+\left(1-\sum_{j=1}^k p_j\right)\right]^{\sum\limits_{i=1}^p n_i}
    \end{align*}
    Por tanto, la función generatriz de momentos de $\sum_{i=1}^p X_i$ coincide con la de una distribución multinomial con los parámetros $\sum_{i=1}^p n_i$ y las probabilidades asociadas a las componentes de $X_i$.
\end{proof}

Veamos ahora la esperanza de la distribución multinomial.
\begin{prop}
    Sea $X\sim M_k(n, p_1, \ldots, p_k)$. Entonces, la esperanza de $X$ es:
    \begin{equation*}
        E[X]=n\left(p_1, \ldots, p_k\right)
    \end{equation*}
\end{prop}
\begin{proof}
    De la definición de esperanza, si $X=(X_1, \ldots, X_k)$, tenemos que:
    \begin{equation*}
        E[X]=\left(E[X_1], \ldots, E[X_k]\right)
    \end{equation*}

    Como hemos visto anteriormente, para cada $j\in \{1, \ldots, k\}$, tenemos que $X_j\sim B(n, p_j)$, por lo que $E[X_j]=n\cdot p_j$. Por tanto, tenemos que:
    \begin{equation*}
        E[X]=\left(n\cdot p_1, \ldots, n\cdot p_k\right)=n\left(p_1, \ldots, p_k\right)
    \end{equation*}
\end{proof}

Veamos ahora la covarianza de cada par de componentes de la distribución multinomial.
\begin{prop}
    Sea $X\sim M_k(n, p_1, \ldots, p_k)$. Entonces, si $X=(X_1, \ldots, X_k)$, se tiene que:
    \begin{equation*}
        \Cov(X_i, X_j)=-n\cdot p_i p_j \qquad \forall i,j\in \{1, \ldots, k\}, i\neq j
    \end{equation*}
\end{prop}
\begin{proof}
    De la definición de covarianza, tenemos que:
    \begin{equation*}
        \Cov(X_i, X_j)=E[X_i\cdot X_j]-E[X_i]\cdot E[X_j]
    \end{equation*}

    Para calcular $E[X_i\cdot X_j]$, usamos la función generatriz de momentos de $X$:
    \begin{align*}
        E[X_i\cdot X_j]&=\dfrac{\partial^2 M_X(t_1, \ldots, t_k)}{\partial t_i\partial t_j}\Bigg|_{t_1=\ldots=t_k=0}
        =\\&= n(n-1)p_ip_je^{t_i}e^{t_j}\left[\left(\sum_{i=1}^k p_i e^{t_i}\right)+\left(1-\sum_{i=1}^k p_i\right)\right]^{n-2}\Bigg|_{t_1=\ldots=t_k=0}
        =\\&= n(n-1)p_ip_j\left[\left(\sum_{i=1}^k p_i\right)+\left(1-\sum_{i=1}^k p_i\right)\right]^{n-2}
        = n(n-1)p_ip_j
    \end{align*}

    Por tanto, como $E[X_i]=n\cdot p_i$, $E[X_j]=n\cdot p_j$, tenemos que:
    \begin{equation*}
        \Cov(X_i, X_j)=n(n-1)p_ip_j-n^2p_ip_j=-n\cdot p_i\cdot p_j
    \end{equation*}
\end{proof}


\section{Distribución Normal Bidimensional}

Generalizamos ahora la distribución normal a dos dimensiones.
\begin{definicion}[Distribución Normal Bidimensional]
    Consideramos un vector aleatorio bidimensional continuo $X=(X_1, X_2)$ 
    y los parámetros $\mu_1, \mu_2\in \bb{R}$, $\sigma_1^2, \sigma_2^2\in \bb{R}^+$ y $\rho\in \left]-1,1\right[$.
    Consideramos además las siguientes matrices:
    \begin{align*}
        \Sigma&=\begin{pmatrix}
            \sigma_1^2 & \rho\sigma_1\sigma_2\\
            \rho\sigma_1\sigma_2 & \sigma_2^2
        \end{pmatrix}\\
        \mu&=\begin{pmatrix}
            \mu_1 & \mu_2
        \end{pmatrix}
    \end{align*}
    donde $\sigma_i=\sqrt{\sigma_i^2}$, $i=1,2$.
    Diremos que $X$ sigue una distribución normal bidimensional con parámetros $\mu_1, \mu_2, \sigma_1^2, \sigma_2^2$ y $\rho$ si su función de densidad es:
    \begin{equation*}
        f_X(x)=\dfrac{1}{2\pi\sqrt{|\Sigma|}}\exp\left(-\dfrac{(x-\mu)\Sigma^{-1}(x-\mu)^t}{2}\right)\qquad x\in \bb{R}^2
    \end{equation*}

    La notaremos por las siguientes dos formas:
    \begin{equation*}
        X\sim \cc{N}_2(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \rho)\quad \text{o}\quad X\sim \cc{N}_2\left(\mu, \Sigma\right)
    \end{equation*}
    donde $\mu, \Sigma$ son las matrices definidas anteriormente.
\end{definicion}

Veamos que está bien definida. En primer lugar, hemos de calcular $|\Sigma|$:
\begin{equation*}
    |\Sigma|=\sigma_1^2\sigma_2^2-\rho^2\sigma_1^2\sigma_2^2=\sigma_1^2\sigma_2^2(1-\rho^2)> 0
\end{equation*}

Por tanto, $\Sigma$ es singular y, por tanto, $\exists \Sigma^{-1}$. Calculémosla:
\begin{equation*}
    \Sigma^{-1}=\dfrac{1}{|\Sigma|}\begin{pmatrix}
        \sigma_2^2 & -\rho\sigma_1\sigma_2\\
        -\rho\sigma_1\sigma_2 & \sigma_1^2
    \end{pmatrix}
\end{equation*}

Calculamos ahora el producto de matrices descrito, considerando $x=(x_1,x_2)$:
\begin{align*}
    (x-\mu)\Sigma^{-1}&(x-\mu)^t=\frac{1}{|\Sigma|}\cdot \begin{pmatrix}
        x_1-\mu_1 & x_2-\mu_2
    \end{pmatrix}\begin{pmatrix}
        \sigma_2^2 & -\rho\sigma_1\sigma_2\\
        -\rho\sigma_1\sigma_2 & \sigma_1^2
    \end{pmatrix}\begin{pmatrix}
        x_1-\mu_1\\
        x_2-\mu_2
    \end{pmatrix}
    =\\&= \frac{1}{|\Sigma|}\cdot 
    \begin{pmatrix}
        x_1-\mu_1 & x_2-\mu_2
    \end{pmatrix}\begin{pmatrix}
        \sigma_2^2(x_1-\mu_1)-\rho\sigma_1\sigma_2(x_2-\mu_2)\\
        -\rho\sigma_1\sigma_2(x_1-\mu_1)+\sigma_1^2(x_2-\mu_2)
    \end{pmatrix}
    =\\&=\frac{1}{\sigma_1^2\sigma_2^2\cdot (1-\rho^2)}\cdot \left[
    \sigma_2^2(x_1-\mu_1)^2-2\rho\sigma_1\sigma_2(x_1-\mu_1)(x_2-\mu_2)+\sigma_1^2(x_2-\mu_2)^2
    \right]
    = \\&= \dfrac{1}{1-\rho^2}\left[\left(\dfrac{x_1-\mu_1}{\sigma_1}\right)^2-2\rho\left(\dfrac{x_1-\mu_1}{\sigma_1}\right)\left(\dfrac{x_2-\mu_2}{\sigma_2}\right)+\left(\dfrac{x_2-\mu_2}{\sigma_2}\right)^2\right]
\end{align*}

Por tanto, la función de densidad de la distribución normal bidimensional es:
\begin{equation*}
    \hspace{-2cm}
    f_X(x_1,x_2)=\dfrac{1}{2\pi \sigma_1\sigma_2\sqrt{1-\rho^2}}\exp{\left(-\frac{1}{2(1-\rho^2)}\left[\left(\frac{x_1-\mu_1}{\sigma_1}\right)^2-2\rho\left(\frac{x_1-\mu_1}{\sigma_1}\right)\left(\frac{x_2-\mu_2}{\sigma_2}\right)+\left(\frac{x_2-\mu_2}{\sigma_2}\right)^2\right]\right)}
\end{equation*}


Veamos ahora distintas expresiones alternativas para la función de densidad de la distribución normal bidimensional.
Multiplicando el término de $\left(\frac{x_1-\mu_1}{\sigma_1}\right)^2$ por $(1-\rho^2+\rho^2)$, el exponente queda:
\begin{align*}
    &-\frac{1}{2(1-\rho^2)}\left[\left(\frac{x_1-\mu_1}{\sigma_1}\right)^2(1-\rho^2+\rho^2)-2\rho\left(\frac{x_1-\mu_1}{\sigma_1}\right)\left(\frac{x_2-\mu_2}{\sigma_2}\right)+\left(\frac{x_2-\mu_2}{\sigma_2}\right)^2\right]
    =\\&= -\frac{\left(x_1-\mu_1\right)^2}{2\sigma_1^2}-\frac{1}{2(1-\rho^2)}\left[\rho^2\left(\frac{x_1-\mu_1}{\sigma_1}\right)^2-2\rho\left(\frac{x_1-\mu_1}{\sigma_1}\right)\left(\frac{x_2-\mu_2}{\sigma_2}\right)+\left(\frac{x_2-\mu_2}{\sigma_2}\right)^2\right]
    =\\&= -\frac{\left(x_1-\mu_1\right)^2}{2\sigma_1^2}-\frac{1}{2(1-\rho^2)}\left[-\rho\left(\frac{x_1-\mu_1}{\sigma_1}\right)+\left(\frac{x_2-\mu_2}{\sigma_2}\right)\right]^2
    =\\&= -\frac{\left(x_1-\mu_1\right)^2}{2\sigma_1^2}-\frac{1}{2\sigma_2^2(1-\rho^2)}\left[-\rho\cdot \frac{\sigma_2}{\sigma_1}\cdot (x_1-\mu_1)+ (x_2-\mu_2)\right]^2
\end{align*}

Por tanto, podemos descomponer la función de densidad de la distribución normal bidimensional en dos funciones:
\begin{multline}\label{eq:descomposicion_normal_bidimensional}
    f_X(x_1,x_2)
    =\overbrace{\dfrac{1}{\sqrt{2\pi}\sigma_2\sqrt{1-\rho^2}}\exp\left(-\dfrac{1}{2\sigma_2^2(1-\rho^2)}\left[-\rho\cdot \frac{\sigma_2}{\sigma_1}\cdot (x_1-\mu_1)+ (x_2-\mu_2)\right]^2\right)}^{h_{x_1}(x_2)}
    \cdot\\\qquad \cdot \underbrace{\dfrac{1}{\sqrt{2\pi}\sigma_1}\exp\left(-\dfrac{(x_1-\mu_1)^2}{2\sigma_1^2}\right)}_{g(x_1)}
\end{multline}
Tenemos que $g(x_1)$ es la función de densidad de $\cc{N}(\mu_1, \sigma_1^2)$ y, para cada $x_1$, $h_{x_1}(x_2)$ es la función de densidad de $\cc{N}(\mu_2+\rho\cdot \frac{\sigma_2}{\sigma_1}\cdot (x_1-\mu_1), \sigma_2^2(1-\rho^2))$.\\
\begin{comment}
Repitiendo el proceso con el término $\left(\frac{x_2-\mu_2}{\sigma_2}\right)^2$, obtenemos que:
\begin{align*}
    f_X(x_1,x_2)
    &=\overbrace{\dfrac{1}{\sqrt{2\pi}\sigma_2}\exp\left(-\dfrac{(x_2-\mu_2)^2}{2\sigma_2^2}\right)}^{p(x_2)}\cdot
    \cdot\\&\qquad \cdot\underbrace{\dfrac{1}{\sqrt{2\pi}\sigma_1\sqrt{1-\rho^2}}\exp\left(-\dfrac{1}{2\sigma_1^2(1-\rho^2)}\left[-\rho\cdot \frac{\sigma_1}{\sigma_2}\cdot (x_2-\mu_2)+ (x_1-\mu_1)\right]^2\right)}_{q_{x_2}(x_1)}
\end{align*}
En este caso, $p(x_2)$ es la función de densidad de $\cc{N}(\mu_2, \sigma_2^2)$ y, para cada $x_2$, $q_{x_2}(x_1)$ es la función de densidad de $\cc{N}(\mu_1+\rho\cdot \frac{\sigma_1}{\sigma_2}\cdot (x_2-\mu_2), \sigma_1^2(1-\rho^2))$.
\end{comment}
Esta descomposición es útil, y la emplearemos en diversas demostraciones en el futuro. Se podrá considerar también la descomposición análoga.


Para terminar de aceptar dicha definición, hemos de comprobar que, efectivamente, se trata de una función de densidad.
Como $f_X(x)\geq 0$ $\forall x\in \bb{R}^2$, tan solo nos falta por comprobar que:
\begin{align*}
    \int_{\bb{R}^2} f_X(x)dx
    &= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} f_X(x_1,x_2)dx_2dx_1
    \AstIg
    \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} g(x_1)h_{x_1}(x_2)dx_2dx_1
    =\\&=
    \int_{-\infty}^{+\infty} g(x_1) \int_{-\infty}^{+\infty} h_{x_1}(x_2)dx_2\ dx_1
    \stackrel{(\ast\ast)}{=}
    \int_{-\infty}^{+\infty} g(x_1)dx_1
    \stackrel{(\ast\ast)}{=} 1
\end{align*}
donde en $(\ast)$ hemos empleado la descomposición de la Ecuación~\ref{eq:descomposicion_normal_bidimensional} y en $(\ast\ast)$ hemos usado que $g(x_1)$ y $h_{x_1}(x_2)$ son funciones de densidad.

Veamos ahora las marginales de la distribución normal bidimensional.
\begin{prop}
    Sean los parámetros $\mu_1, \mu_2\in \bb{R}$, $\sigma_1^2, \sigma_2^2\in \bb{R}^+$ y $\rho\in \left]-1,1\right[$, y sean sus matrices $\mu$ y $\Sigma$ asociadas.
    Si $(X_1, X_2)\sim \cc{N}_2(\mu, \Sigma)$, entonces:
    \begin{equation*}
        X_1\sim \cc{N}(\mu_1, \sigma_1^2)\quad \text{y}\quad X_2\sim \cc{N}(\mu_2, \sigma_2^2)
    \end{equation*}
\end{prop}
\begin{proof}
    Calculamos la marginal de $X_1$:
    \begin{equation*}
        f_{X_1}(x_1)=\int_{-\infty}^{+\infty} f_X(x_1,x_2)dx_2
        \AstIg \int_{-\infty}^{+\infty} g(x_1)h_{x_1}(x_2)dx_2
        = g(x_1)\int_{-\infty}^{+\infty} h_{x_1}(x_2)dx_2
        \stackrel{(\ast\ast)}{=} g(x_1)
    \end{equation*}
    donde en $(\ast)$ hemos empleado la descomposición de la Ecuación~\ref{eq:descomposicion_normal_bidimensional} y en $(\ast\ast)$ hemos usado que $h_{x_1}(x_2)$ es una función de densidad.
    Por tanto, como vimos que $g(x_1)$ es la función de densidad de $\cc{N}(\mu_1, \sigma_1^2)$, tenemos que $X_1\sim \cc{N}(\mu_1, \sigma_1^2)$.
    Usando la descomposición análoga, se demuestra que $X_2\sim \cc{N}(\mu_2, \sigma_2^2)$.
\end{proof}

Veamos ahora las condicionadas de la distribución normal bidimensional.
\begin{prop}
    Sean los parámetros $\mu_1, \mu_2\in \bb{R}$, $\sigma_1^2, \sigma_2^2\in \bb{R}^+$ y $\rho\in \left]-1,1\right[$, y sean sus matrices $\mu$ y $\Sigma$ asociadas.
    Si $(X_1, X_2)\sim \cc{N}_2(\mu, \Sigma)$, entonces, dado $x_1^\ast,~x_2^\ast\in \bb{R}$, se tiene que:
    \begin{align*}
        X_1\mid X_2=x_2^\ast &\sim \cc{N}\left(\mu_1+\rho\cdot \frac{\sigma_1}{\sigma_2}\cdot (x_2^\ast-\mu_2), \sigma_1^2(1-\rho^2)\right)\\
        X_2\mid X_1=x_1^\ast &\sim \cc{N}\left(\mu_2+\rho\cdot \frac{\sigma_2}{\sigma_1}\cdot (x_1^\ast-\mu_1), \sigma_2^2(1-\rho^2)\right)
    \end{align*}
\end{prop}
\begin{proof}
    Dado $x_1^\ast\in \bb{R}$, calculamos la condicionada de $X_2$:
    \begin{equation*}
        f_{X_2\mid X_1=x_1^\ast}(x_2)=\dfrac{f_X(x_1^\ast, x_2)}{f_{X_1}(x_1^\ast)}
        \AstIg \dfrac{g(x_1^\ast)h_{x_1^\ast}(x_2)}{g(x_1^\ast)}
        = h_{x_1^\ast}(x_2)
    \end{equation*}
    donde en $(\ast)$ hemos empleado la descomposición de la Ecuación~\ref{eq:descomposicion_normal_bidimensional}. Como $h_{x_1^\ast}(x_2)$ es la función de densidad de $\cc{N}\left(\mu_2+\rho\cdot \frac{\sigma_2}{\sigma_1}\cdot (x_1^\ast-\mu_1), \sigma_2^2(1-\rho^2)\right)$, tenemos que $X_2\mid X_1=x_1^\ast\sim \cc{N}\left(\mu_2+\rho\cdot \frac{\sigma_2}{\sigma_1}\cdot (x_1^\ast-\mu_1), \sigma_2^2(1-\rho^2)\right)$.\\
\end{proof}
\begin{observacion}
    Una vez llegados aquí, podemos olvidarnos de la descomposición de la función de densidad de la distribución normal bidimensional descrita en la Ecuación~\ref{eq:descomposicion_normal_bidimensional}.
    Esta descomposición en verdad era:
    \begin{align*}
        f_{X}(x_1,x_2) = f_{X_1}(x_1)f_{X_2\mid X_1=x_1}(x_2)
    \end{align*}
    Esta descomposición ya es conocida por el lector.
\end{observacion}


Veamos ahora la función generatriz de momentos de la distribución normal bidimensional.
\begin{prop}
    Sean los parámetros $\mu_1, \mu_2\in \bb{R}$, $\sigma_1^2, \sigma_2^2\in \bb{R}^+$ y $\rho\in \left]-1,1\right[$, y sean sus matrices $\mu$ y $\Sigma$ asociadas.
    Si $(X_1, X_2)\sim \cc{N}_2(\mu, \Sigma)$, entonces, la función generatriz de momentos de $X_1$ y $X_2$ es:    
    \begin{align*}
        M_X(t) &= \exp\left(t\mu^t +\dfrac{t\Sigma t^t}{2}\right)
        =\\&= \exp\left(t_1\mu_1+t_2\mu_2+\dfrac{t_1^2\sigma_1^2+t_2^2\sigma_2^2+2t_1t_2\rho\sigma_1\sigma_2}{2}\right)\qquad \forall t=(t_1, t_2)\in \bb{R}^2
    \end{align*}
\end{prop}
\begin{proof}
    Sea $t=(t_1, t_2)\in \bb{R}^2$. Calculamos la función generatriz de momentos de $X$:
    \begin{align*}
        M_X(t)&=E\left[e^{t_1X_1+t_2X_2}\right]
        = \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} e^{t_1x_1+t_2x_2}f_X(x_1,x_2)dx_1dx_2
        =\\&= \int_{-\infty}^{+\infty}\int_{-\infty}^{+\infty} e^{t_1x_1}e^{t_2x_2}f_{X_2}(x_2)f_{X_1\mid X_2=x_2}(x_1)dx_1dx_2
        =\\&= \int_{-\infty}^{+\infty} e^{t_2x_2}f_{X_2}(x_2)\left[\int_{-\infty}^{+\infty} e^{t_1x_1}f_{X_1\mid X_2=x_2}(x_1)dx_1\right]dx_2
    \end{align*}
    Como $X_1\mid X_2=x_2\sim \cc{N}\left(\mu_1+\rho\cdot \frac{\sigma_1}{\sigma_2}\cdot (x_2-\mu_2), \sigma_1^2(1-\rho^2)\right)$, usando la función generatriz de momentos de la distribución normal evaluada en $t_1$, tenemos que:
    \begin{align*}
        M_X(t)&= \int_{-\infty}^{+\infty} e^{t_2x_2}f_{X_2}(x_2)M_{X_1\mid X_2=x_2}(t_1)dx_2
        =\\&= \int_{-\infty}^{+\infty} e^{t_2x_2}f_{X_2}(x_2)\exp\left[t_1\left(\mu_1+\rho\cdot \frac{\sigma_1}{\sigma_2}\cdot (x_2-\mu_2)\right)+\dfrac{t_1^2\sigma_1^2(1-\rho^2)}{2}\right]dx_2
        =\\&= \exp\left[t_1\left(\mu_1-\rho\cdot \frac{\sigma_1}{\sigma_2}\cdot \mu_2\right)+\dfrac{t_1^2\sigma_1^2(1-\rho^2)}{2}\right]\int_{-\infty}^{+\infty} e^{t_2x_2}f_{X_2}(x_2)\exp\left[t_1\rho\cdot \frac{\sigma_1}{\sigma_2}\cdot x_2\right]dx_2
        =\\&= \exp\left[t_1\left(\mu_1-\rho\cdot \frac{\sigma_1}{\sigma_2}\cdot \mu_2\right)+\dfrac{t_1^2\sigma_1^2(1-\rho^2)}{2}\right]\int_{-\infty}^{+\infty} \exp\left[\left(t_2+t_1\rho\cdot \frac{\sigma_1}{\sigma_2}\right)x_2\right]f_{X_2}(x_2)dx_2
    \end{align*}

    Como $X_2\sim \cc{N}(\mu_2, \sigma_2^2)$, usando la función generatriz de momentos de la distribución normal evaluada en $t_2+t_1\rho\cdot \frac{\sigma_1}{\sigma_2}$, tenemos que:
    \begin{align*}
        &
        M_X(t)= \exp\left[t_1\left(\mu_1-\rho\cdot \frac{\sigma_1}{\sigma_2}\cdot \mu_2\right)+\dfrac{t_1^2\sigma_1^2(1-\rho^2)}{2}\right]M_{X_2}\left(t_2+t_1\rho\cdot \frac{\sigma_1}{\sigma_2}\right)
        \\&= \exp\left[t_1\left(\mu_1-\rho\cdot \frac{\sigma_1}{\sigma_2}\cdot \mu_2\right)+\dfrac{t_1^2\sigma_1^2(1-\rho^2)}{2}\right]\exp\left[\left(t_2+t_1\rho\cdot \frac{\sigma_1}{\sigma_2}\right)\mu_2+\dfrac{\left(t_2+t_1\rho\cdot \frac{\sigma_1}{\sigma_2}\right)^2\sigma_2^2}{2}\right]
        \\&= \exp\left[t_1\mu_1-\cancel{t_1\rho\cdot \frac{\sigma_1}{\sigma_2}\cdot \mu_2}+\dfrac{t_1^2\sigma_1^2}{2}-\bcancel{\rho^2\cdot \dfrac{t_1^2\sigma_1^2}{2}}+t_2\mu_2+\cancel{t_1\rho\cdot \frac{\sigma_1}{\sigma_2}\cdot \mu_2}+\dfrac{t_2^2\sigma_2^2+\bcancel{t_1^2\rho^2\sigma_1^2}+2t_1t_2\rho\sigma_1\sigma_2}{2}\right]
        \\&= \exp\left[t_1\mu_1+t_2\mu_2+\dfrac{t_1^2\sigma_1^2+t_2^2\sigma_2^2+2t_1t_2\rho\sigma_1\sigma_2}{2}\right]
    \end{align*}

    Calculemos ahora el producto de matrices $t\Sigma t^t$:
    \begin{align*}
        t\Sigma t^t&=\begin{pmatrix}
            t_1 & t_2
        \end{pmatrix}\begin{pmatrix}
            \sigma_1^2 & \rho\sigma_1\sigma_2\\
            \rho\sigma_1\sigma_2 & \sigma_2^2
        \end{pmatrix}\begin{pmatrix}
            t_1\\
            t_2
        \end{pmatrix}
        = \begin{pmatrix}
            t_1 & t_2
        \end{pmatrix}\begin{pmatrix}
            \sigma_1^2t_1+\rho\sigma_1\sigma_2t_2\\
            \rho\sigma_1\sigma_2t_1+\sigma_2^2t_2
        \end{pmatrix}
        =\\&= t_1^2\sigma_1^2+t_2^2\sigma_2^2+2t_1t_2\rho\sigma_1\sigma_2
    \end{align*}
    Llegando así a las dos expresiones de la función generatriz de momentos de la distribución normal bidimensional.
\end{proof}



Una vez estudiadas las marginales y la función generatriz de momentos, estamos en condiciones de ver por qué los parámetros de la distribución normal bidimensional se denominan $\mu_1, \mu_2, \sigma_1^2, \sigma_2^2$ y $\rho$, y por qué se definen las matrices $\mu$ y $\Sigma$ asociadas a estos parámetros. Veamos en primer lugar que, como era de esperar, $\mu$ es el vector esperanza de la distribución normal bidimensional.
\begin{prop}
    Sean los parámetros $\mu_1, \mu_2\in \bb{R}$, $\sigma_1^2, \sigma_2^2\in \bb{R}^+$ y $\rho\in \left]-1,1\right[$, y sean sus matrices $\mu$ y $\Sigma$ asociadas.
    Si $X=(X_1, X_2)\sim \cc{N}_2(\mu, \Sigma)$, entonces:
    \begin{equation*}
        E[X] = \mu = \begin{pmatrix}
            \mu_1 & \mu_2
        \end{pmatrix}
    \end{equation*}
\end{prop}
\begin{proof}
    De la definición de esperanza, tenemos que:
    \begin{equation*}
        E[X]=\begin{pmatrix}
            E[X_1] & E[X_2]
        \end{pmatrix}
    \end{equation*}

    Considerando las marginales, como $X_1\sim \cc{N}(\mu_1, \sigma_1^2)$ y $X_2\sim \cc{N}(\mu_2, \sigma_2^2)$, tenemos que $E[X_1]=\mu_1$ y $E[X_2]=\mu_2$. Por tanto, tenemos que:
    \begin{equation*}
        E[X]=\begin{pmatrix}
            \mu_1 & \mu_2
        \end{pmatrix}=\mu
    \end{equation*}
\end{proof}

Veamos ahora que la matriz $\Sigma$ es la matriz de covarianzas de la distribución normal bidimensional.
\begin{prop}
    Sean los parámetros $\mu_1, \mu_2\in \bb{R}$, $\sigma_1^2, \sigma_2^2\in \bb{R}^+$ y $\rho\in \left]-1,1\right[$, y sean sus matrices $\mu$ y $\Sigma$ asociadas.
    Si $X=(X_1, X_2)\sim \cc{N}_2(\mu, \Sigma)$, entonces:
    \begin{equation*}
        \Cov_X=\Sigma = \begin{pmatrix}
            \sigma_1^2 & \rho\sigma_1\sigma_2\\
            \rho\sigma_1\sigma_2 & \sigma_2^2
        \end{pmatrix}
    \end{equation*}
\end{prop}
\begin{proof}
    De la definición de covarianza, tenemos que:
    \begin{equation*}
        \Cov_X=\begin{pmatrix}
            \Cov(X_1, X_1) & \Cov(X_1, X_2)\\
            \Cov(X_2, X_1) & \Cov(X_2, X_2)
        \end{pmatrix}
    \end{equation*}

    Como la covarianza de una variable consigo misma es su varianza y la covarianza es simétrica, tenemos que:
    \begin{equation*}
        \Cov_X=\begin{pmatrix}
            \Var[X_1] & \Cov(X_1, X_2)\\
            \Cov(X_1, X_2) & \Var[X_2]
        \end{pmatrix}
    \end{equation*}

    Considerando las marginales, como $X_1\sim \cc{N}(\mu_1, \sigma_1^2)$ y $X_2\sim \cc{N}(\mu_2, \sigma_2^2)$, tenemos que $\Var[X_1]=\sigma_1^2$ y $\Var[X_2]=\sigma_2^2$. Para calcular la covarianza de $X_1$ y $X_2$, consideramos la función generatriz de momentos de $X$:
    \begin{align*}
        &E[XY] = \dfrac{\partial^2 M_X(t)}{\partial t_1\partial t_2}\Bigg|_{t_1=t_2=0}
        = \dfrac{\partial^2}{\partial t_1\partial t_2}\exp\left(t_1\mu_1+t_2\mu_2+\dfrac{t_1^2\sigma_1^2+t_2^2\sigma_2^2+2t_1t_2\rho\sigma_1\sigma_2}{2}\right)\Bigg|_{t_1=t_2=0}
        =\\&= \dfrac{\partial}{\partial t_2}\exp\left(t_1\mu_1+t_2\mu_2+\dfrac{t_1^2\sigma_1^2+t_2^2\sigma_2^2+2t_1t_2\rho\sigma_1\sigma_2}{2}\right)\left(\mu_1+t_1\sigma_1^2+\rho\sigma_1\sigma_2t_2\right)\Bigg|_{t_1=t_2=0}
        =\\&= \exp\left(t_1\mu_1+t_2\mu_2+\dfrac{t_1^2\sigma_1^2+t_2^2\sigma_2^2+2t_1t_2\rho\sigma_1\sigma_2}{2}\right)\left(\mu_2+t_2\sigma_2^2+\rho\sigma_1\sigma_2t_1\right)\left(\mu_1+t_1\sigma_1^2+\rho\sigma_1\sigma_2t_2\right)+\\&\qquad +\exp\left(t_1\mu_1+t_2\mu_2+\dfrac{t_1^2\sigma_1^2+t_2^2\sigma_2^2+2t_1t_2\rho\sigma_1\sigma_2}{2}\right)(\rho\sigma_1\sigma_2)\Bigg|_{t_1=t_2=0}
        =\\&= \exp\left(0\right)(\mu_2)(\mu_1)+\exp\left(0\right)(\rho\sigma_1\sigma_2)
        = \mu_1\mu_2+\rho\sigma_1\sigma_2
    \end{align*}

    Por tanto, tenemos que:
    \begin{equation*}
        \Cov (X_1, X_2) = E[X_1X_2]-E[X_1]E[X_2] = \mu_1\mu_2+\rho\sigma_1\sigma_2-\mu_1\mu_2 = \rho\sigma_1\sigma_2
    \end{equation*}

    Por tanto, hemos demostrado que $\Cov_X=\Sigma$.
\end{proof}


Por último, interpretemos el parámetro $\rho$ de la distribución normal bidimensional.
\begin{prop}
    Sean los parámetros $\mu_1, \mu_2\in \bb{R}$, $\sigma_1^2, \sigma_2^2\in \bb{R}^+$ y $\rho\in \left]-1,1\right[$, y sean sus matrices $\mu$ y $\Sigma$ asociadas.
    Si $X=(X_1, X_2)\sim \cc{N}_2(\mu, \Sigma)$, entonces $\rho$ es el coeficiente de correlación de $X_1$ y $X_2$.
    \begin{equation*}
        \rho_{X_1, X_2}=\rho
    \end{equation*}
\end{prop}
\begin{proof}
    De la definición de Coeficiente de Correlación, tenemos que:
    \begin{equation*}
        \rho_{X_1, X_2}=\dfrac{\Cov(X_1, X_2)}{\sqrt{\Var[X_1]\Var[X_2]}}
    \end{equation*}

    Usando que $\Sigma$ es la matriz de covarianzas de $X$, tenemos que:
    \begin{equation*}
        \rho_{X_1, X_2}=\dfrac{\rho\sigma_1\sigma_2}{\sqrt{\sigma_1^2\sigma_2^2}}=\rho
    \end{equation*}
\end{proof}

Por tanto, ya hemos interpretado todos los parámetros de la distribución normal bidimensional. Por tanto, de aquí en adelante no es necesario prefijar $\mu_1, \mu_2, \sigma_1^2, \sigma_2^2$ y $\rho$ para definir la distribución normal bidimensional, sino que podemos definirla directamente con $\mu$ y $\Sigma$, siendo la primera un vector esperanza y la segunda una matriz de covarianzas. De ahí la notación de la distribución normal bidimensional $\cc{N}_2(\mu, \Sigma)$.

Estudiemos ahora las combinaciones lineales de variables aleatorias normales bidimensionales.
\begin{prop}
    Fijado $q\in \{1,2\}$, consideramos la matriz $A\in \cc{M}_{2\times q}(\bb{R})$. Sea también $X=(X_1, X_2)\sim \cc{N}_2(\mu, \Sigma)$. Entonces, tenemos que:
    \begin{equation*}
        Y:=XA\sim \cc{N}_q(\mu A, A^t\Sigma A)
    \end{equation*}
\end{prop}
\begin{proof}
    Como $A\in \cc{M}_{2\times q}(\bb{R})$, tenemos que $XA\in \cc{M}_{1\times q}(\bb{R})$, por lo que $Y$ es un vector aleatorio de dimensión $q$. Dado $t\in \bb{R}^q$, calculamos la función generatriz de momentos de $Y$:
    \begin{align*}
        M_Y(t)&=E\left[e^{Yt^t}\right]
        = E\left[e^{XAt^t}\right]
        = E\left[e^{X(tA^t)^t}\right]
        = M_X(tA^t)
        = \exp\left( tA^t \mu^t+\dfrac{tA^t\Sigma (tA^t)^t}{2}\right)
        =\\&= \exp\left(t(\mu A)^t+\dfrac{tA^t\Sigma A t^t}{2}\right)
    \end{align*}

    Por tanto, tenemos que $Y\sim \cc{N}_q(\mu A, A^t\Sigma A)$ (Nótese que, si $q=1$, también se trata de la función generatriz de momentos de la distribución normal unidimensional, donde usamos el producto escalar en $\bb{R}$).
\end{proof}

Por último, caractericemos la independencia en el caso de la distribución normal bidimensional.
\begin{prop}\label{prop:independencia_normal_bidimensional}
    Sean los parámetros $\mu_1, \mu_2\in \bb{R}$, $\sigma_1^2, \sigma_2^2\in \bb{R}^+$ y $\rho\in \left]-1,1\right[$, y sean sus matrices $\mu$ y $\Sigma$ asociadas.
    Si $X=(X_1, X_2)\sim \cc{N}_2(\mu, \Sigma)$, entonces:
    \begin{equation*}
        X_1\text{ y }X_2\text{ son independientes}\iff \rho=0
    \end{equation*}
\end{prop}
\begin{proof}
    Demostramos mediante doble implicación:
    \begin{description}
        \item[$\Longrightarrow$)] Supongamos que $X_1$ y $X_2$ son independientes. Entonces, vimos en su momento que $\Cov(X_1, X_2)=0$, por lo que $\rho_{X_1, X_2}=0$. Además, como $\rho_{X_1, X_2}=\rho$, tenemos que $\rho=0$.
        
        \item[$\Longleftarrow$)] Supongamos que $\rho=0$. Entonces, la función de densidad de $X$ queda:
        \begin{align*}
            f_X(x_1,x_2)&=\dfrac{1}{2\pi\sigma_1\sigma_2}\exp\left(-\dfrac{1}{2}\left[\dfrac{(x_1-\mu_1)^2}{\sigma_1^2}+\dfrac{(x_2-\mu_2)^2}{\sigma_2^2}\right]\right)
            =\\&= \underbrace{\dfrac{1}{2\pi\sigma_1\sigma_2}\exp\left(-\dfrac{(x_1-\mu_1)^2}{2\sigma_1^2}\right)}_{g_1(x_1)}\underbrace{\exp\left(-\dfrac{(x_2-\mu_2)^2}{2\sigma_2^2}\right)}_{g_2(x_2)}
        \end{align*}
        De esta forma, tenemos que $f_X(x_1,x_2)=g_1(x_1)g_2(x_2)$ para determinadas funciones $g_1$ y $g_2$. Por tanto, $X_1$ y $X_2$ son independientes.
    \end{description}
\end{proof}
Notemos que, como ya hemos visto en otros casos, esta equivalencia no es cierta en general para variables aleatorias normales bidimensionales.