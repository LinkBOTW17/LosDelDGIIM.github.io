\chapter{Independencia de Vectores Aleatorios}

Estudiemos ahora la independencia de vectores aleatorios, que puede recordarnos a indepencia de sucesos en estadística descriptiva.
\begin{definicion}[Independencia de vectores aleatorios]
    Sean $X_1, \ldots, X_n$ variables aleatorias definidas sobre el mismo espacio de probabilidad, con funciones de distribución $F_{X_1}, \ldots, F_{X_n}$ y función de distribución conjunta $F_X$. Se dice que dichas variables son mutuamente independientes (o, simplemente, independientes) si:
    \[
        F_X(x_1, \ldots, x_n) = F_{X_1}(x_1) \cdot \ldots \cdot F_{X_n}(x_n), \quad \forall x_1, \ldots, x_n \in \bb{R}
    \]
\end{definicion}


\section{Caracterizaciones de independencia para variables discretas}

A continuación, veremos dos caracterizaciones de independencia para variables aleatorias discretas.
\begin{prop}[Caracterización mediante funciones de masa de probabilidad]
    Sean $X_1, \ldots, X_n$ variables aleatorias discretas definidas sobre el mismo espacio de probabilidad. Entonces, $X_1, \ldots, X_n$ son independientes si y solo si:
    \[
        P[X_1 = x_1, \ldots, X_n = x_n] = P[X_1 = x_1] \cdot \ldots \cdot P[X_n = x_n], \quad \forall x_1, \ldots, x_n \in \bb{R}
    \]
\end{prop}
% // TODO: Caracterización mediante funciones de masa de probabilidad
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T07_CaracterizacionDiscreta.pdf#[0,{%22name%22:%22Fit%22}]

\begin{ejemplo}
    Consideramos las variables aleatorias $X_1$ y $X_2$ con la siguiente función de masa de probabilidad conjunta:
    \begin{equation*}
        \begin{array}{c|cc|c}
            X_1\backslash X2 & -2 & 1 \\
            \hline
            -1 & \nicefrac{1}{12} & \nicefrac{1}{6}  & \nicefrac{3}{12}= \nicefrac{1}{4} \\
            0 & \nicefrac{1}{6} & \nicefrac{1}{3} & \nicefrac{3}{6} = \nicefrac{1}{2} \\
            1 & \nicefrac{1}{12} & \nicefrac{1}{6} & \nicefrac{3}{12} = \nicefrac{1}{4} \\ \hline
            &  \nicefrac{4}{12}= \nicefrac{1}{3} & \nicefrac{4}{6} = \nicefrac{2}{3} & 1
        \end{array}
    \end{equation*}
    Comprobemos que $X_1$ y $X_2$ son independientes.\\

    Notemos que hemos añadido la última fila y columna para facilitar los cálculos. Para comprobar la independencia, debemos comprobar que la función de masa de probabilidad conjunta es igual al producto de las funciones de masa de probabilidad marginales. Directamente con la tabla, vemos se tiene para todos los casos, aunque vamos a escribirlo explícitamente:
    \begin{align*}
        P[X_1 = -1, X_2 = -2] &= \nicefrac{1}{12} = P[X_1 = -1] \cdot P[X_2 = -2] = \nicefrac{1}{3} \cdot \nicefrac{1}{4} = \nicefrac{1}{12} \\
        P[X_1 = -1, X_2 = 1] &= \nicefrac{1}{6} = P[X_1 = -1] \cdot P[X_2 = 1] = \nicefrac{2}{3} \cdot \nicefrac{1}{4} = \nicefrac{2}{12}=\nicefrac{1}{6} \\
        P[X_1 = 0, X_2 = -2] &= \nicefrac{1}{6} = P[X_1 = 0] \cdot P[X_2 = -2] = \nicefrac{1}{2} \cdot \nicefrac{1}{3} = \nicefrac{1}{6} \\
        P[X_1 = 0, X_2 = 1] &= \nicefrac{1}{3} = P[X_1 = 0] \cdot P[X_2 = 1] = \nicefrac{1}{2} \cdot \nicefrac{2}{3} = \nicefrac{1}{3} \\
        P[X_1 = 1, X_2 = -2] &= \nicefrac{1}{12} = P[X_1 = 1] \cdot P[X_2 = -2] = \nicefrac{1}{4} \cdot \nicefrac{1}{3} = \nicefrac{1}{12} \\
        P[X_1 = 1, X_2 = 1] &= \nicefrac{1}{6} = P[X_1 = 1] \cdot P[X_2 = 1] = \nicefrac{1}{4} \cdot \nicefrac{2}{3} = \nicefrac{1}{6}
    \end{align*}

    Por tanto, $X_1$ y $X_2$ son independientes.
\end{ejemplo}


\begin{prop}[Caracterización mediante factorización de la función masa de probabilidad conjunta]
    Sean $X_1, \ldots, X_n$ variables aleatorias discretas definidas sobre el mismo espacio de probabilidad. Entonces, $X_1, \ldots, X_n$ son independientes si y solo si:
    \[
        P[X_1 = x_1, \ldots, X_n = x_n] = h_1(x_1) \cdot \ldots \cdot h_n(x_n), \quad \forall x_1, \ldots, x_n \in \bb{R}
    \]
    siendo $h_i: \bb{R} \to \bb{R}$ funciones arbitrarias para $i = 1, \ldots, n$.
\end{prop}
\begin{observacion}
    Notemos que la segunda caracterización de independencia para variables discretas es más general que la primera, ya que las funciones $h_i$ pueden ser arbitrarias. Por tanto, no es necesario el cálculo de las funciones masa de probabilidad marginales para comprobar la independencia.
\end{observacion}
% // TODO: Caracterización mediante factorización de la función masa de probabilidad conjunta
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T07_FactorizacionDiscreta.pdf#[0,{%22name%22:%22Fit%22}]

Veamos un ejemplo de esta caracterización, donde la anterior observación entra en juego.
\begin{ejemplo}
    Sea $X=(X_1,X_2)$ un vector aleatorio con función de masa de probabilidad conjunta:
    \begin{equation*}
        P[X_1=x_1,X_2=x_2]=\dfrac{1}{2^{x_1+1}}
        \qquad \forall x_1\in \bb{N},~x_2\in \{0,1\}
    \end{equation*}
    Comprobemos que $X_1$ y $X_2$ son independientes.\\

    \begin{description}
        \item[Cálculo de las funciones masa de probabilidad marginales]
        
        Calculamos ambas marginales:
        \begin{align*}
            P[X_1=x_1] &= \sum_{x_2\in \{0,1\}} P[X_1=x_1,X_2=x_2] = \sum_{x_2\in \{0,1\}} \dfrac{1}{2^{x_1+1}} = \dfrac{1}{2^{x_1+1}}\cdot 2 = \dfrac{1}{2^{x_1}} \\
            P[X_2=x_2] &= \sum_{x_1\in \bb{N}} P[X_1=x_1,X_2=x_2] = \sum_{x_1=1}^{\infty} \dfrac{1}{2^{x_1+1}} 
            =\dfrac{1}{2}\sum_{x_1=1}^{\infty} \dfrac{1}{2^{x_1}}
            =\dfrac{1}{2}\left(-1+\sum_{x_1=0}^{\infty} \dfrac{1}{2^{x_1}}\right)
            =\\&=\dfrac{1}{2}\left(-1+\dfrac{1}{1-\nicefrac{1}{2}}\right)
            = \dfrac{1}{2}\left(-1+2\right) = \dfrac{1}{2}
        \end{align*}

        Por tanto, tenemos que:
        \begin{align*}
            P[X_1=x_1]\cdot P[X_2=x_2] &= \dfrac{1}{2^{x_1}}\cdot \dfrac{1}{2} = \dfrac{1}{2^{x_1+1}} = P[X_1=x_1,X_2=x_2]
        \end{align*}
        Por tanto, $X_1$ y $X_2$ son independientes.

        \item[Factorización de la función masa de probabilidad conjunta]
        
        Consideramos las siguientes funciones auxiliares:
        \Func{h_1}{\bb{R}}{\bb{R}}{x_1}{\begin{cases}
            \dfrac{1}{2^{x_1+1}} & \text{si } x_1\in \bb{N} \\
            0 & \text{en otro caso}
        \end{cases}}
        \Func{h_2}{\bb{R}}{\bb{R}}{x_2}{\begin{cases}
            1 & \text{si } x_2\in \{0,1\} \\
            0 & \text{en otro caso}
        \end{cases}}
    
        Dado $x_1\in \bb{N}$ y $x_2\in \{0,1\}$, se tiene:
        \begin{equation*}
            P[X_1=x_1,X_2=x_2]=\dfrac{1}{2^{x_1+1}}=h_1(x_1)\cdot h_2(x_2)
        \end{equation*}
        Por tanto, $X_1$ y $X_2$ son independientes.
    \end{description}

    Como vemos, la segunda caracterización nos ha permitido comprobar la independencia de $X_1$ y $X_2$ sin necesidad de calcular las funciones masa de probabilidad marginales. 
\end{ejemplo}


\section{Caracterizaciones de independencia para variables continuas}

A continuación, veremos dos caracterizaciones de independencia para variables aleatorias continuas; las cuales serán análogas a las de variables discretas.
\begin{prop}[Caracterización mediante funciones de densidad de probabilidad]
    Sean $X_1, \ldots, X_n$ variables aleatorias continuas definidas sobre el mismo espacio de probabilidad. Entonces, $X_1, \ldots, X_n$ son independientes si y solo si:
    \[
        \left\{
            \begin{array}{l}
                X=(X_1, \ldots, X_n) \text{ es un vector aleatorio continuo} \\
                \quad \land \\
                f_X(x_1, \ldots, x_n) = f_{X_1}(x_1) \cdot \ldots \cdot f_{X_n}(x_n), \quad \forall x_1, \ldots, x_n \in \bb{R}
            \end{array}
        \right.
    \]
\end{prop}
% // TODO: Caracterización mediante funciones de densidad de probabilidad
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T07_CaracterizacionContinuaa.pdf#[0,{%22name%22:%22Fit%22}]

\begin{prop}[Caracterización mediante factorización de la función densidad de probabilidad conjunta]
    Sean $X_1, \ldots, X_n$ variables aleatorias continuas definidas sobre el mismo espacio de probabilidad. Entonces, $X_1, \ldots, X_n$ son independientes si y solo si:
    \[
        \left\{
            \begin{array}{l}
                X=(X_1, \ldots, X_n) \text{ es un vector aleatorio continuo} \\
                \quad \land \\
                f_X(x_1, \ldots, x_n) = h_1(x_1) \cdot \ldots \cdot h_n(x_n), \quad \forall x_1, \ldots, x_n \in \bb{R}
            \end{array}
        \right.
    \]
    siendo $h_i: \bb{R} \to \bb{R}$ funciones arbitrarias para $i = 1, \ldots, n$.
\end{prop}
% // TODO: Caracterización mediante factorización de la función densidad de probabilidad conjunta
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T07_FactorizacionContinua.pdf#[0,{%22name%22:%22Fit%22}]

\begin{observacion}
    De nuevo, consideramos la misma observación que en el caso de variables discretas: la segunda caracterización de independencia para variables continuas es más general que la primera, ya que las funciones $h_i$ pueden ser arbitrarias. Por tanto, no es necesario el cálculo de las funciones densidad de probabilidad marginales para comprobar la independencia.
\end{observacion}


\section{Caracterización mediante conjuntos de Borel}

Esta es la última caracterización de independencia que veremos, la cual es más general que las anteriores. Aunque esta no se usará en la práctica, es interesante verla puesto que nos relaciona directamente con la independencia de sucesos, explicada en Estadística Descriptiva.
\begin{prop}[Caracterización mediante conjuntos de Borel]
    Sean $X_1, \ldots, X_n$ variables aleatorias definidas sobre el mismo espacio de probabilidad. Entonces, $X_1, \ldots, X_n$ son independientes si y solo si:
    \[
        P[X_1 \in B_1, \ldots, X_n \in B_n] = P[X_1 \in B_1] \cdot \ldots \cdot P[X_n \in B_n], \quad \forall B_1, \ldots, B_n \in \mathcal{B}
    \]
\end{prop}
% // TODO: Caracterización mediante conjuntos de Borel
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T07_CaracterizacionBorel.pdf#[0,{%22name%22:%22Fit%22}]


\section{Propiedades de la independencia}

\begin{prop}
    Sea $X=c$ una variable aleatoria degenerada. Entonces $X$ es independiente de cualquier otra variable aleatoria $Y$.
\end{prop}
\begin{proof}
    Sea $X=c$ una variable aleatoria degenerada, y sea $Y$ otra variable aleatoria cualquiera. Entonces, $X$ y $Y$ son independientes, ya que:
    \begin{equation*}
        P[X\leq x] = \begin{cases}
            0 & \text{si } x< c \\
            1 & \text{si } x\geq c
        \end{cases}
        \qquad
        P[X\leq x, Y\leq y] = \begin{cases}
            0 & \text{si } x< c \\
            P[Y\leq y] & \text{si } x\geq c
        \end{cases}
    \end{equation*}

    Por tanto, $P[X\leq x, Y\leq y] = P[X\leq x]P[Y\leq y]$.
\end{proof}

\begin{prop}
    Las variables de cualquier subconjunto de variables independientes son independientes. Es decir, si $X_1, \ldots, X_n$ son independientes, entonces $X_{i_1}, \ldots, X_{i_k}$ son independientes para cualquier subconjunto $\{i_1, \ldots, i_k\}\subset \{1, \ldots, n\}$.
\end{prop}
% // TODO: Subconjunto de variables independientes
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T07_DefinicionIndependencia.pdf#[0,{%22name%22:%22Fit%22}]


\begin{prop}
    Sean $X_1, \ldots, X_n$ variables aleatorias sobre el mismo espacio de probabilidad. Estas son independientes si y solo si las distribuciones condicionadas de cualquier subvector a cualquier otro coinciden con la marginal del primero.
\end{prop}
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T07_PropiedadesIndependencia.pdf#[0,{%22name%22:%22Fit%22}]

\begin{prop}
    Sean $X_1, \ldots, X_n$ variables aleatorias sobre el mismo espacio de probabilidad, y consideramos funciones medibles $g_i: \bb{R} \to \bb{R}$ para $i = 1, \ldots, n$. Entonces:
    \begin{equation*}
        X_1, \ldots, X_n \text{ son independientes} \Longrightarrow g_1(X_1), \ldots, g_n(X_n) \text{ son independientes}
    \end{equation*}
\end{prop}

\begin{prop}[Caracterización por funciones generatrices de momentos]
    Sean $X_1, \ldots, X_n$ variables aleatorias sobre el mismo espacio de probabilidad tales que $\exists M_{X_i}$ en un entorno de $I_i$ de $0$ para todo $i = 1, \ldots, n$ en un entorno de $0$. Entonces, $X_1, \ldots, X_n$ son independientes si y solo si:
    \[
        \left\{
            \begin{array}{l}
                \exists M_X(t_1, \ldots, t_n) \quad \forall (t_1, \ldots, t_n) \in I_1 \times \ldots \times I_n \\
                M_X(t_1, \ldots, t_n) = M_{X_1}(t_1) \cdot \ldots \cdot M_{X_n}(t_n) \quad \forall t_i \in I_i
            \end{array}
        \right.
    \]
\end{prop}
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T07_CaracterizacionFGM.pdf#[0,{%22name%22:%22Fit%22}]


\subsection{Teorema de la multiplicación de las esperanzas}
Incluimos el siguiente teorema. Este es de tal importancia que se le ha dado una sección propia, ya que tiene gran variedad de resultados.
\begin{teo}[Teorema de la Multiplicación de las Esperanzas]
    Sean $X_1, \ldots, X_n$ variables aleatorias sobre el mismo espacio de probabilidad, donde suponemos que $\exists E[X_i]$ para todo $i = 1, \ldots, n$. Entonces:
    \begin{equation*}
        X_1, \ldots, X_n \text{ son independientes} \Longrightarrow \left\{
            \begin{array}{l}
                \exists E[X_1 \cdot \ldots \cdot X_n] \\
                \quad \land \\
                E[X_1 \cdot \ldots \cdot X_n] = E[X_1] \cdot \ldots \cdot E[X_n]
            \end{array}
        \right.
    \end{equation*}
\end{teo}

\begin{coro}
    Sean $X,Y$ dos variables aleatorias sobre el mismo espacio de probabilidad independientes. Entonces, si $\exists \Cov[X,Y]$, se tiene:
    \[
        \Cov[X,Y] = 0
    \]
\end{coro}
\begin{proof}
    Dado que $X$ e $Y$ son independientes, se tiene:
    \[
        \Cov[X,Y] = E[XY] - E[X]E[Y] = E[X]E[Y] - E[X]E[Y] = 0
    \]
\end{proof}

\begin{coro}
    Sean $X$ e $Y$ variables aleatorias independientes. Entonces:
    \begin{equation*}
        \exists  \Var[X\pm Y] = \Var[X] + \Var[Y]
    \end{equation*}
\end{coro}
\begin{proof}
    Tenemos que:
    \begin{align*}
        \Var[X \pm Y] &= E[(X\pm Y)^2] - E[X\pm Y]^2
        =\\&= E[X^2 + Y^2 \pm 2XY] - E[X]^2 - E[Y]^2 \mp 2E[X]E[Y]
        =\\&= E[X^2] - E[X]^2 + E[Y^2] -E[Y]^2 \pm 2E[XY] \mp 2E[X]E[Y]
        \AstIg\\&\AstIg \Var[X] + \Var[Y]
    \end{align*}
    donde en $(\ast)$ hemos usado el teorema anterior.
\end{proof}

\begin{coro}
    Sean $X_1, \ldots, X_n$ variables aleatorias sobre el mismo espacio de probabilidad independientes. Entonces, si $\exists E[X_i^2]$ para todo $i = 1, \ldots, n$, se tiene:
    \[
        \Var\left[\sum_{i=1}^n a_i X_i\right] = \sum_{i=1}^n a_i^2\Var[X_i] \quad \forall a_1, \ldots, a_n \in \bb{R}
    \]
\end{coro}
\begin{proof}
    \begin{equation*}
        Var\left[\sum_{i=1}^n a_iX_i\right]
        \AstIg \sum_{i=1}^n \Var[a_iX_i]
        = \sum_{i=1}^n a_i^2 \Var[X_i]
    \end{equation*}
    donde en $(\ast)$ hemos usado el resultado anterior.
\end{proof}


\begin{coro}\label{coro:generatriz_momentos_independientes}
    Sean $X_1,X_2,\ldots,X_n$ variables aleatorias independientes con función generatriz de momentos $M_{X_i}(t)$ para todo $i=1,\dots,n$. Entonces, la función generatriz de momentos de la variable aleatoria $X=\sum\limits_{i=1}^n X_i$ es:
    \begin{equation*}
        M_X(t)=\prod_{i=1}^n M_{X_i}(t)
    \end{equation*}
\end{coro}
\begin{proof}
    \begin{equation*}\begin{split}
        M_X(t)
        &= E[e^{tX}]
        = E\left[e^{t\sum\limits_{i=1}^n X_i}\right]
        = E\left[\prod_{i=1}^n e^{tX_i}\right]
        \AstIg \prod_{i=1}^n E[e^{tX_i}]
        = \prod_{i=1}^n M_{X_i}(t)
    \end{split}\end{equation*}
    donde en $(\ast)$ hemos empleado el teorema de la multiplicación de las esperanzas.
\end{proof}


\section{Distribuciones Reproductivas}

\begin{definicion}[Reproductividad de distribuciones]
    Una distribución de variables aleatorias se dice reproductiva si, dadas $X_1$, $X_2$, \ldots, $X_n$ variables aleatorias que sigan una distribución de dicho tipo (aunque la distribución de cada variable tenga distintos parámetros), entonces la variable aleatoria dada por:
    \begin{equation*}
        X = \sum_{i=1}^{n} X_i
    \end{equation*}
    sigue una distribución del mismo tipo.
\end{definicion}

\begin{observacion}
    Notemos que, para ver que una distribución es reproductiva, usaremos el Corolario~\ref{coro:generatriz_momentos_independientes} para, dadas $n$ variables aleatorias $\{X_i\}_{i \in \{1,\ldots,n\}}$ que sigan una distribución de dicho tipo, calcular la función generatriz de momentos de la variable aleatoria $X=\sum\limits_{i=1}^n X_i$ y, como $M_X(t)$ caracteriza la distribución de $X$, calculando $M_X(t)$ sabremos ya si $X$ sigue una distribución del mismo tipo que las variables aleatorias $X_i$.
\end{observacion}

Veamos en primer lugar algunas distribuciones reproductivas para variables discretas.

\begin{prop}[Distribución Reproductiva - Binomial]
    Sean $X_1, \dots, X_n$ variables aleatorias independientes y $X_i\sim B(k_i, p)$ para $i=1,\dots,n$. Entonces:
    \begin{equation*}
        \sum_{i=1}^{n}X_i \sim B\left(\sum_{i=1}^{n}k_i, p\right)
    \end{equation*}

    \begin{proof}
        Sabemos que la función generatriz de momentos de cada una de las variables aleatorias $X_i$ es:
        \begin{equation*}
            M_{X_i}(t) = (1-p+pe^t)^{k_i} \qquad i=1,\dots,n
        \end{equation*}

        Usando el Corolario \ref{coro:generatriz_momentos_independientes}, tenemos que:
        \begin{align*}
            M_{\sum\limits_{i=1}^{n}X_i}(t) &= \prod_{i=1}^{n}M_{X_i}(t) = \prod_{i=1}^{n}(1-p+pe^t)^{k_i} = (1-p+pe^t)^{\sum\limits_{i=1}^{n}k_i}
        \end{align*}

        Esta es la función generatriz de momentos de una variable aleatoria que sigue una distribución binomial con parámetros $\left(\sum\limits_{i=1}^{n}k_i, p\right)$. Por tanto, hemos demostrado que:
        \begin{equation*}
            \sum_{i=1}^{n}X_i \sim B\left(\sum_{i=1}^{n}k_i, p\right)
        \end{equation*}
    \end{proof}
\end{prop}

\begin{prop}[Distribución Reproductiva - Poisson]
    Sean $X_1, \dots, X_n$ variables aleatorias independientes y $X_i\sim \cc{P}(\lambda_i)$ para $i=1,\dots,n$. Entonces:
    \begin{equation*}
        \sum_{i=1}^{n}X_i \sim \cc{P}\left(\sum_{i=1}^{n}\lambda_i\right)
    \end{equation*}

    \begin{proof}
        Sabemos que la función generatriz de momentos de cada una de las variables aleatorias $X_i$ es:
        \begin{equation*}
            M_{X_i}(t) = e^{\lambda_i(e^t-1)} \qquad i=1,\dots,n
        \end{equation*}

        Usando el Corolario \ref{coro:generatriz_momentos_independientes}, tenemos que:
        \begin{align*}
            M_{\sum\limits_{i=1}^{n}X_i}(t) &= \prod_{i=1}^{n}M_{X_i}(t) = \prod_{i=1}^{n}e^{\lambda_i(e^t-1)} = e^{\left(\sum\limits_{i=1}^{n}\lambda_i\right)(e^t-1)}
        \end{align*}

        Esta es la función generatriz de momentos de una variable aleatoria que sigue una distribución de Poisson con parámetro $\left(\sum\limits_{i=1}^{n}\lambda_i\right)$. Por tanto, hemos demostrado que:
        \begin{equation*}
            \sum_{i=1}^{n}X_i \sim \cc{P}\left(\sum_{i=1}^{n}\lambda_i\right)
        \end{equation*}
    \end{proof}
\end{prop}

\begin{prop}[Distribución Reproductiva - Binomial Negativa]
    Sean $X_1, \dots, X_n$ variables aleatorias independientes y $X_i\sim BN(k_i, p)$ para $i=1,\dots,n$. Entonces:
    \begin{equation*}
        \sum_{i=1}^{n}X_i \sim BN\left(\sum_{i=1}^{n}k_i, p\right)
    \end{equation*}

    \begin{proof}
        Sabemos que la función generatriz de momentos de cada una de las variables aleatorias $X_i$ es:
        \begin{equation*}
            M_{X_i}(t) = \left(\dfrac{p}{1-(1-p)e^t}\right)^{k_i} \qquad i=1,\dots,n
        \end{equation*}

        Usando el Corolario \ref{coro:generatriz_momentos_independientes}, tenemos que:
        \begin{align*}
            M_{\sum\limits_{i=1}^{n}X_i}(t) &= \prod_{i=1}^{n}M_{X_i}(t) = \prod_{i=1}^{n}\left(\dfrac{p}{1-(1-p)e^t}\right)^{k_i} = \left(\dfrac{p}{1-(1-p)e^t}\right)^{\sum\limits_{i=1}^{n}k_i}
        \end{align*}

        Esta es la función generatriz de momentos de una variable aleatoria que sigue una distribución binomial negativa con parámetros $\left(\sum\limits_{i=1}^{n}k_i, p\right)$. Por tanto, hemos demostrado que:
        \begin{equation*}
            \sum_{i=1}^{n}X_i \sim BN\left(\sum_{i=1}^{n}k_i, p\right)
        \end{equation*}
    \end{proof}
\end{prop}


Veamos el siguiente caso, último para distribuciones discretas. Aunque no es una familia de variables reproductivas, cumple la siguiente propiedad.
\begin{prop}
    Sean $X_1, \dots, X_n$ variables aleatorias independientes y $X_i\sim G(p)$ para $i=1,\dots,n$. Entonces:
    \begin{equation*}
        \sum_{i=1}^{n}X_i \sim BN(n,p)
    \end{equation*}
    \begin{proof}
        Sabemos que la función generatriz de momentos de cada una de las variables aleatorias $X_i$ es:
        \begin{equation*}
            M_{X_i}(t) = \dfrac{p}{1-(1-p)e^t} \qquad i=1,\dots,n
        \end{equation*}

        Usando el Corolario \ref{coro:generatriz_momentos_independientes}, tenemos que:
        \begin{align*}
            M_{\sum\limits_{i=1}^{n}X_i}(t) &= \prod_{i=1}^{n}M_{X_i}(t) = \prod_{i=1}^{n}\dfrac{p}{1-(1-p)e^t} = \dfrac{p^n}{(1-(1-p)e^t)^n}
        \end{align*}

        Esta es la función generatriz de momentos de una variable aleatoria que sigue una distribución binomial negativa con parámetros $BN(n,p)$. Por tanto, hemos demostrado que:
        \begin{equation*}
            \sum_{i=1}^{n}X_i \sim BN(n,p)
        \end{equation*}
    \end{proof}
\end{prop}

Veamos ahora algunas distribuciones reproductivas para variables continuas.

\begin{prop}[Distribución Reproductiva - Normal]
    Sean $X_1, \dots, X_n$ variables aleatorias independientes y $X_i\sim N(\mu_i, \sigma_i^2)$ para $i=1,\dots,n$. Entonces:
    \begin{equation*}
        \sum_{i=1}^{n}X_i \sim N\left(\sum_{i=1}^{n}\mu_i, \sum_{i=1}^{n}\sigma_i^2\right)
    \end{equation*}

    \begin{proof}
        Sabemos que la función generatriz de momentos de cada una de las variables aleatorias $X_i$ es:
        \begin{equation*}
            M_{X_i}(t) = e^{\mu_i t + \frac{\sigma_i^2 t^2}{2}} \qquad i=1,\dots,n
        \end{equation*}

        Usando el Corolario \ref{coro:generatriz_momentos_independientes}, tenemos que:
        \begin{align*}
            M_{\sum\limits_{i=1}^{n}X_i}(t) &= \prod_{i=1}^{n}M_{X_i}(t) = \prod_{i=1}^{n}e^{\mu_i t + \frac{\sigma_i^2 t^2}{2}} = e^{\left(\sum\limits_{i=1}^{n}\mu_i\right)t + \frac{\left(\sum\limits_{i=1}^{n}\sigma_i^2\right)t^2}{2}}
        \end{align*}

        Esta es la función generatriz de momentos de una variable aleatoria que sigue una distribución normal con parámetros $\left(\sum\limits_{i=1}^{n}\mu_i, \sum\limits_{i=1}^{n}\sigma_i^2\right)$. Por tanto, hemos demostrado que:
        \begin{equation*}
            \sum_{i=1}^{n}X_i \sim N\left(\sum_{i=1}^{n}\mu_i, \sum_{i=1}^{n}\sigma_i
            ^2\right)
        \end{equation*}
    \end{proof}
\end{prop}

Al igual que ocurría en el caso de la distribución geométrica, la distribución exponencial no es reproductiva. Sin embargo, cumple la siguiente propiedad.
\begin{prop}
    Sean $X_1, \dots, X_n$ variables aleatorias independientes tal que $X_i\sim \exp(\lambda)$ para $i=1,\dots,n$. Entonces:
    \begin{equation*}
        \sum_{i=1}^{n}X_i \sim \cc{E}\left(n,\lm\right)
    \end{equation*}

    \begin{proof}
        Sabemos que la función generatriz de momentos de cada una de las variables aleatorias $X_i$ es:
        \begin{equation*}
            M_{X_i}(t) = \dfrac{\lambda}{\lambda-t} \qquad i=1,\dots,n
        \end{equation*}
    
        Usando el Corolario \ref{coro:generatriz_momentos_independientes}, tenemos que:
        \begin{align*}
            M_{\sum\limits_{i=1}^{n}X_i}(t) &= \prod_{i=1}^{n}M_{X_i}(t) = \prod_{i=1}^{n}\dfrac{\lambda}{\lambda-t} = \dfrac{\lambda^n}{(\lambda-t)^n}
        \end{align*}

        Esta es la función generatriz de momentos de una variable aleatoria que sigue una distribución de Erlang con parámetros $n,\lm$. Por tanto, hemos demostrado que:
        \begin{equation*}
            \sum_{i=1}^{n}X_i \sim \cc{E}\left(n,\lm\right)
        \end{equation*}
    \end{proof}
\end{prop}

\begin{prop}[Distribución Reproductiva - Erlang]
    Sean $X_1, \dots, X_n$ variables aleatorias independientes tal que $X_i\sim \cc{E}(k_i,\lambda)$ para $i=1,\dots,n$. Entonces:
    \begin{equation*}
        \sum_{i=1}^{n}X_i \sim \cc{E}\left(\sum_{i=1}^{n}k_i,\lambda\right)
    \end{equation*}

    \begin{proof}
        Sabemos que la función generatriz de momentos de cada una de las variables aleatorias $X_i$ es:
        \begin{equation*}
            M_{X_i}(t) = \dfrac{\lambda^{k_i}}{(\lambda-t)^{k_i}} \qquad i=1,\dots,n
        \end{equation*}
    
        Usando el Corolario \ref{coro:generatriz_momentos_independientes}, tenemos que:
        \begin{align*}
            M_{\sum\limits_{i=1}^{n}X_i}(t) &= \prod_{i=1}^{n}M_{X_i}(t) = \prod_{i=1}^{n}\dfrac{\lambda^{k_i}}{(\lambda-t)^{k_i}} = \dfrac{\lambda^{\sum\limits_{i=1}^{n}k_i}}{(\lambda-t)^{\sum\limits_{i=1}^{n}k_i}}
        \end{align*}

        Esta es la función generatriz de momentos de una variable aleatoria que sigue una distribución de Erlang con parámetros $\left(\sum\limits_{i=1}^{n}k_i,\lambda\right)$. Por tanto, hemos demostrado que:
        \begin{equation*}
            \sum_{i=1}^{n}X_i \sim \cc{E}\left(\sum_{i=1}^{n}k_i,\lambda\right)
        \end{equation*}
    \end{proof}
\end{prop}

\begin{prop}[Distribución Reproductiva - Gamma]
    Sean $X_1, \dots, X_n$ variables aleatorias independientes tal que $X_i\sim \Gamma(u_i,\lambda)$ para $i=1,\dots,n$. Entonces:
    \begin{equation*}
        \sum_{i=1}^{n}X_i \sim \Gamma\left(\sum_{i=1}^{n}u_i,\lambda\right)
    \end{equation*}

    \begin{proof}
        Sabemos que la función generatriz de momentos de cada una de las variables aleatorias $X_i$ es:
        \begin{equation*}
            M_{X_i}(t) = \dfrac{\lambda^{u_i}}{(\lambda-t)^{u_i}} \qquad i=1,\dots,n
        \end{equation*}
    
        Usando el Corolario \ref{coro:generatriz_momentos_independientes}, tenemos que:
        \begin{align*}
            M_{\sum\limits_{i=1}^{n}X_i}(t) &= \prod_{i=1}^{n}M_{X_i}(t) = \prod_{i=1}^{n}\dfrac{\lambda^{u_i}}{(\lambda-t)^{u_i}} = \dfrac{\lambda^{\sum\limits_{i=1}^{n}u_i}}{(\lambda-t)^{\sum\limits_{i=1}^{n}u_i}}
        \end{align*}

        Esta es la función generatriz de momentos de una variable aleatoria que sigue una distribución gamma con parámetros $\left(\sum\limits_{i=1}^{n}u_i,\lambda\right)$. Por tanto, hemos demostrado que:
        \begin{equation*}
            \sum_{i=1}^{n}X_i \sim \Gamma\left(\sum_{i=1}^{n}u_i,\lambda\right)
        \end{equation*}
    \end{proof}
\end{prop}


\section{Independencia para familias de variables aleatorias}

Notemos que las definiciones de independencia que hemos dado hasta ahora son para un número finito de variables aleatorias. Sin embargo, podemos extender estas definiciones a familias de variables aleatorias infinitas numerables.
\begin{definicion}[Independencia mutua]
    Sea $\{X_i\}_{i\in T}$ una familia de variables aleatorias, con $T$ infinito numerable. Diremos que la familia $\{X_i\}_{i\in T}$, es independiente si, para todo subconjunto finito $T_0\subset T$, las variables aleatorias $\{X_i\}_{i\in T_0}$ son independientes.
\end{definicion}

\begin{definicion}[Independencia dos a dos]
    Sea $\{X_i\}_{i\in T}$ una familia de variables aleatorias, con $T$ infinito numerable. Diremos que la familia $\{X_i\}_{i\in T}$, es independiente dos a dos si, para todo par de índices $i,j\in T$, $i\neq j$, las variables aleatorias $X_i$ y $X_j$ son independientes.
\end{definicion}

Es directo ver que la independencia mutua implica la independencia dos a dos. Sin embargo, la independencia dos a dos no implica la independencia mutua. Por tanto, la independencia mutua es una propiedad más fuerte que la independencia dos a dos.

\section{Independencia para vectores aleatorios}

Hasta ahora, hemos visto la independencia para variables aleatorias unidimensionales. Sin embargo, podemos extender estas definiciones a vectores aleatorios multidimensionales.
\begin{definicion}[Independencia para vectores aleatorios]
    Sean $X^{(1)},\dots,X^{(n)}$ vectores aleatorios de dimensión $d_1,\dots,d_n$ respectivamente. Sea $F_X^{(i)}$ la función de distribución de $X^{(i)}$ para $i=1,\dots,n$, y $F_X$ la función de distribución conjunta de $X=(X^{(1)},\dots,X^{(n)})$. Diremos que los vectores aleatorios $X^{(1)},\dots,X^{(n)}$ son independientes si:
    \begin{equation*}
        F_X(x^{(1)},\dots,x^{(n)}) = F_X^{(1)}(x^{(1)})\cdot\dots\cdot F_X^{(n)}(x^{(n)}) \quad \forall x^{(1)}\in\bb{R}^{d_1},\dots,x^{(n)}\in\bb{R}^{d_n}
    \end{equation*}
\end{definicion}
