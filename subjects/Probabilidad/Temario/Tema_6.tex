\chapter{Teorema Central del Límite}

El objetivo de este tema es dar una solución aproximada al problema de encontrar la distribución de probabilidad de las sumas parciales de una sucesión de variables aleatorias. Es decir, dado $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias definidas sobre el mismo espacio de probabilidad $(\Omega, \mathcal{A}, P)$, se busca encontrar la distribución de probabilidad de la variable aleatoria:
\begin{equation*}
    S_n = \sum_{i=1}^{n} X_i
\end{equation*}

Como se trata de sumas parciales (no infinitas), se trata de una transformación medible luego $S_n$ efectivamente también es una variable aleatoria para todo $n\in \bb{N}$. Introducimos los siguientes tipos de convergencia.
\begin{definicion}[Convergencia Casi Segura]
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias definidas sobre el mismo espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Decimos que la sucesión converge casi seguramente a una variable aleatoria $X$, que notaremos por $\{X_n\} \xrightarrow{c.s.} X$, si:
    \begin{equation*}
        P\left[\lim_{n\to \infty} X_n = X\right] = 1
    \end{equation*}
    Análogamente, usando la probabilidad en $(\Omega, \mathcal{A}, P)$, se tiene que:
    \begin{equation*}
        P\left[\omega\in \Omega\mid \lim_{n\to \infty} X_n(\omega) = X(\omega)\right] = 1
    \end{equation*}
\end{definicion}
Notemos que, si $\{X_n\} \xrightarrow{c.s.} X$, entonces el conjunto de puntos de convergencia de $X_n$ tiene probabilidad $1$. Se tiene además el siguiente lema.
\begin{lema}\label{lema:suma_convergencia_cs}
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias definidas sobre el mismo espacio de probabilidad $(\Omega, \mathcal{A}, P)$ y $X$ una variable aleatoria. Entonces:
    \begin{equation*}
        \{X_n\} \xrightarrow{c.s.} X \Longrightarrow \{X_n+ c\} \xrightarrow{c.s} X + c \qquad \forall c\in \bb{R}
    \end{equation*}
\end{lema}
\begin{proof}
    Tenemos que:
    \begin{align*}
        1 &= P\left[\omega\in \Omega\mid \lim_{n\to \infty} X_n(\omega) = X(\omega)\right]
        =\\&= P\left[\omega\in \Omega\mid \lim_{n\to \infty} X_n(\omega) +c = X(\omega)+c\right]
        =\\&= P\left[\omega\in \Omega\mid \lim_{n\to \infty} (X_n+c)(\omega) = (X+c)(\omega)\right]
    \end{align*}

    Por tanto, se tiene que $\{X_n+ c\} \xrightarrow{c.s.} X + c$.
\end{proof}

\begin{definicion}[Convergencia en Probabilidad]
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias definidas sobre el mismo espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Decimos que la sucesión converge en probabilidad a una variable aleatoria $X$, notado por $\{X_n\} \xrightarrow{P} X$, si:
    \begin{equation*}
        \forall \veps \in \bb{R}^+,\qquad \lim_{n\to \infty} P\left[|X_n - X| \leq \veps\right] = 1
    \end{equation*}
\end{definicion}
Notemos que, si $\{X_n\} \xrightarrow{P} X$, entonces la probabilidad de que la diferencia entre $X_n$ y $X$ sea menor que cualquier $\veps$ tiende a $1$. Es decir, para cierto $n\in \bb{N}$ suficientemente grande, la probabilidad de que $X_n$ esté cerca de $X$ es muy alta. De la definición, se obtiene de forma directa el siguiente lema simplemente aplicando la definición de convergencia en probabilidad.
\begin{lema}\label{lema:suma_convergencia_probabilidad}
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias definidas sobre el mismo espacio de probabilidad $(\Omega, \mathcal{A}, P)$ y $X$ una variable aleatoria. Entonces:
    \begin{equation*}
        \{X_n\} \xrightarrow{P} X \Longrightarrow \{X_n+ c\} \xrightarrow{P} X + c \qquad \forall c\in \bb{R}
    \end{equation*}
\end{lema}
\begin{proof}
    Fijado $\veps \in \bb{R}^+$, se tiene que:
    \begin{align*}
        P\left[|X_n - X| \leq \veps\right]
        &= P\left[|X_n + c - X-c| \leq \veps\right]
        =\\&= P\left[|(X_n + c) - (X+c)| \leq \veps\right]
    \end{align*}

    Por tanto, si $\{X_n\} \xrightarrow{P} X$, entonces $\{X_n+ c\} \xrightarrow{P} X + c$.
\end{proof}

\begin{definicion}[Convergencia en Ley o en Distribución]
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias definidas sobre el mismo espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Sean $F_{X_n}$ la función de distribución de $X_n$ para cada $n\in \bb{N}$. Decimos que la sucesión converge en ley a una variable aleatoria $X$ con función de distribución $F_X$, notado por $\{X_n\} \xrightarrow{L} X$, si:
    \begin{equation*}
        \lim_{n\to \infty} F_{X_n}(x) = F_X(x) \qquad \forall x\in C(F_X)
    \end{equation*}    
    donde $C(F_X)$ es el conjunto de puntos de continuidad de $F_X$.
\end{definicion}

Veamos ahora la relación que hay entre estas tres formas de convergencia.
\begin{prop}
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias definidas sobre el mismo espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Entonces:
    \begin{equation*}
        \{X_n\} \xrightarrow{c.s.} X \Longrightarrow \{X_n\} \xrightarrow{P} X \Longrightarrow \{X_n\} \xrightarrow{L} X
    \end{equation*}
\end{prop}
% // TODO: Demostrar

Introducimos ahora la siguiente proposición, que nos da una condición suficiente para la convergencia en Ley, que será necesaria más adelante.
\begin{prop}
    Sean $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias definidas sobre el mismo espacio de probabilidad $(\Omega, \mathcal{A}, P)$ y $X$ una variable aleatoria definida sobre el mismo espacio de probabilidad. Supongamos además que existen las funciones generatrices de momentos $M_{X_n}, M_X$ de $X_n, X$ respectivamente en un entorno $N$ del origen. Se cumple que:
    \begin{equation*}
        \lim_{n\to \infty} M_{X_n}(t) = M_X(t) \qquad \forall t\in N
        \Longrightarrow \{X_n\} \xrightarrow{L} X
    \end{equation*}
\end{prop}
% // TODO: Demostrar. La tiene Carlos. ¿Condición necesaria?

En las siguientes secciones, trataremos condiciones suficientes o necesarias relativas a:
\begin{itemize}
    \item Convergencia en Ley (Teorema Central del Límite).
    \item Convergencia en Probabilidad (Ley Débil de los Grandes Números).
    \item Convergencia Casi Segura (Ley Fuerte de los Grandes Números).
\end{itemize}

\section{Leyes de los Grandes Números}

Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias \emph{independientes} definidas sobre el mismo espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Adicionalmente, sea $\{S_n\}_{n\in \bb{N}}$ la variable aleatoria definida como las sumas parciales de la sucesión:
\begin{equation*}
    S_n = \sum_{i=1}^{n} X_i\qquad \forall n\in \bb{N}
\end{equation*}

Se consideran dos sucesiones de números reales $\{a_n\}_{n\in \bb{N}}, \{b_n\}_{n\in \bb{N}}$ tales que $\{b_n\}\nearrow
+\infty$ (es decir, es creciente y diverge positivamente). Se definen entonces las siguientes Leyes.
\begin{definicion}[Ley Débil de los Grandes Números]
    En las condiciones anteriores, se dice que $\{X_n\}$ satisface la Ley Débil de los Grandes Números respecto a $\{a_n\}, \{b_n\}$ si:
    \begin{equation*}
        \left\{\frac{S_n - a_n}{b_n}\right\} \xrightarrow{P} 0
    \end{equation*}
\end{definicion}

\begin{definicion}[Ley Fuerte de los Grandes Números]
    En las condiciones anteriores, se dice que $\{X_n\}$ satisface la Ley Fuerte de los Grandes Números respecto a $\{a_n\}, \{b_n\}$ si:
    \begin{equation*}
        \left\{\frac{S_n - a_n}{b_n}\right\} \xrightarrow{c.s.} 0
    \end{equation*}
\end{definicion}

Notemos que, el nombre de convergencia \emph{débil} y \emph{fuerte} se debe a que, como la convergencia casi segura implica la convergencia en probabilidad, si se cumple la Ley Fuerte, entonces también se cumple la Ley Débil.\\

A continuación se formulan resultados que proporcionan condiciones suficientes o condiciones necesarias y suficientes para que una sucesión de variables aleatorias $\{X_n\}_{n\in \bb{N}}$ satisfaga las leyes de los grandes números, aunque antes se introducirá el siguiente resultado, muy útil por ser una caracterización de las hipótesis de la mayor parte de los resultados del tema.
\begin{prop}
    Sean $X_1,\dots,X_n$ variables aleatorias definidas sobre el mismo espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Entonces, se tiene que $X_1,\dots,X_n$ son independientes e idénticamente distribuidas si y solo si:
    \begin{equation*}
        F_{(X_1,\dots,X_n)}(x_1,\dots,x_n) = \prod_{i=1}^{n}F_{X_k}(x_i) \qquad \forall x_1,\dots,x_n\in \bb{R}, k\in \{1,\dots,n\}
    \end{equation*}
\end{prop}
\begin{proof}
    Demostraremos por doble implicación:
    \begin{description}
        \item[$\Longrightarrow$)] Supongamos que $X_1,\dots,X_n$ son independientes e idénticamente distribuidas. Entonces, por ser independientes, tenemos que:
        \begin{equation*}
            F_{(X_1,\dots,X_n)}(x_1,\dots,x_n) = F_{X_1}(x_1)\cdots F_{X_n}(x_n) \qquad \forall x_1,\dots,x_n\in \bb{R}
        \end{equation*}

        Además, por ser idénticamente distribuidas, se tiene que:
        \begin{equation*}
            F_{X_1}(x) = \cdots = F_{X_n}(x) \qquad \forall x\in \bb{R}
        \end{equation*}

        Por tanto, podemos escoger cualquier $k\in \{1,\dots,n\}$ y se tiene que:
        \begin{equation*}
            F_{(X_1,\dots,X_n)}(x_1,\dots,x_n) = \prod_{i=1}^{n}F_{X_k}(x_i) \qquad \forall x_1,\dots,x_n\in \bb{R}, k\in \{1,\dots,n\}
        \end{equation*}

        \item[$\Longleftarrow$)] Supongamos que se cumple que:
        \begin{equation*}
            F_{(X_1,\dots,X_n)}(x_1,\dots,x_n) = \prod_{i=1}^{n}F_{X_k}(x_i) \qquad \forall x_1,\dots,x_n\in \bb{R}, k\in \{1,\dots,n\}
        \end{equation*}

        Fijado $x\in \bb{R}$, y tomando $x_1=x_2=\dots=x_n=x$, se tiene que:
        \begin{equation*}
            (F_{X_1}(x))^n = (F_{X_2}(x))^n = \cdots = (F_{X_n}(x))^n
        \end{equation*}

        Tomando la raíz $n-$ésima, como la función de distribución es no-negativa, se tiene que:
        \begin{equation*}
            F_{X_1}(x) = F_{X_2}(x) = \cdots = F_{X_n}(x)\qquad \forall x\in \bb{R}
        \end{equation*}
        Por tanto, las variables aleatorias $X_1,\dots,X_n$ son idénticamente distribuidas.\\

        Sabiendo esto, tenemos que:
        \begin{equation*}
            F_{(X_1,\dots,X_n)}(x_1,\dots,x_n) = \prod_{i=1}^{n}F_{X_k}(x_i) = \prod_{i=1}^{n}F_{X_i}(x_i) \qquad \forall x_1,\dots,x_n\in \bb{R}
        \end{equation*}

        Por tanto, también tenemos que las variables aleatorias $X_1,\dots,X_n$ son independientes.
    \end{description}
\end{proof}

\subsection{Leyes Débiles de los Grandes Números}
\begin{prop}[Ley Débil de los Grandes Números de Khintchine, 1928]
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias independientes e idénticamente distribuidas tal que $\exists \mu\in \bb{R}$ de forma que $E[X_n]=\mu~\forall n\in \bb{N}$. Entonces, se cumple que:
    \begin{equation*}
        \left\{\frac{S_n-E[S_n]}{n}\right\} \xrightarrow{P} 0
    \end{equation*}
\end{prop}
% // TODO: Demostración
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T10_LeyKhintchine.pdf#[0,{%22name%22:%22Fit%22}]

De forma directa, por el Lema \ref{lema:suma_convergencia_probabilidad}, se tiene el siguiente corolario.
\begin{coro}
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias independientes e idénticamente distribuidas tal que $\exists \mu\in \bb{R}$ de forma que $E[X_n]=\mu$ para algún $n\in \bb{N}$. Entonces, se cumple que:
    \begin{equation*}
        \left\{\frac{S_n}{n}\right\} \xrightarrow{P} \mu
    \end{equation*}
\end{coro}
\begin{proof}
    Tenemos que:
    \begin{equation*}
        E[S_n] = E\left[\sum_{i=1}^{n} X_i\right] = \sum_{i=1}^{n} E[X_i] = \sum_{i=1}^{n} \mu = n\mu
    \end{equation*}
    donde hemos usado que $E[X_i]=\mu~\forall i\in \{1,\ldots,n\}$. Por tanto, aplicando el Lema \ref{lema:suma_convergencia_probabilidad}, se tiene que:
    \begin{equation*}
        \left\{\frac{S_n-E[S_n]}{n}\right\} \xrightarrow{P} 0 \Longrightarrow 
        \left\{\frac{S_n-E[S_n]}{n}+\frac{E[S_n]}{n}\right\} \xrightarrow{P} 0+\frac{E[S_n]}{n}=\frac{n\mu}{n}=\mu
    \end{equation*}
\end{proof}
\begin{observacion}
    Notemos que, como las variables aleatorias $X_n$ son independientes e idénticamente distribuidas, se tiene que la condición de que $\exists \mu\in \bb{R}$ de forma que $E[X_n]=\mu~\forall n\in \bb{N}$ es equivalente a que $\exists \mu\in \bb{R}$ de forma que $E[X_n]=\mu$ para algún $n\in \bb{N}$, ya que:
    \begin{equation*}
        E[X_i]=E[X_j]\qquad \forall i,j\in \bb{N}
    \end{equation*}
\end{observacion}

Históricamente, es relevante el caso en el que la distribución de las variables aleatorias $X_n$ es de Bernoulli, ya que este se descubrió y demostró dos siglos antes. Se introduce entonces como corolario, pero notemos que es un caso particular de los anteriores usando que $X_n\sim B(p)$ y $E[X_n]=p$ para todo $n\in \bb{N}$.

\begin{coro}[Ley Débil de los Grandes Números de Bernoulli, 1713]
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias independientes e idénticamente distribuidas con distribución de Bernoulli de parámetro $p\in \left]0,1\right[$. Es decir, $X_n\sim B(p)~\forall n\in \bb{N}$. Entonces, se cumple que:
    \begin{equation*}
        \left\{\frac{S_n-E[S_n]}{n}\right\} \xrightarrow{P} 0
    \end{equation*}
\end{coro}
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T10_LeyBernoulli.pdf#[0,{%22name%22:%22Fit%22}]
\begin{coro}
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias independientes e idénticamente distribuidas con distribución de Bernoulli de parámetro $p\in \left]0,1\right[$. Entonces, se cumple que:
    \begin{equation*}
        \left\{\frac{S_n}{n}\right\} \xrightarrow{P} p
    \end{equation*}
\end{coro}


\subsection{Leyes Fuertes de los Grandes Números}

En primer lugar, tenemos el siguiente resultado, cuya demostración no se incluye por exceder a los contenidos de este curso.
\begin{prop}[Ley fuerte de Kolmogorov]
    Si $\{X_n\}_{n\in \bb{N}}$ es una sucesión de variables aleatorias independientes e idénticamente distribuidas. Se tiene:
    \begin{equation*}
        \left\{\frac{S_n-E[S_n]}{n}\right\} \xrightarrow{c.s.} 0
        \Longleftrightarrow \exists E[X_n] \text{ para algún } n\in \bb{N}
    \end{equation*}
\end{prop}

Notemos que, como la convergencia casi segura implica la convergencia en probabilidad, tiene hipótesis más simples que la Ley Débil de los Grandes Números de Khintchine y resultados más fuertes (ya que se trata también de una condición necesaria). Además, se tiene el siguiente corolario, análogo al visto para el visto para la Ley Débil de los Grandes Números de Khintchine.
\begin{coro}
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias independientes e idénticamente distribuidas. Entonces, se cumple que:
    \begin{equation*}
        \left\{\frac{S_n}{n}\right\} \xrightarrow{c.s.} E[X_n]\qquad \forall n\in \bb{N}
        \Longleftrightarrow \exists E[X_n] \text{ para algún } n\in \bb{N}
    \end{equation*}
\end{coro}
\begin{proof}
    Fijado $n\in \bb{N}$, se tiene que:
    \begin{equation*}
        E[S_n] = E\left[\sum_{i=1}^{n} X_i\right] = \sum_{i=1}^{n} E[X_i] = \sum_{i=1}^{n} E[X_n] = nE[X_n]
    \end{equation*}
    donde hemos usado que $E[X_i]=E[X_n]~\forall i\in \{1,\ldots,n\}$. Del Lema \ref{lema:suma_convergencia_cs}, se tiene:
    \begin{equation*}
        \left\{\frac{S_n-E[S_n]}{n}\right\} \xrightarrow{c.s.} 0 \Longrightarrow 
        \left\{\frac{S_n-E[S_n]}{n}+\frac{E[S_n]}{n}\right\} \xrightarrow{c.s.} 0+\frac{E[S_n]}{n}=\frac{nE[X_n]}{n}=E[X_n]
    \end{equation*}
\end{proof}

Históricamente, es relevante el caso en el que la distribución de las variables aleatorias $X_n$ es de Bernoulli, ya que este se descubrió antes. Se introduce entonces como corolario, pero notemos que es un caso particular de los anteriores usando que $X_n\sim B(p)$ y $E[X_n]=p$ para todo $n\in \bb{N}$.
\begin{coro}[Ley Fuerte de los Grandes Números de Borel]
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias independientes e idénticamente distribuidas con distribución de Bernoulli de parámetro $p\in \left]0,1\right[$. Es decir, $X_n\sim B(p)~\forall n\in \bb{N}$. Entonces, se cumple que:
    \begin{equation*}
        \left\{\frac{S_n-E[S_n]}{n}\right\} \xrightarrow{c.s.} 0
    \end{equation*}
\end{coro}
\begin{coro}
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias independientes e idénticamente distribuidas con distribución de Bernoulli de parámetro $p\in \left]0,1\right[$. Entonces, se cumple que:
    \begin{equation*}
        \left\{\frac{S_n}{n}\right\} \xrightarrow{c.s.} p
    \end{equation*}
\end{coro}

\section{Problema Central Del Límite}

En la presente sección buscamos dar soluciones al Problema Central del Límite. Dicho problema considera una sucesión de variables aleatorias $\{X_n\}_{n\in \bb{N}}$ independientes definidas sobre el mismo espacio de probabilidad $(\Omega, \mathcal{A}, P)$ y su sucesión de sumas parciales $\{S_n\}_{n\in \bb{N}}$. Se trata de encontrar condiciones que garanticen las siguientes convergencias de las sumas centradas y tipificadas:
\begin{enumerate}
    \item Convergencia en Ley a la Distribución Degenerada:
    \begin{equation*}
        \exists E[X_n]~\forall n\in \bb{N}\Longrightarrow \left\{\frac{S_n - E[S_n]}{n}\right\} \xrightarrow{L} 0
    \end{equation*}
    \item Convergencia en Ley a la Distribución Normal Tipificada:
    \begin{equation*}
        \exists E[X_n^2]~\forall n\in \bb{N}\Longrightarrow \left\{\frac{S_n - E[S_n]}{\sqrt{\Var[S_n]}}\right\} \xrightarrow{L} Z\sim \cc{N}(0,1)
    \end{equation*}
\end{enumerate}

La solución definitiva a este problema, que se obtuvo en el primer cuarto del siglo XX, establece condiciones necesarias y suficientes para la convergencia en ambos casos. Aquí nos limitamos a exponer una de las soluciones más populares, de enorme importancia por su gran aplicabilidad a diferentes problemas prácticos, el denominado Teorema de Lévy.
\begin{teo}[Teorema de Lévy]\label{teo:teorema_de_levy}
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias independientes e idénticamente distribuidas. Entonces, se cumple el Teorema Central del Límite, es decir:
    \begin{enumerate}
        \item Convergencia en Ley a la Distribución Degenerada:
        \begin{equation*}
            \exists E[X_n]~\forall n\in \bb{N}\Longrightarrow \left\{\frac{S_n - E[S_n]}{n}\right\} \xrightarrow{L} 0
        \end{equation*}
        \item Convergencia en Ley a la Distribución Normal Tipificada:
        \begin{equation*}
            \exists E[X_n^2]~\forall n\in \bb{N}\Longrightarrow \left\{\frac{S_n - E[S_n]}{\sqrt{\Var[S_n]}}\right\} \xrightarrow{L} Z\sim \cc{N}(0,1)
        \end{equation*}
    \end{enumerate}
\end{teo}
% // TODO: Demostrar. Copiada por Carlos
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T10_TeoremadeLevy.pdf#[0,{%22name%22:%22Fit%22}]

Notemos que este es el resultado empleado en las aproximaciones que vimos en la Sección~\ref{{sec:aproximaciones}}.
Históricamente, es relevante el caso en el que la distribución de las variables aleatorias $X_n$ es de Bernoulli, ya que este resultado se descubrió antes. Se incluyen como corolarios, ya que es un caso particular del Teorema de Lévy. El caso de la convergencia en Ley a la Distribución Degenerada fue demostrado por Bernoulli, mientras que el caso de la convergencia en Ley a la Distribución Normal fue demostrado por De Moivre y Laplace.
\begin{coro}[Teorema De Bernouilli]
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias independientes e idénticamente distribuidas con distribución de Bernoulli de parámetro $p\in \left]0,1\right[$. Es decir, $X_n\sim B(p)~\forall n\in \bb{N}$. Entonces:
    \begin{equation*}
        \left\{\frac{S_n - E[S_n]}{n}\right\} \xrightarrow{L} 0
    \end{equation*}
\end{coro}
\begin{coro}[Teorema De Moivre-Laplace]\label{coro:teorema_de_moivre_laplace}
    Sea $\{X_n\}_{n\in \bb{N}}$ una sucesión de variables aleatorias independientes e idénticamente distribuidas con distribución de Bernoulli de parámetro $p\in \left]0,1\right[$. Es decir, $X_n\sim B(p)~\forall n\in \bb{N}$. Entonces:
    \begin{equation*}
        \left\{\frac{S_n - E[S_n]}{\sqrt{\Var[S_n]}}\right\} \xrightarrow{L} Z\sim \cc{N}(0,1)
    \end{equation*}
\end{coro}

Buscamos por último entender el Teorema de De Moivre-Laplace. Si $X_n\sim B(p)~\forall n\in \bb{N}$, por la reproductividad de la binomial tenemos que $S_n\sim B(n,p)~\forall n\in \bb{N}$. Por tanto, se tiene que:
\begin{equation*}
    E[S_n] = np\qquad \Var[S_n] = np(1-p)
\end{equation*}

Por tanto, tenemos que:
\begin{equation*}
    \left\{\frac{S_n - E[S_n]}{\sqrt{\Var[S_n]}}\right\} = \left\{\frac{S_n - np}{\sqrt{np(1-p)}}\right\} \xrightarrow{L} Z\sim \cc{N}(0,1)
    \Longrightarrow S_n \xrightarrow{L} \sqrt{np(1-p)}Z + np
\end{equation*}

No obstante, tenemos que $\sqrt{np(1-p)}Z + np$ es una variable aleatoria normal de media $np$ y varianza $np(1-p)$, es decir:
\begin{equation*}
    S_n \xrightarrow{L} X\sim \cc{N}(np, \sqrt{np(1-p)}Z + np)
\end{equation*}

Por tanto, dada una variable aleatoria $X_n\sim B(n,p)$ que represente el número de éxitos en $n$ ensayos de Bernoulli, consideramos las variables aleatorias $X_i$ que representan los éxitos en cada ensayo. Entonces, repitiendo el proceso, tenemos que:
\begin{equation*}
    X_n \xrightarrow{L} X\sim \cc{N}(np, \sqrt{np(1-p)}Z + np)
\end{equation*}