\chapter{Esperanza Condicionada}

Cuando se considera un conjunto de variables aleatorias definidas en relación a un determinado experimento, es usual que existan relaciones entre ellas. El problema de regresión consiste en encontrar una función matemática que permita aproximar el valor de determinadas variables, conocidos los valores del resto.
\begin{observacion}
    Este concepto de regresión es el análogo al visto en la asignatura de EDIP. En este caso, en vez de establecer un carácter $Y$ en función de un carácter $X$ en función de sus frecuencias, buscamos establecer una relación entre dos variables aleatorias.
\end{observacion}

En el presente curso, por simplicidad, nos limitaremos a analizar el problema de regresión bidimensional; es decir, dadas dos variables aleatorias $X$ e $Y$, queremos encontrar una función matemática que nos permita aproximar el valor de $Y$ conocido el valor de $X$. Para encontrar esta función matemática usaremos el criterio de optimalidad denominado mínimos cuadrados, visto ya en EDIP.

\section{Esperanza condicionada}
Para encontrar esta función matemática, necesitamos introducir el concepto de esperanza condicionada.
\begin{definicion}[Esperanza condicionada]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, se define la esperanza condicionada de $X$ a $Y$, notada por $E[X\mid Y]$, como la variable aleatoria siguiente (en el caso de que exista):
    \begin{equation*}
        E[X\mid Y](y) := E[X\mid Y = y]
    \end{equation*}

    Notemos que esta esperanza es una esperanza normal, solo que considerando la distribución condicionada de $X$ a $Y = y$.
\end{definicion}
Veamos en función de si es discreta o continua:
\begin{itemize}
    \item Si $X$ e $Y$ son discretas, entonces:
    \begin{equation*}
        E[X\mid Y = y] = \sum_{x\in E_x} x\cdot P[X = x\mid Y = y] \qquad \text{con}~P[Y=y]>0
    \end{equation*}
    \item Si $X$ e $Y$ son continuas, entonces:
    \begin{equation*}
        E[X\mid Y = y] = \int_{-\infty}^{\infty} x\cdot f_{X\mid Y=y}(x)~dx \qquad \text{con}~f_Y(y)>0
    \end{equation*}
\end{itemize}

Notemos que la definición de $E[X\mid Y = y]$ requiere que la distribución condicionada de $X$ a $Y = y$ esté bien definida. Por este motivo, es preciso exigir que $P[Y = y] \neq 0$ en el caso discreto, y $f_Y(y) \neq 0$ en el caso continuo. Así, la variable aleatoria $E[X\mid Y]$ es una función de la variable $Y$, definida sobre el conjunto de sus valores, $E_Y$.

Veamos ahora qué hemos de imponer para que exista la esperanza condicionada de $X$ a $Y$.
\begin{prop}
    Sean dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad. Entonces:
    \begin{equation*}
        \exists E[X] \Longrightarrow \exists E[X\mid Y]
    \end{equation*}
\end{prop}
\begin{proof}
    Distinguimos en función de si $X$ e $Y$ son discretas o continuas:
    \begin{itemize}
        \item Si $X$ e $Y$ son discretas, entonces:
        \begin{align*}
            E[X\mid Y=y] &= \sum_{x\in E_x} |x|\cdot P[X = x\mid Y = y] = \sum_{x\in E_x} |x|\cdot \dfrac{P[X = x, Y = y]}{P[Y = y]} =\\&= \dfrac{\sum_{x\in E_x} |x|\cdot P[X = x, Y = y]}{P[Y = y]} \leq \dfrac{\sum_{x\in E_x} |x|\cdot P[X = x]}{P[Y = y]} = \dfrac{E[|X|]}{P[Y = y]}<\infty
        \end{align*}
        donde en la desigualdad hemos hecho uso de que
        \begin{equation*}
            P[X = x, Y = y] \leq P[X = x] \forall x,y
        \end{equation*}
        y, al final, hemos usado que $E[|X|]<\infty$ por tener $E[X]$ finita. Por tanto, $E[X\mid Y]$ existe.

        \item Si $X$ e $Y$ son continuas, entonces:
        \begin{align*}
            E[X\mid Y=y] &= \int_{-\infty}^{\infty} |x|\cdot f_{X\mid Y=y}(x)~dx = \int_{-\infty}^{\infty} |x|\cdot \dfrac{f_{X,Y}(x,y)}{f_Y(y)}~dx =\\&= \dfrac{\int_{-\infty}^{\infty} |x|\cdot f_{X,Y}(x,y)~dx}{f_Y(y)}
        \end{align*}
        Como $E[|X|]<\infty$, entonces:
        \begin{multline*}
            E[|X|]=\int_{-\infty}^{\infty} |x|\int_{-\infty}^{\infty} f_{X,Y}(x,y)~dx~dy = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} |x|\cdot f_{X,Y}(x,y)~dx~dy<\infty
            \Longrightarrow\\\Longrightarrow
            \int_{-\infty}^{\infty} |x|\cdot f_{X,Y}(x,y)~dx<\infty
        \end{multline*}
        Por tanto, $E[X\mid Y]$ existe.
    \end{itemize}
\end{proof}

\begin{ejemplo}
    Sea $(X,Y)$ un vector aleatorio bidimensional con función de densidad conjunta:
    \begin{equation*}
        f_{X,Y}(x,y) = \begin{cases}
            2 & \text{si } 0< x< y< 1 \\
            0 & \text{en otro caso}
        \end{cases}
    \end{equation*}
    Calcular $E[X\mid Y]$ y $E[Y\mid X]$.\\

    Tenemos que:
    \begin{align*}
        E[X\mid Y = y] &= \int_{-\infty}^{\infty} x\cdot f_{X\mid Y=y}(x)~dx\\
        E[Y\mid X = x] &= \int_{-\infty}^{\infty} y\cdot f_{Y\mid X=x}(y)~dy
    \end{align*}

    Para calcular ambas funciones de densidad condicionada, necesitamos calcular las marginales de $X$ e $Y$:
    \begin{align*}
        f_X(x) &= \int_{-\infty}^{\infty} f_{X,Y}(x,y)~dy = \int_{x}^{1} 2~dy = 2(1-x)\\
        f_Y(y) &= \int_{-\infty}^{\infty} f_{X,Y}(x,y)~dx = \int_{0}^{y} 2~dx = 2y
    \end{align*}

    Así, las funciones de densidad condicionada son:
    \begin{align*}
        f_{X\mid Y=y}(x) &= \frac{f_{X,Y}(x,y)}{f_Y(y)} = \frac{2}{2y} = \frac{1}{y} \quad \text{si } 0< x< y< 1\\
        f_{Y\mid X=x}(y) &= \frac{f_{X,Y}(x,y)}{f_X(x)} = \frac{2}{2(1-x)} = \frac{1}{1-x} \quad \text{si } 0< x< y< 1
    \end{align*}

    Por tanto, las esperanzas condicionadas son:
    \begin{align*}
        E[X\mid Y = y] &= \int_{-\infty}^{\infty} x\cdot f_{X\mid Y=y}(x)~dx = \int_{0}^{y} x\cdot \frac{1}{y}~dx = \frac{1}{y}\int_{0}^{y} x~dx = \frac{1}{y}\left[\frac{x^2}{2}\right]_{0}^{y} = \frac{y}{2}\\
        E[Y\mid X = x] &= \int_{-\infty}^{\infty} y\cdot f_{Y\mid X=x}(y)~dy = \int_{x}^{1} y\cdot \frac{1}{1-x}~dy = \frac{1}{1-x}\int_{x}^{1} y~dy = \frac{1}{1-x}\left[\frac{y^2}{2}\right]_{x}^{1} =\\&= \dfrac{1}{1-x}\left(\dfrac{1}{2}-\dfrac{x^2}{2}\right) = \dfrac{1}{2}\cdot \dfrac{1-x^2}{1-x} = \dfrac{1+x}{2}
    \end{align*}
\end{ejemplo}

Al igual que considerábamos $E[g(X)]$, consideramos ahora la esperanza condicionada de una función de $X$ a $Y$.
\begin{definicion}[Esperanza condicionada de una función]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, y una función $g:\mathbb{R}\to\mathbb{R}$ medible, se define la esperanza condicionada de $g(X)$ a $Y$, notada por $E[g(X)\mid Y]$, como la variable aleatoria siguiente (en el caso de que exista):
    \begin{equation*}
        E[g(X)\mid Y](y) := E[g(X)\mid Y = y]
    \end{equation*}

    Veamos en función de si es discreta o continua:
    \begin{itemize}
        \item Si $X$ e $Y$ son discretas, entonces:
        \begin{equation*}
            E[g(X)\mid Y = y] = \sum_{x\in E_x} g(x)\cdot P[X = x\mid Y = y] \qquad \text{con}~P[Y=y]>0
        \end{equation*}
        \item Si $X$ e $Y$ son continuas, entonces:
        \begin{equation*}
            E[g(X)\mid Y = y] = \int_{-\infty}^{\infty} g(x)\cdot f_{X\mid Y=y}(x)~dx \qquad \text{con}~f_Y(y)>0
        \end{equation*}
    \end{itemize}
\end{definicion}

Al igual que ocurría con la esperanza condicionada de $X$ a $Y$, para que exista basta con imponer que $E[g(X)]<\infty$. Es decir, la demostración de la siguiente proposición es análoga a la del proposición anterior.
\begin{prop}
    Sean dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, y una función $g:\mathbb{R}\to\mathbb{R}$ medible. Entonces:
    \begin{equation*}
        \exists E[g(X)] \Longrightarrow \exists E[g(X)\mid Y]
    \end{equation*}
\end{prop}


\begin{ejemplo}
    En el experimento aleatorio del lanzamiento de tres monedas se consideran las variables:
    \begin{itemize}
        \item $X$: Número de caras.
        \item $Y$: Diferencia, en valor absoluto, entre el número de caras y el número de cruces.
    \end{itemize}
    Calcular $E[X^2\mid Y]$.\\

    El espacio muestral es el siguiente:
    \begin{equation*}
        \Omega = \{CCC, CC+, C+C, +CC, C++, +C+, ++C, +++\}
    \end{equation*}

    La función masa de probabilidad conjunta viene dada por:
    \begin{equation*}
        \begin{array}{c|cc||c}
            X\setminus Y & 1 & 3 & P[X=x_i] \\ \hline
            0 & 0 & \nicefrac{1}{2^3} & \nicefrac{1}{2^3} \\
            1 & \nicefrac{3}{2^3} & 0 & \nicefrac{3}{2^3} \\
            2 & \nicefrac{3}{2^3} & 0 & \nicefrac{3}{2^3} \\
            3 & 0 & \nicefrac{1}{2^3} & \nicefrac{1}{2^3} \\ \hline \hline
            P[Y=y_j] & \nicefrac{3}{2^2} & \nicefrac{1}{2^2} & 1
        \end{array}
    \end{equation*}
    donde además hemos calculado las marginales de $X$ e $Y$. Entonces, tenemos que:
    \begin{align*}
        E[X^2\mid Y=1]&=\sum_{x\in E_x} x^2\cdot P[X=x\mid Y=1] =\\&= 1^2\cdot P[X=1\mid Y=1] + 2^2\cdot P[X=2\mid Y=1] =\\&= 1\cdot \dfrac{P[X=1,Y=1]}{P[Y=1]} + 4\cdot \dfrac{P[X=2,Y=1]}{P[Y=1]} =\\&= 1\cdot \dfrac{\nicefrac{3}{2^3}}{\nicefrac{3}{2^2}} + 4\cdot \dfrac{\nicefrac{3}{2^3}}{\nicefrac{3}{2^2}} = \dfrac{5}{2}\\
        E[X^2\mid Y=3]&=\sum_{x\in E_x} x^2\cdot P[X=x\mid Y=3] =\\&= 0^2\cdot P[X=0\mid Y=3] + 3^2\cdot P[X=3\mid Y=3] =\\&= 0\cdot \dfrac{P[X=0,Y=3]}{P[Y=3]} + 9\cdot \dfrac{P[X=3,Y=3]}{P[Y=3]} =\\&= 9\cdot \dfrac{\nicefrac{1}{2^3}}{\nicefrac{1}{2^2}} = \dfrac{9}{2}
    \end{align*}

    Por tanto, la esperanza condicionada de $X^2$ a $Y$ es:
    \begin{equation*}
        E[X^2\mid Y] = \begin{cases}
            \nicefrac{5}{2} & \text{si } Y=1 \\
            \nicefrac{9}{2} & \text{si } Y=3
        \end{cases}
    \end{equation*}
\end{ejemplo}

\subsubsection{Propiedades de la esperanza condicionada}

Introducimos las siguientes propiedades, que se deducen de forma directa en la mayoría de los casos haciendo uso de que la esperanza condicionada de $X$ a $Y$ es la esperanza de $X$ con respecto a la distribución condicionada de $X$ a $Y$.
\begin{prop}
    Sea $Y$ una variable aleatoria discreta sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$ y $c\in \mathbb{R}$. Entonces:
    \begin{equation*}
        E[c\mid Y] = c
    \end{equation*}
\end{prop}
\begin{proof}
    Como $Y$ es discreta, entonces:
    \begin{equation*}
        E[c\mid Y = y] = \sum_{x\in E_x} x\cdot P[X = x\mid Y = y] = c\cdot P[X = c\mid Y = y] = c
    \end{equation*}
\end{proof}
Notemos que no consideramos $Y$ continua, ya que $c$ es una variable aleatoria constante (discreta) y solo hemos definido la esperanza condicionada para variables aleatorias del mismo tipo.

\begin{prop}[Linealidad]
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Entonces:
    \begin{equation*}
        E[(aX+b)\mid Y] = aE[X\mid Y] + b \quad \forall a,b\in\mathbb{R}
    \end{equation*}
\end{prop}

\begin{prop}
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Entonces:
    \begin{equation*}
        E[aX_1+bX_2\mid Y] = aE[X_1\mid Y] + bE[X_2\mid Y] \quad \forall a,b\in\mathbb{R}
    \end{equation*}
\end{prop}

\begin{prop}
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Si $X\geq 0$, entonces:
    \begin{equation*}
        E[X\mid Y] \geq 0 \land E[X\mid Y] =0\Longleftrightarrow P[X=0] = 1
    \end{equation*}
\end{prop}

\begin{prop}[Conservación del Orden]
    Sean $X_1,X_2$ e $Y$ tres variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Si $X_1\leq X_2$, entonces:
    \begin{equation*}
        E[X_1\mid Y] \leq E[X_2\mid Y]
    \end{equation*}
\end{prop}

\begin{prop}
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$, y consideramos una función $g:\mathbb{R}\to\mathbb{R}$ medible. Entonces:
    \begin{equation*}
        \text{Si } X,Y \text{ independientes} \Longrightarrow E[g(X)\mid Y] = E[g(X)]
    \end{equation*}
    En particular, tomando $g=Id$, tenemos:
    \begin{equation*}
        \text{Si } X,Y \text{ independientes} \Longrightarrow E[X\mid Y] = E[X]
    \end{equation*}
\end{prop}
\begin{proof}
    Distinguimos en función de si $X$ e $Y$ son discretas o continuas:
    \begin{itemize}
        \item Si $X$ e $Y$ son discretas, como $X$ e $Y$ son independientes, entonces:
        \begin{equation*}
            P[X = x\mid Y = y] = P[X = x] \quad \forall x,y
        \end{equation*}
        Por tanto, tenemos:
        \begin{align*}
            E[g(X)\mid Y = y] &= \sum_{x\in E_X} g(x)\cdot P[X = x\mid Y = y] = \sum_{x\in E_X} g(x)\cdot P[X = x] = E[g(X)]
        \end{align*}

        \item Si $X$ e $Y$ son continuas, como $X$ e $Y$ son independientes, entonces:
        \begin{equation*}
            f_{X\mid Y=y}(x) = f_X(x) \quad \forall x,y
        \end{equation*}
        Por tanto, tenemos:
        \begin{align*}
            E[g(X)\mid Y = y] &= \int_{-\infty}^{\infty} g(x)\cdot f_{X\mid Y=y}(x)~dx = \int_{-\infty}^{\infty} g(x)\cdot f_X(x)~dx = E[g(X)]
        \end{align*}
    \end{itemize}
\end{proof}

\begin{prop}
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$, y consideramos una función $g:\mathbb{R}\to\mathbb{R}$ medible. Entonces:
    \begin{equation*}
        E[E[g(X)\mid Y]] = E[g(X)]
    \end{equation*}
    En particular, tomando $g=Id$, tenemos:
    \begin{equation*}
        E[E[X\mid Y]] = E[X]
    \end{equation*}
\end{prop}
\begin{proof}
    Distinguimos en función de si $X$ e $Y$ son discretas o continuas:
    \begin{itemize}
        \item Si $X$ e $Y$ son discretas, entonces, como $E[g(X)\mid Y]$ es una variable aleatoria en función de $Y$, usando la esperanza de una función de una variable aleatoria, tenemos:
        \begin{align*}
            E[E[g(X)\mid Y]] &= \sum_{y\in E_Y} E[g(X)\mid Y = y]\cdot P[Y = y] =\\&= \sum_{y\in E_Y} \sum_{x\in E_X} g(x)\cdot P[X = x\mid Y = y]\cdot P[Y = y] =\\&= \sum_{x\in E_X} g(x)\cdot \sum_{y\in E_Y} P[X = x, Y = y] =\\&= \sum_{x\in E_X} g(x)\cdot P[X = x] = E[g(X)]
        \end{align*}

        \item Si $X$ e $Y$ son continuas, entonces, como $E[g(X)\mid Y]$ es una variable aleatoria en función de $Y$, usando la esperanza de una función de una variable aleatoria, tenemos:
        \begin{align*}
            E[E[g(X)\mid Y]] &= \int_{-\infty}^{\infty} E[g(X)\mid Y = y]\cdot f_Y(y)~dy =\\&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x)\cdot f_{X\mid Y=y}(x)~dx\cdot f_Y(y)~dy =\\&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x)\cdot f_{X,Y}(x,y)~dx~dy =\\&= \int_{-\infty}^{\infty} g(x)\cdot \int_{-\infty}^{\infty} f_{X,Y}(x,y)~dy~dx =\\&= \int_{-\infty}^{\infty} g(x)\cdot f_X(x)~dx = E[g(X)]
        \end{align*}
    \end{itemize}
\end{proof}