\chapter{Esperanza Condicionada}

Cuando se considera un conjunto de variables aleatorias definidas en relación a un determinado experimento, es usual que existan relaciones entre ellas. El problema de regresión consiste en encontrar una función matemática que permita aproximar el valor de determinadas variables, conocidos los valores del resto.
\begin{observacion}
    Este concepto de regresión es el análogo al visto en la asignatura de EDIP. En este caso, en vez de establecer un carácter $Y$ en función de un carácter $X$ en función de sus frecuencias, buscamos establecer una relación entre dos variables aleatorias.
\end{observacion}

En el presente curso, por simplicidad, nos limitaremos a analizar el problema de regresión bidimensional; es decir, dadas dos variables aleatorias $X$ e $Y$, queremos encontrar una función matemática que nos permita aproximar el valor de $Y$ conocido el valor de $X$. Para encontrar esta función matemática usaremos el criterio de optimalidad denominado mínimos cuadrados, visto ya en EDIP.

\section{Esperanza condicionada}
Para encontrar esta función matemática, necesitamos introducir el concepto de esperanza condicionada.
\begin{definicion}[Esperanza condicionada]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, se define la esperanza condicionada de $X$ a $Y$, notada por $E[X\mid Y]$, como la variable aleatoria siguiente (en el caso de que exista):
    \begin{equation*}
        E[X\mid Y](y) := E[X\mid Y = y]
    \end{equation*}

    Notemos que esta esperanza es una esperanza normal, solo que considerando la distribución condicionada de $X$ a $Y = y$.
\end{definicion}
Veamos en función de si es discreta o continua:
\begin{itemize}
    \item Si $X$ e $Y$ son discretas, entonces:
    \begin{equation*}
        E[X\mid Y = y] = \sum_{x\in E_x} x\cdot P[X = x\mid Y = y] \qquad \text{con}~P[Y=y]>0
    \end{equation*}
    \item Si $X$ e $Y$ son continuas, entonces:
    \begin{equation*}
        E[X\mid Y = y] = \int_{-\infty}^{\infty} x\cdot f_{X\mid Y=y}(x)~dx \qquad \text{con}~f_Y(y)>0
    \end{equation*}
\end{itemize}

Notemos que la definición de $E[X\mid Y = y]$ requiere que la distribución condicionada de $X$ a $Y = y$ esté bien definida. Por este motivo, es preciso exigir que $P[Y = y] \neq 0$ en el caso discreto, y $f_Y(y) \neq 0$ en el caso continuo. Así, la variable aleatoria $E[X\mid Y]$ es una función de la variable $Y$, definida sobre el conjunto de sus valores, $E_Y$.

Veamos ahora qué hemos de imponer para que exista la esperanza condicionada de $X$ a $Y$.
\begin{prop}
    Sean dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad. Entonces:
    \begin{equation*}
        \exists E[X] \Longrightarrow \exists E[X\mid Y]
    \end{equation*}
\end{prop}
\begin{proof}
    Distinguimos en función de si $X$ e $Y$ son discretas o continuas:
    \begin{itemize}
        \item Si $X$ e $Y$ son discretas, entonces:
        \begin{align*}
            E[X\mid Y=y] &= \sum_{x\in E_x} |x|\cdot P[X = x\mid Y = y] = \sum_{x\in E_x} |x|\cdot \dfrac{P[X = x, Y = y]}{P[Y = y]} =\\&= \dfrac{\sum_{x\in E_x} |x|\cdot P[X = x, Y = y]}{P[Y = y]} \leq \dfrac{\sum_{x\in E_x} |x|\cdot P[X = x]}{P[Y = y]} = \dfrac{E[|X|]}{P[Y = y]}<\infty
        \end{align*}
        donde en la desigualdad hemos hecho uso de que
        \begin{equation*}
            P[X = x, Y = y] \leq P[X = x] \forall x,y
        \end{equation*}
        y, al final, hemos usado que $E[|X|]<\infty$ por tener $E[X]$ finita. Por tanto, $E[X\mid Y]$ existe.

        \item Si $X$ e $Y$ son continuas, entonces:
        \begin{align*}
            E[X\mid Y=y] &= \int_{-\infty}^{\infty} |x|\cdot f_{X\mid Y=y}(x)~dx = \int_{-\infty}^{\infty} |x|\cdot \dfrac{f_{X,Y}(x,y)}{f_Y(y)}~dx =\\&= \dfrac{\int_{-\infty}^{\infty} |x|\cdot f_{X,Y}(x,y)~dx}{f_Y(y)}
        \end{align*}
        Como $E[|X|]<\infty$, entonces:
        \begin{multline*}
            E[|X|]=\int_{-\infty}^{\infty} |x|\int_{-\infty}^{\infty} f_{X,Y}(x,y)~dx~dy = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} |x|\cdot f_{X,Y}(x,y)~dx~dy<\infty
            \Longrightarrow\\\Longrightarrow
            \int_{-\infty}^{\infty} |x|\cdot f_{X,Y}(x,y)~dx<\infty
        \end{multline*}
        Por tanto, $E[X\mid Y]$ existe.
    \end{itemize}
\end{proof}

\begin{ejemplo}
    Sea $(X,Y)$ un vector aleatorio bidimensional con función de densidad conjunta:
    \begin{equation*}
        f_{X,Y}(x,y) = \begin{cases}
            2 & \text{si } 0< x< y< 1 \\
            0 & \text{en otro caso}
        \end{cases}
    \end{equation*}
    Calcular $E[X\mid Y]$ y $E[Y\mid X]$.\\

    Tenemos que:
    \begin{align*}
        E[X\mid Y = y] &= \int_{-\infty}^{\infty} x\cdot f_{X\mid Y=y}(x)~dx\\
        E[Y\mid X = x] &= \int_{-\infty}^{\infty} y\cdot f_{Y\mid X=x}(y)~dy
    \end{align*}

    Para calcular ambas funciones de densidad condicionada, necesitamos calcular las marginales de $X$ e $Y$:
    \begin{align*}
        f_X(x) &= \int_{-\infty}^{\infty} f_{X,Y}(x,y)~dy = \int_{x}^{1} 2~dy = 2(1-x)\\
        f_Y(y) &= \int_{-\infty}^{\infty} f_{X,Y}(x,y)~dx = \int_{0}^{y} 2~dx = 2y
    \end{align*}

    Así, las funciones de densidad condicionada son:
    \begin{align*}
        f_{X\mid Y=y}(x) &= \frac{f_{X,Y}(x,y)}{f_Y(y)} = \frac{2}{2y} = \frac{1}{y} \quad \text{si } 0< x< y< 1\\
        f_{Y\mid X=x}(y) &= \frac{f_{X,Y}(x,y)}{f_X(x)} = \frac{2}{2(1-x)} = \frac{1}{1-x} \quad \text{si } 0< x< y< 1
    \end{align*}

    Por tanto, las esperanzas condicionadas son:
    \begin{align*}
        E[X\mid Y = y] &= \int_{-\infty}^{\infty} x\cdot f_{X\mid Y=y}(x)~dx = \int_{0}^{y} x\cdot \frac{1}{y}~dx = \frac{1}{y}\int_{0}^{y} x~dx = \frac{1}{y}\left[\frac{x^2}{2}\right]_{0}^{y} = \frac{y}{2}\\
        E[Y\mid X = x] &= \int_{-\infty}^{\infty} y\cdot f_{Y\mid X=x}(y)~dy = \int_{x}^{1} y\cdot \frac{1}{1-x}~dy = \frac{1}{1-x}\int_{x}^{1} y~dy = \frac{1}{1-x}\left[\frac{y^2}{2}\right]_{x}^{1} =\\&= \dfrac{1}{1-x}\left(\dfrac{1}{2}-\dfrac{x^2}{2}\right) = \dfrac{1}{2}\cdot \dfrac{1-x^2}{1-x} = \dfrac{1+x}{2}
    \end{align*}
\end{ejemplo}

Al igual que considerábamos $E[g(X)]$, consideramos ahora la esperanza condicionada de una función de $X$ a $Y$.
\begin{definicion}[Esperanza condicionada de una función]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, y una función $g:\mathbb{R}\to\mathbb{R}$ medible, se define la esperanza condicionada de $g(X)$ a $Y$, notada por $E[g(X)\mid Y]$, como la variable aleatoria siguiente (en el caso de que exista):
    \begin{equation*}
        E[g(X)\mid Y](y) := E[g(X)\mid Y = y]
    \end{equation*}

    Veamos en función de si es discreta o continua:
    \begin{itemize}
        \item Si $X$ e $Y$ son discretas, entonces:
        \begin{equation*}
            E[g(X)\mid Y = y] = \sum_{x\in E_x} g(x)\cdot P[X = x\mid Y = y] \qquad \text{con}~P[Y=y]>0
        \end{equation*}
        \item Si $X$ e $Y$ son continuas, entonces:
        \begin{equation*}
            E[g(X)\mid Y = y] = \int_{-\infty}^{\infty} g(x)\cdot f_{X\mid Y=y}(x)~dx \qquad \text{con}~f_Y(y)>0
        \end{equation*}
    \end{itemize}
\end{definicion}

Al igual que ocurría con la esperanza condicionada de $X$ a $Y$, para que exista basta con imponer que $E[g(X)]<\infty$. Es decir, la demostración de la siguiente proposición es análoga a la del proposición anterior.
\begin{prop}
    Sean dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, y una función $g:\mathbb{R}\to\mathbb{R}$ medible. Entonces:
    \begin{equation*}
        \exists E[g(X)] \Longrightarrow \exists E[g(X)\mid Y]
    \end{equation*}
\end{prop}


\begin{ejemplo}
    En el experimento aleatorio del lanzamiento de tres monedas se consideran las variables:
    \begin{itemize}
        \item $X$: Número de caras.
        \item $Y$: Diferencia, en valor absoluto, entre el número de caras y el número de cruces.
    \end{itemize}
    Calcular $E[X^2\mid Y]$.\\

    El espacio muestral es el siguiente:
    \begin{equation*}
        \Omega = \{CCC, CC+, C+C, +CC, C++, +C+, ++C, +++\}
    \end{equation*}

    La función masa de probabilidad conjunta viene dada por:
    \begin{equation*}
        \begin{array}{c|cc||c}
            X\setminus Y & 1 & 3 & P[X=x_i] \\ \hline
            0 & 0 & \nicefrac{1}{2^3} & \nicefrac{1}{2^3} \\
            1 & \nicefrac{3}{2^3} & 0 & \nicefrac{3}{2^3} \\
            2 & \nicefrac{3}{2^3} & 0 & \nicefrac{3}{2^3} \\
            3 & 0 & \nicefrac{1}{2^3} & \nicefrac{1}{2^3} \\ \hline \hline
            P[Y=y_j] & \nicefrac{3}{2^2} & \nicefrac{1}{2^2} & 1
        \end{array}
    \end{equation*}
    donde además hemos calculado las marginales de $X$ e $Y$. Entonces, tenemos que:
    \begin{align*}
        E[X^2\mid Y=1]&=\sum_{x\in E_x} x^2\cdot P[X=x\mid Y=1] =\\&= 1^2\cdot P[X=1\mid Y=1] + 2^2\cdot P[X=2\mid Y=1] =\\&= 1\cdot \dfrac{P[X=1,Y=1]}{P[Y=1]} + 4\cdot \dfrac{P[X=2,Y=1]}{P[Y=1]} =\\&= 1\cdot \dfrac{\nicefrac{3}{2^3}}{\nicefrac{3}{2^2}} + 4\cdot \dfrac{\nicefrac{3}{2^3}}{\nicefrac{3}{2^2}} = \dfrac{5}{2}\\
        E[X^2\mid Y=3]&=\sum_{x\in E_x} x^2\cdot P[X=x\mid Y=3] =\\&= 0^2\cdot P[X=0\mid Y=3] + 3^2\cdot P[X=3\mid Y=3] =\\&= 0\cdot \dfrac{P[X=0,Y=3]}{P[Y=3]} + 9\cdot \dfrac{P[X=3,Y=3]}{P[Y=3]} =\\&= 9\cdot \dfrac{\nicefrac{1}{2^3}}{\nicefrac{1}{2^2}} = \dfrac{9}{2}
    \end{align*}

    Por tanto, la esperanza condicionada de $X^2$ a $Y$ es:
    \begin{equation*}
        E[X^2\mid Y] = \begin{cases}
            \nicefrac{5}{2} & \text{si } Y=1 \\
            \nicefrac{9}{2} & \text{si } Y=3
        \end{cases}
    \end{equation*}
\end{ejemplo}

\subsubsection{Propiedades de la esperanza condicionada}

Introducimos las siguientes propiedades, que se deducen de forma directa en la mayoría de los casos haciendo uso de que la esperanza condicionada de $X$ a $Y$ es la esperanza de $X$ con respecto a la distribución condicionada de $X$ a $Y$.
\begin{prop}
    Sea $Y$ una variable aleatoria discreta sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$ y $c\in \mathbb{R}$. Entonces:
    \begin{equation*}
        E[c\mid Y] = c
    \end{equation*}
\end{prop}
\begin{proof}
    Como $Y$ es discreta, entonces:
    \begin{equation*}
        E[c\mid Y = y] = \sum_{x\in E_x} x\cdot P[X = x\mid Y = y] = c\cdot P[X = c\mid Y = y] = c
    \end{equation*}
\end{proof}
Notemos que no consideramos $Y$ continua, ya que $c$ es una variable aleatoria constante (discreta) y solo hemos definido la esperanza condicionada para variables aleatorias del mismo tipo.

\begin{prop}[Linealidad]
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Entonces:
    \begin{equation*}
        E[(aX+b)\mid Y] = aE[X\mid Y] + b \quad \forall a,b\in\mathbb{R}
    \end{equation*}
\end{prop}

\begin{prop}
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Entonces:
    \begin{equation*}
        E[aX_1+bX_2\mid Y] = aE[X_1\mid Y] + bE[X_2\mid Y] \quad \forall a,b\in\mathbb{R}
    \end{equation*}
\end{prop}

\begin{prop}
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Si $X\geq 0$, entonces:
    \begin{equation*}
        E[X\mid Y] \geq 0 \land E[X\mid Y] =0\Longleftrightarrow P[X=0] = 1
    \end{equation*}
\end{prop}

\begin{prop}[Conservación del Orden]
    Sean $X_1,X_2$ e $Y$ tres variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Si $X_1\leq X_2$, entonces:
    \begin{equation*}
        E[X_1\mid Y] \leq E[X_2\mid Y]
    \end{equation*}
\end{prop}

\begin{prop}
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$, y consideramos una función $g:\mathbb{R}\to\mathbb{R}$ medible. Entonces:
    \begin{equation*}
        \text{Si } X,Y \text{ independientes} \Longrightarrow E[g(X)\mid Y] = E[g(X)]
    \end{equation*}
    En particular, tomando $g=Id$, tenemos:
    \begin{equation*}
        \text{Si } X,Y \text{ independientes} \Longrightarrow E[X\mid Y] = E[X]
    \end{equation*}
\end{prop}
\begin{proof}
    Distinguimos en función de si $X$ e $Y$ son discretas o continuas:
    \begin{itemize}
        \item Si $X$ e $Y$ son discretas, como $X$ e $Y$ son independientes, entonces:
        \begin{equation*}
            P[X = x\mid Y = y] = P[X = x] \quad \forall x,y
        \end{equation*}
        Por tanto, tenemos:
        \begin{align*}
            E[g(X)\mid Y = y] &= \sum_{x\in E_X} g(x)\cdot P[X = x\mid Y = y] = \sum_{x\in E_X} g(x)\cdot P[X = x] = E[g(X)]
        \end{align*}

        \item Si $X$ e $Y$ son continuas, como $X$ e $Y$ son independientes, entonces:
        \begin{equation*}
            f_{X\mid Y=y}(x) = f_X(x) \quad \forall x,y
        \end{equation*}
        Por tanto, tenemos:
        \begin{align*}
            E[g(X)\mid Y = y] &= \int_{-\infty}^{\infty} g(x)\cdot f_{X\mid Y=y}(x)~dx = \int_{-\infty}^{\infty} g(x)\cdot f_X(x)~dx = E[g(X)]
        \end{align*}
    \end{itemize}
\end{proof}

\begin{prop}
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$, y consideramos una función $g:\mathbb{R}\to\mathbb{R}$ medible. Entonces:
    \begin{equation*}
        E[E[g(X)\mid Y]] = E[g(X)]
    \end{equation*}
    En particular, tomando $g=Id$, tenemos:
    \begin{equation*}
        E[E[X\mid Y]] = E[X]
    \end{equation*}
\end{prop}
\begin{proof}
    Distinguimos en función de si $X$ e $Y$ son discretas o continuas:
    \begin{itemize}
        \item Si $X$ e $Y$ son discretas, entonces, como $E[g(X)\mid Y]$ es una variable aleatoria en función de $Y$, usando la esperanza de una función de una variable aleatoria, tenemos:
        \begin{align*}
            E[E[g(X)\mid Y]] &= \sum_{y\in E_Y} E[g(X)\mid Y = y]\cdot P[Y = y] =\\&= \sum_{y\in E_Y} \sum_{x\in E_X} g(x)\cdot P[X = x\mid Y = y]\cdot P[Y = y] =\\&= \sum_{x\in E_X} g(x)\cdot \sum_{y\in E_Y} P[X = x, Y = y] =\\&= \sum_{x\in E_X} g(x)\cdot P[X = x] = E[g(X)]
        \end{align*}

        \item Si $X$ e $Y$ son continuas, entonces, como $E[g(X)\mid Y]$ es una variable aleatoria en función de $Y$, usando la esperanza de una función de una variable aleatoria, tenemos:
        \begin{align*}
            E[E[g(X)\mid Y]] &= \int_{-\infty}^{\infty} E[g(X)\mid Y = y]\cdot f_Y(y)~dy =\\&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x)\cdot f_{X\mid Y=y}(x)~dx\cdot f_Y(y)~dy =\\&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x)\cdot f_{X,Y}(x,y)~dx~dy =\\&= \int_{-\infty}^{\infty} g(x)\cdot \int_{-\infty}^{\infty} f_{X,Y}(x,y)~dy~dx =\\&= \int_{-\infty}^{\infty} g(x)\cdot f_X(x)~dx = E[g(X)]
        \end{align*}
    \end{itemize}
\end{proof}

\section{Momentos condicionados}

Como una variable aleatoria que es, se pueden definir los momentos condicionados de una variable aleatoria a otra.
Comenzamos con los momentos condicionados no centrados.
\subsection{Momentos condicionados no centrados}
\begin{definicion}[Momento condicionado]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad y $k\in \bb{N}$, se define el momento condicionado de orden $k$ de $X$ a $Y$ como la variable aleatoria siguiente (en el caso de que exista):
    \begin{equation*}
        E[X^k\mid Y]
    \end{equation*}
\end{definicion}

Usando la esperanza condicionada de una función, tenemos que $\exists E[X^k]$ implica que exista $E[X^k\mid Y]$, y se calcula como:
\begin{itemize}
    \item Si $X$ e $Y$ son discretas, entonces:
    \begin{equation*}
        E[X^k\mid Y = y] = \sum_{x\in E_x} x^k\cdot P[X = x\mid Y = y] \qquad \text{con}~P[Y=y]>0
    \end{equation*}

    \item Si $X$ e $Y$ son continuas, entonces:
    \begin{equation*}
        E[X^k\mid Y = y] = \int_{-\infty}^{\infty} x^k\cdot f_{X\mid Y=y}(x)~dx \qquad \text{con}~f_Y(y)>0
    \end{equation*}
\end{itemize}

\subsection{Momentos condicionados centrados}
Respecto de los momentos condicionados centrados, tenemos la siguiente definición.
\begin{definicion}[Momento condicionado centrado]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad y $k\in \bb{N}$, se define el momento condicionado de orden $k$ centrado de $X$ a $Y$ como la variable aleatoria siguiente (en el caso de que exista):
    \begin{equation*}
        E[(X-E[X\mid Y])^k\mid Y]
    \end{equation*}
\end{definicion}

En caso de existencia, los valores de estas variables se calculan teniendo en cuenta que al considerar el condicionamiento a un valor arbitrario, $Y = y$, la variable $E[X\mid Y]$ toma el valor $E[X\mid Y = y]$ y, por tanto:
\begin{equation*}
    E[(X - E[X\mid Y])^k\mid Y = y] = E[(X - E[X\mid Y = y])^k\mid Y = y] \quad \forall y\in E_Y
\end{equation*}

Entonces, ya que dado $Y = y$, $E[X\mid Y = y]$ es una constante, la variable $(X - E[X\mid Y = y])^k$ sólo depende de $X$, y aplicando de nuevo la expresión de la esperanza condicionada de una función de una variable aleatoria, se tiene:
\begin{itemize}
    \item Si $X$ e $Y$ son discretas, entonces:
    \begin{equation*}
        \hspace{-1cm}E[(X - E[X\mid Y])^k\mid Y = y] = \sum_{x\in E_x} (x - E[X\mid Y = y])^k\cdot P[X = x\mid Y = y] \qquad \text{con}~P[Y=y]>0
    \end{equation*}
    \item Si $X$ e $Y$ son continuas, entonces:
    \begin{equation*}
        E[(X - E[X\mid Y])^k\mid Y = y] = \int_{-\infty}^{\infty} (x - E[X\mid Y = y])^k\cdot f_{X\mid Y=y}(x)~dx \qquad \text{con}~f_Y(y)>0
    \end{equation*}
\end{itemize}

Las propiedades de los momentos condicionados son similares a las de los momentos sin condicionar. En particular,
la existencia de los momentos condicionados no centrados equivale a la de los momentos condicionados centrados.

\section{Varianza condicionada}

La varianza condicionada es un caso particular de los momentos condicionados centrados, y es de especial relevancia en el estudio de la regresión, por lo que estudiaremos su definición y propiedades.
\begin{definicion}
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, se define la varianza condicionada de $X$ a $Y$ como el momento condicionado centrado de orden 2 de $X$ a $Y$:
    \begin{equation*}
        \Var[X\mid Y] := E[(X - E[X\mid Y])^2\mid Y]
    \end{equation*}
\end{definicion}

Veamos ahora algunas de sus propiedades.
\begin{prop}
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Si $\exists E[X^2]$, entonces:
    \begin{enumerate}
        \item $\exists \Var[X\mid Y]$ y $\Var[X\mid Y] \geq 0$.
        \item $\Var[X\mid Y] = E[X^2\mid Y] - (E[X\mid Y])^2$.
    \end{enumerate}
\end{prop}
\begin{proof}
    Demostramos cada uno de los apartados:
    \begin{enumerate}
        \item Dado que $\exists E[X^2]$, entonces $\exists E[X^2\mid Y]$. Como la existencia de momentos no centrados implica la de momentos centrados, entonces $\exists E[(X - E[X\mid Y])^2\mid Y] = \Var[X\mid Y]$.
        
        Por otro lado, como $E[(X - E[X\mid Y])^2\mid Y]$, es la esperanza condicionada de una variable aleatoria no negativa, entonces $\Var[X\mid Y] \geq 0$.

        \item Partiendo de la definición de varianza condicionada, fijado $Y = y$, tenemos:
        \begin{align*}
            &\Var[X\mid Y=y] = E[(X - E[X\mid Y = y])^2\mid Y = y]
            =\\&= E[X^2 - 2XE[X\mid Y = y] + (E[X\mid Y = y])^2\mid Y = y]
            \AstIg\\&\AstIg E[X^2\mid Y = y] - 2E[\red{E[X\mid Y = y]}\cdot X\mid Y=y] + E[\red{(E[X\mid Y = y])^2}\mid Y = y]
            \stackrel{(\ast\ast)}{=}\\&\stackrel{(\ast\ast)}{=} E[X^2\mid Y = y] - 2E[X\mid Y = y]\cdot E[X\mid Y=y] + (E[X\mid Y = y])^2
            =\\&= E[X^2\mid Y = y] - 2(E[X\mid Y = y])^2 + (E[X\mid Y = y])^2
            =\\&= E[X^2\mid Y = y] - (E[X\mid Y = y])^2
        \end{align*}
        donde en $(\ast)$ hemos usado la linealidad de la esperanza condicionada, y en $(\ast\ast)$ hemos usado que, como ya hemos condicionado a $Y=y$, $\red{E[X\mid Y = y]}$ es una constante.
    \end{enumerate}
\end{proof}

Introducimos además esta última proposición, que cobrará gran importancia en la siguiente sección.
\begin{prop}[Descomposición de la varianza]
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Si $\exists E[X^2]$, entonces se tiene que $\exists \Var[E[X\mid Y]]$ y $\exists E[\Var[X\mid Y]]$, y se cumple que:
    \begin{equation*}
        \Var[X] = \Var[E[X\mid Y]] + E[\Var[X\mid Y]]
    \end{equation*}
\end{prop}
\begin{proof}
    En primer lugar, teneos que $\exists E[X^2]$ implica $\exists E[E[X^2\mid Y]]$. Por tanto, del segundo apartado de la proposición anterior, tenemos que:
    \begin{equation*}
        0\leq \Var[X\mid Y] = E[X^2\mid Y] - (E[X\mid Y])^2
        \Longrightarrow 0\leq (E[X\mid Y])^2 \leq E[X^2\mid Y]
    \end{equation*}
    Por tanto, como $(E[X\mid Y])^2$ es una variable aleatoria acotada por variables aleatorias con esperanza, entonces $\exists E[(E[X\mid Y])^2]$. Tenemos por tanto, que:
    \begin{itemize}
        \item $\exists E[(E[X\mid Y])^2]\Longrightarrow \exists \Var[E[X\mid Y]] = E[(E[X\mid Y])^2] - (E[E[X\mid Y]])^2$.
        \item $\exists E[(E[X\mid Y])^2]$ y $\exists E[E[X^2\mid Y]]$, lo que implica que:
        \begin{equation*}
            \exists E[E[X^2\mid Y]] - E[(E[X\mid Y])^2] = E[E[X^2\mid Y] - (E[X\mid Y])^2] = E[\Var[X\mid Y]]
        \end{equation*}
    \end{itemize}

    Por tanto, hemos demostrado las dos existencias. Para demostrar la igualdad, tenemos que:
    \begin{align*}
        \Var[E[X\mid Y]] &+ E[\Var[X\mid Y]]
        =\\&= \cancel{E[(E[X\mid Y])^2]} - (E[E[X\mid Y]])^2 + E[E[X^2\mid Y]]-\cancel{E[(E[X\mid Y])^2] }
        =\\&= - (E[X])^2 + E[X^2] =\\&= \Var[X]
    \end{align*}
    Como queríamos demostrar.
\end{proof}

\section{Regresión Mínimo Cuadrática}

Explicamos de nuevo el problema de regresión, que ya se introdujo para Estadística Descriptiva. Dadas dos variables aleatorias, $X$ e $Y$, definidas sobre el mismo espacio de probabilidad, el problema de regresión de $Y$ sobre $X$ consiste en determinar una función de $X$ que proporcione una representación, lo más precisa posible, de la variable $Y$. Esto es, se trata de aproximar la variable $Y$ por una función de $X$, de manera que la aproximación sea óptima en algún sentido preestablecido.
\begin{equation*}
    Y\approx \varphi(X)
\end{equation*}
donde:
\begin{itemize}
    \item $Y$ es la variable dependiente, explicada o endógena.
    \item $X$ es la variable independiente, explicativa o exógena.
    \item $\varphi$ es la función de regresión.
\end{itemize}

El criterio de optimalidad más usual para abordar el problema de regresión es el basado en el principio de mínimos cuadrados, y consiste en encontrar la función que minimiza la media de las desviaciones cuadráticas de las aproximaciones respecto de los verdaderos valores de la variable aproximada. Esto es, se trata de encontrar una función, $\varphi$, que minimice $E[(Y - \varphi(X))^2]$.

\begin{definicion}[Error cuadrático medio]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, el error cuadrático medio asociado a la función de regresión $\varphi$ de forma que $Y\approx \varphi(X)$ es:
    \begin{equation*}
        \text{E.C.M.}(\varphi) = E[(Y - \varphi(X))^2]
    \end{equation*}
\end{definicion}

Por tanto, el problema de regresión abordado bajo esta perspectiva consiste en encontrar una función, $\varphi_{\text{opt}}$, que minimice el E.C.M.

\subsection{Búsqueda de la función de regresión óptima, $\varphi_{\text{opt}}$}

En la presente subsección, suponemos que estamos buscando aproximar $Y$ por una función de $X$, $\varphi(X)$, y que hemos decidido que la función de regresión óptima es aquella que minimiza el E.C.M. asociado a la aproximación. 
\begin{observacion}
    Se deja para el lector generalizar el caso para aproximar $X$ por una función de $Y$.
\end{observacion}

Así, el problema de regresión se reduce a encontrar la función de regresión óptima, $\varphi_{\text{opt}}$, que minimiza el E.C.M. El siguiente teorema no se demostrará por ser un resultado que excede los conocimientos de la asignatura, pero nos proporciona la solución al problema de regresión bajo el criterio de mínimos cuadrados.
\begin{teo}
    Sea $Y$ una variable aleatoria sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$, y $X$ una variable aleatoria sobre el mismo espacio de probabilidad. Entonces, $E[Y\mid X]$ minimiza el E.C.M. Es decir:
    \begin{equation*}
        \varphi_{\text{opt}}(X) = E[Y\mid X]
    \end{equation*}

    Además, esta función de regresión óptima es única.
\end{teo}

El E.C.M. asociado a la función de regresión óptima, que es el mínimo error cuadrático medio cometido al aproximar $Y$ a partir de $X$, es:
\begin{align*}
    \text{E.C.M.}(\varphi_{\text{opt}}) &=
    \text{E.C.M.}(E[Y\mid X]) = E[(Y - E[Y\mid X])^2]
    =\\&=
    E[E[(Y - E[Y\mid X])^2\mid X]] = E[\Var[Y\mid X]]
\end{align*}

Considerando la descomposición de la varianza, tenemos:
\begin{equation*}
    \Var[Y]=\Var[E[Y\mid X]] + E[\Var[Y\mid X]]
    = \Var[\varphi_{\text{opt}}(X)] + \text{E.C.M.}(\varphi_{\text{opt}})
\end{equation*}

% Ejemplo 1
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T08_ProblemaRegresion.pdf#[0,{%22name%22:%22Fit%22}]

\begin{definicion}[Curva de regresión mínimo cuadrática]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, la curva de regresión mínimo cuadrática de $Y$ sobre $X$ es la curva obtenida empleando $\varphi_{\text{opt}}(X)$. Es decir:
    \begin{itemize}
        \item Curva de Regresión Mínimo Cuadrática de $Y$ sobre $X$:
        \begin{equation*}
            \wh{Y}(x)=E[Y\mid X=x]\qquad \forall x\in E_X
        \end{equation*}

        \item Curva de Regresión Mínimo Cuadrática de $X$ sobre $Y$:
        \begin{equation*}
            \wh{X}(y)=E[X\mid Y=y] \qquad \forall y\in E_Y
        \end{equation*}
    \end{itemize}
\end{definicion}

Veamos algunos casos particulares, para los que antes debemos introducir las siguientes definiciones.
\begin{definicion}[Dependencia funcional]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, se dice que $Y$ depende funcionalmente de $X$ si existe una función $f:E_X\to E_Y$ tal que:
    \begin{equation*}
        Y = f(X) \qquad \forall x\in E_X
    \end{equation*}

    Dicha función $f$ se denomina curva de dependencia.
\end{definicion}
\begin{prop}
    Sean $X,Y$ dos variables aleatorias sobre el mismo espacio de probabilidad.
    \begin{equation*}
        \Var[Y\mid X] = 0 \Longleftrightarrow Y \text{ depende funcionalmente de } X
    \end{equation*}
\end{prop}
\begin{proof}
    Demostramos mediante la doble implicación:
    \begin{description}
        \item[$\Longrightarrow$)] Partimos de:
        \begin{equation*}
            \Var[Y\mid X= x] = 0\qquad \forall x\in E_X
        \end{equation*}
    
        Por tanto, fijado $x\in E_X$, $\exists!~c_x\in E_y$ tal que:
        \begin{equation*}
            [Y\mid X=x] = c_x
        \end{equation*}
    
        Por tanto, definimos la función $f:E_X\to E_Y$ como:
        \begin{equation*}
            f(x) = c_x\qquad \forall x\in E_X
        \end{equation*}

        Por tanto, $Y$ depende funcionalmente de $X$.

        \item[$\Longleftarrow$)] Partimos de que $Y$ depende funcionalmente de $X$, es decir, $\exists f:E_X\to E_Y$ tal que:
        \begin{equation*}
            Y = f(X)
        \end{equation*}

        Entonces, tenemos que:
        \begin{align*}
            \Var[Y\mid X=x] &= \Var[f(X)\mid X=x] =\\&= \Var[f(x)\mid X=x] = 0\qquad \forall x\in E_X
        \end{align*}
        donde hemos usado que la varianza de una constante es 0.
    \end{description}
\end{proof}

\begin{definicion}[Dependencia recíproca]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, se dice que hay dependencia recíproca entre $X$ e $Y$ si $Y$ depende funcionalmente de $X$ y $X$ depende funcionalmente de $Y$ con la misma función. Es decir, existe una función $f:E_X\to E_Y$ tal que:
    \begin{align*}
        Y = f(X) \qquad \forall x\in E_X\\
        X = f^{-1}(Y) \qquad \forall y\in E_Y
    \end{align*}

    Dicha función $f$ se denomina curva de dependencia.
\end{definicion}
\begin{coro}
    Sean $X,Y$ dos variables aleatorias sobre el mismo espacio de probabilidad.
    \begin{equation*}
        \Var[Y\mid X] = \Var[X\mid Y] = 0 \Longleftrightarrow Y \text{ y } X \text{ dependen recíprocamente}
    \end{equation*}
\end{coro}
\begin{proof}
    Demostramos mediante doble implicación:
    \begin{description}
        \item[$\Longrightarrow$)] Como corolario de la proposición anterior, sabemos que $\exists f:E_X\to E_Y$, $g:E_Y\to E_X$ tales que:
        \begin{align*}
            Y = f(X) \qquad \forall x\in E_X\\
            X = g(Y) \qquad \forall y\in E_Y
        \end{align*}

        Sea $c_x\in E_y$, y consideramos su preimagen
        $$f^{-1}(c_x) = \{x\in E_X\mid f(x) = c_x\}= \{x\in E_X\mid Y = c_x\}$$

        No obstante, usando la demostración de la proposición anterior tenemos que dicho $c_x$ es único. Por tanto, $f^{-1}=g$ y, por tanto, $Y$ y $X$ dependen recíprocamente.
        
        \item[$\Longleftarrow$)] Se tiene de forma directa como corolario de la proposición anterior.
    \end{description}
\end{proof}


Algunos casos particulares son:
\begin{itemize}
    \item Si $Y$ depende funcionalmente de $X$ con curva de dependencia $Y=f(X)$, entonces la curva de regresión mínimo cuadrática de $Y$ sobre $X$ es la propia curva de dependencia:
    \begin{equation*}
        y = f(x) = E[Y\mid X=x] \qquad \forall x\in E_X
    \end{equation*}
    \begin{proof}
        Dado que $Y=f(X)$, entonces:
        \begin{equation*}
            E[Y\mid X=x] = E[f(X)\mid X=x] = E[f(x)\mid X=x] = f(x)\qquad \forall x\in E_X
        \end{equation*}
    \end{proof}

    \item Si hay dependencia recíproca entre $X$ e $Y$, es decir, $Y=f(X)$ y $X=f^{-1}(Y)$, entonces ambas curvas de regresión mínimo cuadrática coinciden con las curvas de dependencia:
    \begin{align*}
        y = f(x) = E[Y\mid X=x] \qquad \forall x\in E_X\\
        x = f^{-1}(y) = E[X\mid Y=y] \qquad \forall y\in E_Y
    \end{align*}

    \item Si $X$ e $Y$ son independientes, entonces la curva de regresión mínimo cuadrática de $Y$ sobre $X$ es la esperanza de $Y$, y la de $X$ sobre $Y$ es la esperanza de $X$:
    \begin{align*}
        \wh{Y}(x) = E[Y] \qquad \forall x\in E_X\\
        \wh{X}(y) = E[X] \qquad \forall y\in E_Y
    \end{align*}
    Como vemos, estas curvas de regresión mínimo cuadrática son constantes y paralelas a los ejes, lo que muestra que no tiene sentido plantear un problema de regresión en este caso.
    \begin{proof}
        Dado que $X$ e $Y$ son independientes, sabemos de forma directa que:
        \begin{align*}
            E[Y\mid X=x] &= E[Y] \qquad \forall x\in E_X\\
            E[X\mid Y=y] &= E[X] \qquad \forall y\in E_Y
        \end{align*}
    \end{proof}
\end{itemize}

Por tanto, a modo de resumen, en la Tabla~\ref{tab:resumen_regresion} se recogen las predicciones de $Y$ según lo que estemos observando.
\begin{table}[H]
    \centering
    \begin{tabular}{>{\centering\arraybackslash}m{3cm} |>{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm} >{\centering\arraybackslash}m{3cm}}
        \toprule
        & Sin observar $X$ & Observando $X$ & Para $X=x$\\
        \midrule
        $\wh{Y}$ & $E[Y]$ & $E[Y\mid X]$ & $E[Y\mid X=x]$\\
        E.C.M. & $\Var[Y]$ & $\Var[Y\mid X]$ & $\Var[Y\mid X=x]$\\
        \bottomrule
    \end{tabular}
    \caption{Resumen de las predicciones de $Y$ sobre en función de lo que se observe.}
    \label{tab:resumen_regresion}
\end{table}


\subsection{Razones de Correlación}

Buscamos ahora estudiar el grado de bondad de la aproximación mínimo cuadrática de cada variable a partir de la otra. Partimos de la siguiente igualdad:
\begin{equation*}
    Y = E[Y\mid X] + (Y - E[Y\mid X])
\end{equation*}

Sabemos que $\wh{Y}=E[Y\mid X]$ es la mejor aproximación que hemos obtenido. Vemos por tanto que el error es la variable aleatoria $(Y - E[Y\mid X])$ unidades, que denominaremos \emph{residuo}.
\begin{definicion}[Residuo]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, el residuo asociado a la aproximación de $Y$ a partir de $X$ es la siguiente variable aleatoria:
    \begin{equation*}
        R = Y - E[Y\mid X]
    \end{equation*}
\end{definicion}

La aproximación será mejor cuanto menor sea $|R|$. Esta es una variable aleatoria, cuya comparación para estudiar la bondad no es sencilla. Usaremos por tanto valores numéricos que resuman la bondad de la aproximación. 

Como primer intento, buscamos comparar mediante $E[R]$. Tenemos que:
\begin{align*}
    E[R] &= E[Y - E[Y\mid X]] = E[Y] - E[E[Y\mid X]] = E[Y] - E[Y] = 0
\end{align*}
Por tanto, siempre será nulo, lo que no nos aporta información sobre la bondad de la aproximación. Buscamos por tanto comparar mediante la varianza de $R$, conocida como \emph{varianza residual}.
\begin{definicion}[Varianza residual]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, la varianza residual asociada a la aproximación de $Y$ a partir de $X$ es varianza del residuo, es decir:
    \begin{equation*}
        \Var[R] = \Var[Y - E[Y\mid X]]
    \end{equation*}
\end{definicion}

Calculemos el valor de la varianza residual.
\begin{align*}
    \Var[R] &= \Var[Y - E[Y\mid X]] = E[(Y - E[Y\mid X])^2] - (E[Y - E[Y\mid X])^2 =\\
    &= E[(Y - E[Y\mid X])^2] - \cancel{E[R]^2} = E[(Y - E[Y\mid X])^2]
    =\\&= E[E[(Y - E[Y\mid X])^2\mid X]] = E[\Var[Y\mid X]] = \text{E.C.M.}(\varphi_{\text{opt}})
\end{align*}

Por tanto, usando la descomposición de la varianza, tenemos que:
\begin{equation}\label{eq:varianza_residual}
    \Var[Y] = \Var[E[Y\mid X]] + E[\Var[Y\mid X]] = \Var\left[\wh{Y}\right] + \Var[R]
    = \Var\left[\wh{Y}\right] + \text{E.C.M.}(\varphi_{\text{opt}})
\end{equation}

Vemos que el $\text{E.C.M.}(\varphi_{\text{opt}})$ será menor conforme mayor sea $\Var\left[\wh{Y}\right]$, por lo que la aproximación será mejor conforme mayor sea $\Var\left[\wh{Y}\right]=\Var[E[Y\mid X]]$. No obstante, usar este valor para comparar la bondad de la aproximación introduce los siguientes problemas:
\begin{itemize}
    \item $\Var\left[E[Y\mid X]\right]$ no es adimensional, por lo que no es un valor que podamos comparar.
    \item No es invariante frente a cambios de escala, lo cual puede llevar a conclusiones engañosas. Por ejemplo, sean:
    \begin{itemize}
        \item $Y$ una variable aleatoria que mide la altura de una persona en metros.
        \item $Y'$ una variable aleatoria que mide la altura de una persona en centímetros.
    \end{itemize}
    Entonces, es razonable pensar que cualquier variable $X$ debe aproximar igual de bien a $Y$ que a $Y'=100$. No obstante, tenemos que:
    \begin{equation*}
        \Var\left[E[Y'\mid X]\right] = \Var\left[E[100Y\mid X]\right] = \Var\left[100E[Y\mid X]\right] = 100^2\Var\left[E[Y\mid X]\right]
    \end{equation*}
    De esta forma, midiendo la bondad de la aproximación por su varianza sin tener en cuenta la unidad de medida de las variables aproximadas, podríamos concluir que la variable $X$ aproxima mucho mejor a $Y'$ que a $Y$.
\end{itemize}

Estos inconvenientes se salvan normalizando la varianza de la función de regresión y usando el siguiente coeficiente, que es el que buscábamos presentar en esta sección.
\begin{definicion}[Razón de correlación]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, se define la razón de correlación de $Y$ sobre $X$:
    \begin{equation*}
        \eta_{Y/X}^2 = \frac{\Var\left[E[Y\mid X]\right]}{\Var[Y]}
    \end{equation*}

    De forma análoga se define la razón de correlación de $X$ sobre $Y$:
    \begin{equation*}
        \eta_{X/Y}^2 = \frac{\Var\left[E[X\mid Y]\right]}{\Var[X]}
    \end{equation*}
\end{definicion}

Usando la Ecuación~\ref{eq:varianza_residual}, tenemos:
\begin{equation*}
    \eta_{Y/X}^2 = \frac{\Var\left[E[Y\mid X]\right]}{\Var[Y]} = \dfrac{\Var[Y]-E[\Var[Y\mid X]]}{\Var[Y]} = 1 - \dfrac{E[\Var[Y\mid X]]}{\Var[Y]}
\end{equation*}

Veamos que $\eta_{Y/X}^2$ solventa los dos problemas que presentábamos al usar $\Var\left[E[Y\mid X]\right]$ para medir la bondad de la aproximación.
\begin{itemize}
    \item $\eta_{Y/X}^2$ es adimensional, ya que las unidades de medida de $\Var\left[E[Y\mid X]\right]$ y $\Var[Y]$ son las mismas.
    \item Veamos que es invariante frente a cambios de escala. Sean $Y$ e $Y'$ dos variables aleatorias, y $Y'=aY$ con $a\in\bb{R}$. Entonces:
    \begin{equation*}
        \eta_{Y'/X}^2 = \eta_{aY/X}^2 = \frac{\Var\left[E[aY\mid X]\right]}{\Var[aY]} = \frac{\Var\left[aE[Y\mid X]\right]}{a^2\Var[Y]} = \frac{a^2\Var\left[E[Y\mid X]\right]}{a^2\Var[Y]} = \eta_{Y/X}^2
    \end{equation*}
\end{itemize}

Por tanto, la razón de correlación es un valor adimensional e invariante frente a cambios de escala que nos permite comparar la bondad de la aproximación de una variable a partir de la otra. En particular, $\eta_{Y/X}^2$ mide la proporción de la varianza de $Y$ que queda explicada por la función de regresión $\wh{Y}=E[Y\mid X]$. En este sentido, se puede interpretar como una medida de la bondad del ajuste de la distribución a la curva de regresión correspondiente, de forma que \emph{a mayor valor del coeficiente, mejor será la aproximación}.

\begin{prop}
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Entonces:
    \begin{enumerate}
        \item $0\leq \eta_{Y/X}^2, \eta_{X/Y}^2 \leq 1$.
        \item $\eta_{Y/X}^2 = 0 \iff $ La curva de regresión de $Y$ sobre $X$ es $Y = E[Y]$.\\
        $\eta_{X/Y}^2 = 0 \iff $ La curva de regresión de $X$ sobre $Y$ es $X = E[X]$.
        \item $\eta_{Y/X}^2 = 1 \iff $ $Y$ depende funcionalmente de $X$.\\
        $\eta_{X/Y}^2 = 1 \iff $ $X$ depende funcionalmente de $Y$.
        \item $\eta_{Y/X}^2 = \eta_{X/Y}^2 = 1 \iff $ $X$ e $Y$ tienen dependencia recíproca.
    \end{enumerate}
\end{prop}
\begin{proof}
    Demostraremos tan solo los resultados para $\eta_{Y/X}^2$, ya que los resultados para $\eta_{X/Y}^2$ son análogos.
    \begin{enumerate}
        \item Tenemos en primer lugar  $\Var\left[E[Y\mid X]\right]\geq 0$ y $\Var[Y]\geq 0$ por ser varianzas. Por tanto lado, usamos que:
        \begin{equation*}
            \eta_{Y/X}^2 = 1-\dfrac{E[\Var[Y\mid X]]}{\Var[Y]}
        \end{equation*}
        Tenemos que $\Var[Y\mid X], \Var[Y]\geq 0$ por ser varianzas. Además, como la esperanza de una variable aleatoria no negativa es no negativa, tenemos que $E[\Var[Y\mid X]]\geq 0$. Por tanto, $\eta_{Y/X}^2\leq 1$ y tenemos lo buscado.

        \item Tenemos que:
        \begin{equation*}
            \eta_{Y/X}^2 = 0 \iff \Var\left[E[Y\mid X]\right] = 0 \iff \exists c\in \bb{R} \text{ tal que } E[Y\mid X] = c\quad \forall x\in E_X
        \end{equation*}
        donde la última implicación se debe a las propiedades de la varianza.
        Por tanto, tenemos que:
        \begin{equation*}
            c = E[c] = E[E[Y\mid X]] = E[Y]
        \end{equation*}

        Por tanto, tenemos que:
        \begin{equation*}
            \eta_{Y/X}^2 = 0 \iff E[Y\mid X] = E[Y]
        \end{equation*}

        \item Tenemos que:
        \begin{align*}
            \eta_{Y/X}^2 = 1 &\iff E[\Var[Y\mid X]] = 0 \iff P[\Var[Y\mid X] = 0] = 1
            \iff\\&\iff
            \Var[Y\mid X] = 0
        \end{align*}

        Por tanto, tenemos que $Y$ depende funcionalmente de $X$.

        \item Por lo razonado anteriormente, tenemos que:
        \begin{equation*}
            \eta_{Y/X}^2 = \eta_{X/Y}^2 = 1 \iff \Var[Y\mid X] = 0 = \Var[X\mid Y]
        \end{equation*}
        Por tanto, $X$ e $Y$ tienen dependencia recíproca.
    \end{enumerate}
\end{proof}

En esta última proposición, vemos que nos falta estudiar el caso en el que $\eta_{Y/X}^2 = \eta_{X/Y}^2 = 0$. En este caso, tenemos que ambas curvas de regresión son las esperanzas, y ninguna de las variables explica a la otra. Introducimos por tanto el siguiente concepto:
\begin{definicion}[Incorrelación]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, se dice que $X$ e $Y$ son incorreladas si $$\eta_{Y/X}^2 = \eta_{X/Y}^2 = 0.$$
\end{definicion}

Como consecuencia de lo anteriormente visto, tenemos que si $X$ e $Y$ son independientes, entonces son incorreladas. No obstante, el recíproco no es cierto, como muestra el Ejercicio~\ref{ej:4.7}.


\section{Rectas de Regresión}

Cuando el cálculo de la esperanza condicionada es complicado y se tiene relativa seguridad de que los puntos de la distribución teórica se ajustan a una determinada forma funcional (exponencial, parabólica, etc.) puede ser útil restringir la búsqueda de la función de regresión óptima a la clase de funciones de dicha forma.

Un caso particularmente importante es el de la regresión lineal. Las rectas que mejor se ajustan a los puntos de la distribución en el sentido de mínimos cuadrados (para aproximar cada una de las variables en términos de la otra) se denominan rectas de regresión.

\begin{observacion}
    Supongamos de nuevo que queremos obtener la recta de regresión de $Y$ sobre $X$, es decir, $Y=\varphi(X)$.
\end{observacion}

En estos casos, tenemos que $\varphi$ ha de ser una recta, por lo que:
\begin{equation*}
    \varphi(X) = aX + b\qquad a,b\in\bb{R}
\end{equation*}

Por tanto, razonando de forma idéntica a como hicimos en la sección anterior, tenemos que el problema de regresión se reduce a encontrar los valores de $a$ y $b$ que minimizan el E.C.M. asociado a la aproximación. Es decir, la recta de regresión de $Y$ sobre $X$ es:
\begin{equation*}
    \varphi_{\text{opt}}^L(X) = \min_{a,b\in\bb{R}} E[(Y - aX - b)^2]
\end{equation*}

El problema de regresión lineal se reduce por tanto a minimizar la siguiente función de dos variables:
\begin{align*}
    L(a,b) &= E[(Y - aX - b)^2]
    = E[Y^2] + a^2E[X^2] + b^2 - 2aE[XY] - 2bE[Y] + 2abE[X]
\end{align*}

Para calcular el mínimo de $L$, derivamos parcialmente e igualamos a 0:
\begin{align*}
    \frac{\partial L}{\partial a} &= 2aE[X^2] - 2E[XY] + 2bE[X] = 0\\
    \frac{\partial L}{\partial b} &= 2b - 2E[Y] + 2aE[X] = 0 \Longrightarrow
    2b = 2E[Y] - 2aE[X]
\end{align*}

Por tanto, suponiendo $\Var[X]> 0$ (caso que estudiaremos más adelante), tenemos que:
\begin{align*}
    2aE[X^2] - 2E[XY] &+ (2E[Y] - 2aE[X])E[X] = 0
    \Longrightarrow  \\ &\Longrightarrow
    a(E[X^2] - E[X]^2) = E[XY] - E[X]E[Y]
    \Longrightarrow \\ &\Longrightarrow
    a = \dfrac{\Cov(X,Y)}{\Var[X]}
    \Longrightarrow  \\ &\Longrightarrow
    b = E[Y] - \dfrac{\Cov(X,Y)}{\Var[X]} \cdot E[X]
\end{align*}
que, como se comprueba calculando la matriz de derivadas segundas\footnote{Minimización de funciones en varias variables es materia de Análisis Matemático II.}, proporciona el mínimo de la función $L$:
\begin{equation*}
    \begin{vmatrix}
        \dfrac{\partial^2 L}{\partial a^2} & \dfrac{\partial^2 L}{\partial a\partial b}\\ \\
        \dfrac{\partial^2 L}{\partial b\partial a} & \dfrac{\partial^2 L}{\partial b^2}
    \end{vmatrix} =
    \begin{vmatrix}
        2E[X^2] & 2E[X]\\
        2E[X] & 2
    \end{vmatrix} = 4(E[X^2] - E[X]^2) = 4\Var[X] > 0
\end{equation*}

Por tanto, tenemos que la recta de regresión de $Y$ sobre $X$ es:
\begin{equation*}
    \wh{Y}=\varphi_{\text{opt}}^L(X) = E[Y] + \dfrac{\Cov(X,Y)}{\Var[X]}(X - E[X])
\end{equation*}

Este resultado refuerza el resultado que ya vimos del signo de la covarianza:
\begin{itemize}
    \item Si $\Cov(X,Y)>0$, entonces las rectas de regresión son crecientes.
    \item Si $\Cov(X,Y)<0$, entonces las rectas de regresión son decrecientes.
    \item Si $\Cov(X,Y)=0$, entonces las rectas de regresión son constantes e iguales a la esperanza de la variable explicada.
\end{itemize}

Calculemos ahora su error cuadrático medio:
\begin{align*}
    &\text{E.C.M.}(\varphi_{\text{opt}}^L)
    = E[(Y - \varphi_{\text{opt}}^L(X))^2]
    = E\left[\left(Y - E[Y] - \dfrac{\Cov(X,Y)}{\Var[X]}(X - E[X])\right)^2\right]
    =\\&=
    E\left[
        (Y-E[Y])^2 - 2\dfrac{\Cov(X,Y)}{\Var[X]}(Y-E[Y])(X-E[X]) + \dfrac{\Cov^2(X,Y)}{\Var^2[X]}(X-E[X])^2
    \right]=\\
    &= E[(Y-E[Y])^2] - 2\dfrac{\Cov(X,Y)}{\Var[X]}E[(Y-E[Y])(X-E[X])] + \dfrac{\Cov^2(X,Y)}{\Var^2[X]}E[(X-E[X])^2]
    =\\
    &= \Var[Y] - 2\dfrac{\Cov(X,Y)}{\Var[X]}\cdot \Cov(X,Y) + \dfrac{\Cov^2(X,Y)}{\Var^2[X]}\cdot \Var[X] =\\
    &= \Var[Y] - \dfrac{\Cov^2(X,Y)}{\Var[X]}
\end{align*}

Veamos ahora el caso particular en el que $\Var[X]=0$. Entonces, $X=k\in \bb{R}$ es una variable degenerada (constante). Por tanto, cualquier función lineal de $X$ es constante, por lo que el problema de regresión lineal se reduce a encontrar la constante que minimiza $E[(Y - c)^2]$. Dicha constante es la esperanza de $Y$:
\begin{equation*}
    \varphi_{\text{opt}}^L(X) = E[Y]
\end{equation*}
De aquí en adelante, supondremos que $\Var[X]>0$ cuando estemos trabajando con la regresión lineal de $Y$ sobre $X$, y análogamente para la regresión lineal de $X$ sobre $Y$. 

\subsection{Coeficiente de determinación lineal}

Veamos ahora el equivalente a las razones de correlación en el caso de la regresión lineal. Para calcularlo, seguimos el mismo razonamiento que en la sección anterior, y este será:
\begin{equation*}
    \dfrac{\Var[\varphi_{\text{opt}}^L(X)]}{\Var[Y]}
\end{equation*}

Calculemos dicha varianza:
\begin{align*}
    \Var[\varphi_{\text{opt}}^L(X)] &= \Var\left[E[Y] + \dfrac{\Cov(X,Y)}{\Var[X]}(X - E[X])\right]
    =\\&=
    \dfrac{\Cov^2(X,Y)}{\Var^2[X]}\cdot \Var[X]= \dfrac{\Cov^2(X,Y)}{\Var[X]}
\end{align*}

Por tanto, tenemos que:
\begin{equation*}
    \dfrac{\Var[\varphi_{\text{opt}}^L(X)]}{\Var[Y]} = \dfrac{\Cov^2(X,Y)}{\Var[X]\cdot \Var[Y]}
\end{equation*}

Introducimos entonces la siguiente definición:
\begin{definicion}[Coeficiente de determinación lineal]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, se define el coeficiente de determinación lineal de $Y$ sobre $X$, notado como $\rho^2_{Y/X}$, como:
    \begin{equation*}
        \rho_{Y/X}^2 = \dfrac{\Cov^2(X,Y)}{\Var[X]\cdot \Var[Y]}
    \end{equation*}
\end{definicion}
\begin{observacion}
    Notemos que este coeficiente coincide con el producto de las pendientes de las rectas de regresión de $Y$ sobre $X$ y de $X$ sobre $Y$. Esto nos será muy útil en la práctica.
\end{observacion}

Como primer resultado directo que se deduce de la definición, tenemos que:
\begin{equation*}
    \rho_{Y/X}^2 = \rho_{X/Y}^2
\end{equation*}

Por tanto, tenemos que ambos coeficientes de correlación son iguales, lo que nos permite referirnos simplemente a $\rho^2_{Y/X}$ o $\rho^2_{X/Y}$ como coeficiente de determinación lineal.

Al igual que en el caso de las razones de correlación, el coeficiente de determinación lineal es un valor adimensional e invariante frente a cambios de escala que nos permite comparar la bondad de la aproximación de una variable a partir de la otra. En este sentido, se puede interpretar como una medida de la bondad del ajuste de la distribución a la recta de regresión correspondiente, de forma que \emph{a mayor valor del coeficiente, mejor será la aproximación}.

Como resultados, incluimos las siguientes propiedades del coeficiente de determinación lineal:
\begin{prop}
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Entonces:
    \begin{enumerate}

        \item $\rho_{aX+b,cY+d}^2 = \rho_{X,Y}^2$ para $a,b,c,d\in\bb{R}$.
        \item $0\leq \rho_{Y/X}^2 \leq 1$.
        \item Tenemos que:
        \begin{align*}
            0 = \rho_{Y/X}^2 &\iff \text{La recta de regresión de } Y \text{ sobre } X \text{ es } Y = E[Y]\\
            & \iff \text{La recta de regresión de } X \text{ sobre } Y \text{ es } X = E[X]
        \end{align*}

        \item $1=\rho_{Y/X}^2 \iff$ $X$ y $Y$ tienen dependencia funcional lineal recíproca.
        
        \item $\rho_{Y/X}^2 \leq \eta_{Y/X}^2,~\eta_{X/Y}^2$.
        
        \item $\rho_{Y/X}^2 = \eta_{Y/X}^2 \iff$ La curva de regresión de $Y$ sobre $X$ coincide con la recta de regresión de $Y$ sobre $X$.\\
        $\rho_{Y/X}^2 = \eta_{X/Y}^2 \iff$ La curva de regresión de $X$ sobre $Y$ coincide con la recta de regresión de $X$ sobre $Y$.
    \end{enumerate}
\end{prop}
% https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T08_CoeficienteDeterminacion.pdf#[0,{%22name%22:%22Fit%22}]

\subsection{Coeficiente de correlación lineal de Pearson}

El coeficiente de determinación lineal es una medida de la bondad de la aproximación de una variable a partir de la otra, pero no nos da información sobre la dirección de la relación entre las variables. Para ello, introducimos el siguiente concepto:
\begin{definicion}[Coeficiente de correlación lineal de Pearson]
    Dadas dos variables aleatorias $X$ e $Y$ sobre el mismo espacio de probabilidad, se define el coeficiente de correlación lineal de Pearson entre $Y$ y $X$, notado como $\rho_{Y/X}$, como:
    \begin{equation*}
        \rho_{Y/X} = \dfrac{\Cov(X,Y)}{\sqrt{\Var[X]\cdot \Var[Y]}}
    \end{equation*}
\end{definicion}

Notemos que $\rho_{Y/X}$ es la raíz cuadrada del coeficiente de determinación lineal empleado el signo de la covarianza. Debido a que el signo de $\rho_{Y/X}$ coincide con el signo de la covarianza, y este nos da la dirección de la relación entre las variables, el coeficiente de correlación lineal de Pearson nos da información sobre la dirección de la relación entre las variables.
\begin{itemize}
    \item Si $\rho_{Y/X}>0$, entonces las variables están positivamente correladas; es decir, si una de las variables aumenta, la otra también lo hace.
    \item Si $\rho_{Y/X}<0$, entonces las variables están negativamente correladas; es decir, si una de las variables aumenta, la otra disminuye.
    \item Si $\rho_{Y/X}=0$, entonces las rectas de regresión son constantes e iguales a las esperanzas de las variables dependientes. En este caso, se dice que son \emph{linealmente incorreladas}.
\end{itemize}

El siguiente resultado se deduce de forma directa de la definición del coeficiente de correlación lineal de Pearson y de las propiedades del coeficiente de determinación lineal:
\begin{prop}
    Sean $X$ e $Y$ dos variables aleatorias sobre el espacio de probabilidad $(\Omega, \mathcal{A}, P)$. Entonces:
    \begin{enumerate}
        \item $\rho_{Y/X} = \rho_{X/Y}$.
        \item $-1\leq \rho_{Y/X} \leq 1$.
        \item $\rho_{Y/X}$ es invariante frente a cambios de escala (salvo signo). Esto se suele notar como:
        \begin{equation*}
            \rho_{aX+b,cY+d} = \pm \rho_{X,Y}\qquad a,b,c,d\in\bb{R}
        \end{equation*}
    \end{enumerate}
\end{prop}