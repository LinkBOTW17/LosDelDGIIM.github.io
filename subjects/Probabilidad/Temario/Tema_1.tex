\chapter{Distribuciones de Probabilidad Continua}

En el presente capítulo, se estudiarán las distribuciones de probabilidad continua más importantes. Al igual que 
en la asignatura de EDIP se vieron para variables aleatorias discretas, en este tema se presentarán las más
relevantes para variables aleatorias continuas.

\section{Distribución Uniforme Continua}

\begin{definicion}[Distribución Uniforme Continua]
    Una variable aleatoria continua $X$ sigue una distribución uniforme en el intervalo $[a,b]$, con $a,b\in \bb{R}$, $a<b$, si su función de densidad
    toma un valor constante en dicho intervalo, siendo nula fuera de él.
    Lo denotaremos como $X\sim \cc{U}(a,b)$.
\end{definicion}

\begin{prop}
    Sea $X\sim \cc{U}(a,b)$, entonces su función de densidad es:
    \Func{f}{\bb{R}}{[0,1]}{x}{\frac{1}{b-a}}
\end{prop}
\begin{proof}
    Sea $f$ la función de densidad de $X$. Para que sea una función de densidad, debe cumplir:
    \begin{equation*}
        \int_{-\infty}^{\infty} f(x)dx = \int_{a}^{b} f(x)dx = 1
    \end{equation*}

    Como $f$ es constante en $[a,b]$, sea entonces $f(x) = k$ para $x\in [a,b]$. Entonces:
    \begin{equation*}
        \int_{a}^{b} f(x)dx =
        \int_{a}^{b} kdx = k\int_{a}^{b} dx = k(b-a) = 1 \Longrightarrow k = \frac{1}{b-a}
    \end{equation*}

    Por tanto, la función de densidad de $X$ es:
    \begin{equation*}
        f(x) = \frac{1}{b-a} \quad \forall x\in [a,b]
    \end{equation*}
\end{proof}

\begin{prop}
    Sea $X\sim \cc{U}(a,b)$, entonces su función de distribución es:
    \Func{F_X}{\bb{R}}{[0,1]}{x}{\begin{cases}
        0 & x < a\\
        \dfrac{x-a}{b-a} & x\in [a,b]\\
        1 & x > b
    \end{cases}}
\end{prop}
\begin{proof}
    Tenemos que:
    \begin{align*}
        F_X(x) = \int_{-\infty}^x f(t)~dt
        = \int_{a}^{x} \dfrac{1}{b-a}~dt
        = \dfrac{1}{b-a} \int_a^x ~dt
        = \dfrac{x-a}{b-a}
    \end{align*}
\end{proof}

Como consecuencia inmediata a la proposición anterior, vemos como definición alternativa
que, si $X$ es una variable aleatoria continua tal que $X\sim \cc{U}(a,b)$, entonces se tiene que
la probabilidad de que $X$ tome un valor en un intervalo $[c,d]$, con $a\leq c\leq d\leq b$, es
directamente proporcional a la longitud del intervalo.

\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \cc{U}(a,b)$. Su función generatriz de momentos es:
    \begin{equation*}
        M_X(t) = \dfrac{e^{tb} - e^{ta}}{(b-a)t} \quad t\neq 0
    \end{equation*}
    Para $t=0$, $M_X(0) = 1$.
\end{prop}
\begin{proof}
    Veamos en primer lugar el caso $t=0$. Aunque ya esté demostrado en el temario de EDIP, esto es una propiedad
    general de las funciones generatrices de momentos, ya que:
    \begin{equation*}
        M_X(0) = E\left[e^{0X}\right] = E[1] = 1
    \end{equation*}

    Para $t\neq 0$, tenemos que:
    \begin{align*}
        M_X(t) &= E\left[e^{tX}\right] = \int_{a}^{b} e^{tx} \dfrac{1}{b-a}~dx = \dfrac{1}{b-a} \int_{a}^{b} e^{tx}~dx =\\
        &= \dfrac{1}{b-a} \left[ \dfrac{e^{tx}}{t} \right]_{a}^{b} = \dfrac{e^{tb} - e^{ta}}{(b-a)t}
    \end{align*}
\end{proof}

Calculemos ahora los momentos de una variable aleatoria $X$ tal que $X\sim \cc{U}(a,b)$.
\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \cc{U}(a,b)$. Entonces, su momento no centrado de orden $k\in \bb{N}\cup \{0\}$ es:
    \begin{equation*}
        m_k = E[X^k] = \dfrac{b^{k+1} - a^{k+1}}{(k+1)(b-a)}
    \end{equation*}
\end{prop}
\begin{proof}
    Tenemos que:
    \begin{align*}
        m_k = E[X^k] &= \int_{a}^{b} x^k \dfrac{1}{b-a}~dx = \dfrac{1}{b-a} \int_{a}^{b} x^k~dx =\\
        &= \dfrac{1}{b-a} \left[ \dfrac{x^{k+1}}{k+1} \right]_{a}^{b} = \dfrac{b^{k+1} - a^{k+1}}{(k+1)(b-a)}
    \end{align*}
\end{proof}

Como consecuencia, tenemos que:
\begin{equation*}
    m_1 = E[X] = \dfrac{b^2 - a^2}{2(b-a)} = \dfrac{b+a}{2}
\end{equation*}

\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \cc{U}(a,b)$. Entonces, su momento centrado de orden $k\in \bb{N}$ es:
    \begin{equation*}
        \mu_k = \begin{cases}
            0 & k \text{ impar}\\
            \dfrac{(b-a)^k}{(k+1)2^k} & k \text{ par}
        \end{cases}
    \end{equation*}
\end{prop}
\begin{proof}
    Tenemos que:
    \begin{align*}
        \mu_k &= E[(X-m_1)^k] = E\left[\left(X-\dfrac{a+b}{2}\right)^k\right] = \int_{a}^{b} \left(x-\dfrac{a+b}{2}\right)^k \dfrac{1}{b-a}~dx =\\
        &= \dfrac{1}{b-a} \int_{a}^{b} \left(x-\dfrac{a+b}{2}\right)^k~dx
        = \dfrac{1}{b-a} \left[ \dfrac{\left(x-\dfrac{a+b}{2}\right)^{k+1}}{k+1} \right]_{a}^{b}
        =\\&= \dfrac{\left(b-\dfrac{a+b}{2}\right)^{k+1} - \left(a-\dfrac{a+b}{2}\right)^{k+1}}{(k+1)(b-a)}
        = \dfrac{\left(\dfrac{b-a}{2}\right)^{k+1} - \left(-\dfrac{b-a}{2}\right)^{k+1}}{(k+1)(b-a)}
    \end{align*}

    Distinguimos ahora en función de la paridad de $k$:
    \begin{itemize}
        \item Si $k$ es impar, entonces $\mu_k = 0$.
        \item Si $k$ es par, entonces $\mu_k = \dfrac{(b-a)^k}{(k+1)2^k}$.
    \end{itemize}
\end{proof}


Algunos ejemplos de su utilidad son los siguientes:
\begin{itemize}
    \item La distribución uniforme proporciona una representación adecuada para
    redondear las diferencias que surgen al medir cantidades físicas entre los
    valores observados y los reales.
    Por ejemplo, si el peso de una persona se redondea al $kg$ más cercano,
    entonces la diferencia entre el peso observado y el real será algún valor entre
    $-0.5$ y $0.5~kg$. Es común que el error de redondeo siga entonces una distribución
    $\cc{U}(-0.5,0.5)$.
    
    \item La generación de números aleatorios en un intervalo $[a,b]$ debe seguir una distribución
    $\cc{U}(a,b)$.
\end{itemize}


\section{Distribución Normal}

Esta es la distribución de probabilidad más importante en la Teoía de la Probabilidad y la Estadística Matemática.

\begin{definicion}[Distribución Normal]
    Una variable aleatoria continua $X$ sigue una distribución normal con parámetros $\mu\in \bb{R}$ y $\sigma^2 \in \bb{R}^+$, si su función de densidad es:
    \begin{equation*}
        f_X(x) = \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}, \quad x\in \bb{R}
    \end{equation*}
    donde $\sigma=+\sqrt{\sigma^2}$.
    Lo denotaremos como $X\sim \cc{N}(\mu,\sigma^2)$.
\end{definicion}

A pesar de darse como definición, hemos de demostrar que efectivamente es una función de densidad.
Para ello, incluimos el siguiente Lema, cuya demostración no se incluye por su complejidad,
al requerir de integración en varias variables con cambio a coordenadas polares.
\begin{lema}[Integral de Gauss]
    Sea $a,b\in \bb{R}^+$, entonces:
    \begin{equation*}
        \int_{-\infty}^{\infty} e^{-a(x+b)^2} \, dx = \sqrt{\dfrac{\pi}{a}}
    \end{equation*}
\end{lema}

Usando este lema, podemos demostrar que la función de densidad de la normal es efectivamente una función de densidad.
\begin{proof}
    La función $f_X$ debe cumplir las siguientes propiedades:
    \begin{itemize}
        \item $f_X(x)\geq 0$ para todo $x\in \mathbb{R}$.
        
        Esto es directo puesto que el término exponencial siempre es positivo.
        
        \item $\displaystyle \int_{-\infty}^{\infty} f_X(x) \, dx = 1$.
        \begin{align*}
            \int_{-\infty}^{\infty} f_X(x) \, dx &= \int_{-\infty}^{\infty} \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\dfrac{(x-\mu)^2}{2\sigma^2}} \, dx = \dfrac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^{\infty} e^{-\dfrac{(x-\mu)^2}{2\sigma^2}} \, dx =\\
            &= \MetInt{x=\sigma t +\mu}{\dfrac{dx}{dt} = \sigma} = \dfrac{1}{\sqrt{2\pi}\cancel{\sigma}} \int_{-\infty}^{\infty} e^{-\dfrac{t^2}{2}} \cdot \cancel{\sigma}\, dt \AstIg \dfrac{1}{\sqrt{2\pi}} \cdot \sqrt{2\pi} = 1
        \end{align*}
        donde en $(\ast)$ hemos aplicado la Integral de Gauss con $a=\nicefrac{1}{2}$ y $b=0$.
    \end{itemize}
\end{proof}


\begin{teo}[Tipificación de la Normal]
    Sea $X\sim \cc{N}(\mu,\sigma^2)$. Entonces, la variable aleatoria $Z = \dfrac{X-\mu}{\sigma}$ se dice que es la variable aleatoria tipificada de $X$.
    Se cumple que:
    \begin{enumerate}
        \item $Z\sim \cc{N}(0,1)$.
        \item $P[X\leq x] = P\left[Z\leq \dfrac{x-\mu}{\sigma}\right]$ para todo $x\in \bb{R}$.
    \end{enumerate}
\end{teo}
\begin{proof}
    Demostramos cada uno de los puntos:
    \begin{enumerate}
        \item Para esto, hay que emplear el Teorema de Cambio de Variable de
        variable aleatoria continua a variable aleatoria continua. Tenemos que $Re_X = \bb{R}$, y definimos por comodidad la siguiente función:
        \Func{g}{\bb{R}}{\bb{R}}{x}{\dfrac{x-\mu}{\sigma}}

        Tenemos por tanto que $Z=g(X)$, y como $g$ es biyectiva tenemos que $Re_Z = Re_X = \bb{R}$.
        La inversa de $g$ es:
        \Func{g^{-1}}{\bb{R}}{\bb{R}}{x}{\sigma x + \mu}

        La función de densidad de $Z$ es, por tanto:
        \begin{align*}
            f_Z(z) &= f_X(g^{-1}(z))\left|(g^{-1})(z)\right| = f_X(\sigma z + \mu)\cdot \sigma = \dfrac{1}{\sqrt{2\pi}\cancel{\sigma}}e^{-\dfrac{(\sigma z+\bcancel{\mu} -\bcancel{\mu})^2}{2\sigma^2}} \cdot \cancel{\sigma} =\\&= \dfrac{1}{\sqrt{2\pi}}e^{-\dfrac{z^2}{2}}
        \end{align*}

        Por tanto, identificando términos, tenemos que $Z\sim \cc{N}(0,1)$.

        \item Para demostrar esto, tenemos que:
        \begin{align*}
            P[X\leq x] &= \int_{-\infty}^{x} f_X(t)~dt = \int_{-\infty}^{x} \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\dfrac{(t-\mu)^2}{2\sigma^2}}~dt \\
            P\left[Z\leq \dfrac{x-\mu}{\sigma}\right] &= \int_{-\infty}^{\frac{x-\mu}{\sigma}} f_Z(t)~dt = \int_{-\infty}^{\frac{x-\mu}{\sigma}} \dfrac{1}{\sqrt{2\pi}} e^{-\dfrac{t^2}{2}}~dt
        \end{align*}

        Resolvamos la primera integral meidante el Cambio de Variable siguiente:
        \begin{equation*}
            \MetInt{t = \sigma u + \mu}{\dfrac{dt}{du} = \sigma} \qquad
            \left\{
                \begin{array}{l}
                    \text{Cuando } t = -\infty, u = -\infty\\
                    \text{Cuando } t = x, u = \dfrac{x-\mu}{\sigma}
                \end{array}
            \right.
        \end{equation*}

        Por tanto, tenemos que:
        \begin{align*}
            P[X\leq x] &= \int_{-\infty}^{x} \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\dfrac{(t-\mu)^2}{2\sigma^2}}~dt
            \AstIg \int_{-\infty}^{\frac{x-\mu}{\sigma}} \dfrac{1}{\sqrt{2\pi}\cancel{\sigma}} e^{-\dfrac{u^2}{2}}\cdot \cancel{\sigma} ~ du
            =\\&= \int_{-\infty}^{\frac{x-\mu}{\sigma}} \dfrac{1}{\sqrt{2\pi}} e^{-\dfrac{u^2}{2}}~du
            = P\left[Z\leq \dfrac{x-\mu}{\sigma}\right]
        \end{align*}
        donde en $(\ast)$ hemos empleado el cambio de variable.
    \end{enumerate}
\end{proof}

\begin{prop}
    Sea $X\sim \cc{N}(\mu,\sigma^2)$. Entonces, su función generatriz de momentos es:
    \begin{equation*}
        M_X(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}}
    \end{equation*}
\end{prop}
\begin{proof} Demostraremos este resultado en dos pasos:
    \begin{description}
        \item[Caso particular]  Demostramos en primer lugar el caso para la variable $Z\sim \cc{N}(0,1)$:
        \begin{align*}
            M_Z(t) &= E\left[e^{tZ}\right]
            = \int_\bb{R} e^{tz} \dfrac{1}{\sqrt{2\pi}} e^{-\frac{z^2}{2}}~dz
            = \dfrac{1}{\sqrt{2\pi}} \int_\bb{R} e^{tz-\frac{z^2}{2}}~dz
            = \dfrac{1}{\sqrt{2\pi}} \int_\bb{R} e^{-\frac{z^2-2tz + t^2-t^2}{2}}~dz
            =\\&= \dfrac{1}{\sqrt{2\pi}} \int_\bb{R} e^{-\frac{(z-t)^2}{2}}e^{\frac{t^2}{2}}~dz
            = \dfrac{e^{\frac{t^2}{2}}}{\sqrt{2\pi}} \int_\bb{R} e^{-\frac{(z-t)^2}{2}}~dz
        \end{align*}

        Debido a que la integral de una función de densidad de una variable aleatoria con distribución $\cc{N}(t,1)$ en todo $\bb{R}$ es 1, tenemos que:
        \begin{equation*}
            \int_\bb{R} \dfrac{1}{\sqrt{2\pi}} e^{-\frac{(z-t)^2}{2}}~dz = 1
        \end{equation*}

        Por tanto:
        \begin{equation*}
            M_Z(t) = e^{\frac{t^2}{2}}
        \end{equation*}

        \item[Caso general]  Demostramos ahora el caso general para $X\sim \cc{N}(\mu,\sigma^2)$. Tenemos:
        \begin{equation*}
            Z = \dfrac{X-\mu}{\sigma} \Longrightarrow X = \sigma Z + \mu
        \end{equation*}que
    
        Por tanto, tenemos que:
        \begin{align*}
            M_X(t) &= E\left[e^{tX}\right] = E\left[e^{t(\sigma Z + \mu)}\right] = E\left[e^{t\sigma Z}e^{t\mu}\right]
        \end{align*}
    
        Puesto que $e^{t\mu}$ es una constante, por la linealidad de la esperanza tenemos:
        \begin{equation*}
            M_X(t) = E\left[e^{t\sigma Z}e^{t\mu}\right] = e^{t\mu}E\left[e^{(t\sigma) Z}\right]
            = e^{t\mu}M_Z(t\sigma) = e^{t\mu}e^{\frac{(t\sigma)^2}{2}} = e^{t\mu + \frac{\sigma^2 t^2}{2}}
        \end{equation*}
    \end{description}
\end{proof}

Una vez ya razonada la función generatriz de momentos, podemos entonces entender por qué los parámetros de la normal son $\mu$ y $\sigma^2$.
Veamos en primer lugar que $\mu$ es la esperanza de la variable aleatoria ($E[X]$ se nota también como $\ol{X}$ o $\mu$):
\begin{prop}
    Sea $X\sim \cc{N}(\mu,\sigma^2)$. Entonces, su esperanza es:
    \begin{equation*}
        E[X] = \mu
    \end{equation*}
\end{prop}
\begin{proof}
    Tenemos que:
    \begin{align*}
        E[X] &= \dfrac{d}{dt} M_X(t) \Big|_{t=0} = \dfrac{d}{dt} e^{t\mu + \frac{\sigma^2 t^2}{2}} \Big|_{t=0}
        = e^{t\mu + \frac{\sigma^2 t^2}{2}}\left(\mu + \sigma^2 t\right) \Big|_{t=0} = e^{0}(\mu + 0) = \mu
    \end{align*}
\end{proof}

De igual forma, podemos ver que $\sigma^2$ es la varianza de la variable aleatoria:
\begin{prop}
    Sea $X\sim \cc{N}(\mu,\sigma^2)$. Entonces, su varianza es:
    \begin{equation*}
        \Var[X] = \sigma^2
    \end{equation*}
\end{prop}
\begin{proof}
    Calculemos en primer lugar $E[X^2]$ con la función generatriz de momentos:
    \begin{align*}
        E[X^2] &= \dfrac{d^2}{dt^2} M_X(t) \Big|_{t=0} = \dfrac{d^2}{dt^2} e^{t\mu + \frac{\sigma^2 t^2}{2}} \Big|_{t=0}
        = \dfrac{d}{dt} \left[e^{t\mu + \frac{\sigma^2 t^2}{2}}(\mu + \sigma^2 t)\right] \Big|_{t=0} =\\
        &= \left(e^{t\mu + \frac{\sigma^2 t^2}{2}}\left(\mu + \sigma^2 t\right)^2 + e^{t\mu + \frac{\sigma^2 t^2}{2}}\sigma^2\right) \Big|_{t=0}
        = e^{0}(\mu^2) + e^{0}\sigma^2 = \mu^2 + \sigma^2
    \end{align*}

    Tenemos por tanto, usando que $E[X] = \mu$:
    \begin{align*}
        \Var[X] &= E[X^2] - E[X]^2 = \mu^2 + \sigma^2 - \mu^2 = \sigma^2
    \end{align*}
\end{proof}

Una de las propiedades más importantes de la distribución normal es que es simétrica respecto a su media.
\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \cc{N}(\mu,\sigma^2)$. Entonces, es simétrica respecto a su media, es decir:
    \begin{equation*}
        P[X\leq \mu-x] = P[X\geq \mu+x] \quad \forall x\in \bb{R}
    \end{equation*}
\end{prop}
\begin{proof}
    Para probar esto, probaremos que su función de densidad es simétrica respecto a su media.
    Tenemos que:
    \begin{align*}
        f_X(\mu-x) &= \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\dfrac{((\mu-x)-\mu)^2}{2\sigma^2}}
        = \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\dfrac{(-x)^2}{2\sigma^2}}
        = \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\dfrac{x^2}{2\sigma^2}}
        =\\&= \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\dfrac{((\mu+x)-\mu)^2}{2\sigma^2}}
        = f_X(\mu+x)
    \end{align*}

    Veamos ahora lo pedido. Como $f_X$ es una función de densidad, tenemos que:
    \begin{align*}
        P[X\geq \mu+x] &= 1-P[X\leq \mu+x] = 1-\int_{-\infty}^{\mu+x} f_X(t)~dt
        =\\&= \int_{-\infty}^{+\infty} f_X(t)~dt - \int_{-\infty}^{\mu+x} f_X(t)~dt
        = \int_{\mu+x}^{+\infty} f_X(t)~dt
    \end{align*}
    donde podemos restar las integrales puesto que todas ellas son convergentes.
    Aplicamos ahora el cambio de variable $t=\mu+u$:
    \begin{equation*}
        \MetInt{t=\mu+u}{\dfrac{dt}{du} = 1} \qquad \left\{
            \begin{array}{l}
                \lim\limits_{u\to x} t = \mu+x \\
                \lim\limits_{u\to \infty} t = +\infty
            \end{array}
        \right.
    \end{equation*}

    Por tanto, tenemos que:
    \begin{align*}
        P[X\geq \mu+x] &= \int_{\mu+x}^{+\infty} f_X(t)~dt
        = \int_{x}^{\infty} f_X(\mu+u)~du
        = \int_{x}^{\infty} f_X(\mu-u)~du
    \end{align*}
    
    Notemos que este primer cambio de variable ha sido esencial para poder aplicar la simetría.
    Aplicamos ahora el cambio de variable $u=-v+\mu$:
    \begin{equation*}
        \MetInt{u=-v+\mu}{\dfrac{du}{dv} = -1} \qquad \left\{
            \begin{array}{l}
                \lim\limits_{v\to \mu-x} t = x \\
                \lim\limits_{v\to -\infty} t = +\infty
            \end{array}
        \right.
    \end{equation*}

    Por tanto, tenemos que:
    \begin{align*}
        P[X\geq \mu+x]
        &= \int_{x}^{\infty} f_X(\mu-u)~du
        = -\int_{\mu-x}^{-\infty} f_X(v)~dv
        = \int_{-\infty}^{\mu-x} f_X(v)~dv
        =\\&= P[X\leq \mu-x]
    \end{align*}

    Notemos que, de forma intuitiva, lo que hacemos con el primer cambio de variable es ``llevarlo al eje de simetría'',
    y en ese eje aplicamos la simetría y ``deshacemos'' el cambio hecho anteriormente.
\end{proof}

Otra propieda importante de la distribución normal es que la media, mediana y moda coincide.
\begin{coro}
    Sea $X$ una variable aleatoria tal que $X\sim \cc{N}(\mu,\sigma^2)$. Entonces:
    \begin{equation*}
        \mu = E[X] = Me[X] = Mo[X]
    \end{equation*}
\end{coro}
\begin{proof}
    Calculemos por separado ambos valores:
    \begin{description}
        \item[Cálculo de la Mediana] Sabiendo que la distribución es simétrica respecto a su media, 
        veamos que $P[X\leq \mu] = P[X\geq \mu] = \nicefrac{1}{2}$.

        La primera igualdad es directa, puesto que $P[X\leq \mu] = P[X\geq \mu]$ por ser simétrica respecto de $\mu$.
        Posteriormente, tenemos que:
        \begin{align*}
            P[X\geq \mu] &= 1-P[X\leq \mu] = 1-P[X\geq \mu] \Longrightarrow P[X\geq \mu] = \dfrac{1}{2}
        \end{align*}

        Por tanto, $\mu=Me[X]$.

        \item[Cálculo de la Moda] Es el máximo absoluto de la función de densidad. Calculémoslo mediante el estudio de la primera derivada:
        \begin{equation*}
            f_X'(x) = \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\dfrac{(x-\mu)^2}{2\sigma^2}} \left(-\dfrac{2(x-\mu)}{2\sigma^2}\right) = 0\Longleftrightarrow x=\mu
        \end{equation*}
        
        Por tanto, vemos que el único candidato a extremo relativo es $\mu$. Además, vemos que $f'_X$ es creciente para $x<\mu$ y decreciente para $x>\mu$,
        de lo que deducimos que $\mu$ es un máximo absoluto. Por tanto, $\mu = Mo[X]$.
        \qedhere
    \end{description}
\end{proof}

\begin{teo}[Regla de la Probabilidad Normal]
    Sea $X$ una variable aleatoria continua tal que $X\sim \cc{N}(\mu,\sigma^2)$. Entonces:
    \begin{enumerate}
        \item $P[\mu-\sigma\leq X\leq \mu+\sigma] \approx 0.6826$
        \item $P[\mu-2\sigma\leq X\leq \mu+2\sigma] \approx 0.9544$
        \item $P[\mu-3\sigma\leq X\leq \mu+3\sigma] \approx 0.9974$
    \end{enumerate}
\end{teo}
\begin{proof}
    Vamos a demostrar el primer apartado, siendo los demás análogos.
    \begin{align*}
        P[\mu-\sigma\leq X\leq \mu+\sigma] &= P\left[\dfrac{\mu-\sigma-\mu}{\sigma} \leq Z\leq \dfrac{\mu+\sigma-\mu}{\sigma}\right]
        = P[-1\leq Z\leq 1] =\\&= P[Z\leq 1] - P[Z\leq -1]
        = P[Z\leq 1] -P[Z\geq 1]=\\&= 2P[Z\leq 1] -1 \approx 2\cdot 0.8413-1 \approx 0.6826
    \end{align*}
    donde $Z$ representa la variable aleatoria tipificada de $X$ y,
    al terminar, hemos consultado los valores en la tabla de la distribución normal estándar.
\end{proof}

Su gráfica es ampliamente conocida y tiene forma de campana, como podemos ver en la Figura \ref{fig:normal}.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            no markers, domain=-3:3, samples=80,
            axis lines*=left, xlabel=$x$, ylabel=$f_X(x)$,
            height=5cm, width=12cm,
            xtick={-3,-2,-1,0,1,2,3}, ytick=\empty,
            enlargelimits=false, clip=false, axis on top,
            grid = major
        ]

            % Bucle para distintos valores de sigma^2 y mu, con leyenda
            \addplot [very thick,cyan!50!black] {1/(sqrt(2*pi)*1) * exp(-x^2/2)};
            \addlegendentry{$\mu=0, \sigma^2=1.0$}

            \addplot [very thick,red!50!black] {1/(sqrt(2*pi)*0.5) * exp(-x^2/0.5)};
            \addlegendentry{$\mu=0, \sigma^2=0.5$}

            \addplot [very thick,green!50!black] {1/(sqrt(2*pi)*2) * exp(-(x-1)^2/2)};
            \addlegendentry{$\mu=1, \sigma^2=2.0$}
        \end{axis}
    \end{tikzpicture}
    \caption{Función de densidad de una v. a. con distribución normal.}
    \label{fig:normal}
\end{figure}


\subsection{Aproximaciones}

La distribución normal es una de las más importantes en la Estadística, y es común que se utilice para aproximar otras distribuciones.
Esto se debe a que la distribución normal es una de las más sencillas de trabajar. En esta sección estudiaremos algunas de estas aproximaciones.

\begin{observacion}
    Notemos que estas aproximaciones solo tienen sentido hoy en día histórico o docente, ya que
    actualmente se dispone de herramientas computacionales que permiten trabajar con cualquier distribución sin necesidad de aproximarla.
    En el pasado, no obstante, estas aproximaciones eran muy útiles al no existir dichas herramientas.
    Igual ocurre en el ámbito docente actualmente.
\end{observacion}

\begin{prop}[Aproximación de la Binomial por la Normal]
    Sea $X$ una variable aleatoria tal que $X\sim B(n,p)$. Entonces, si $n$ es suficientemente grande y $p$ no está muy cerca de 0 o 1, se tiene que $X$
    se puede aproximar por una distribución normal con parámetros $\mu = np$ y $\sigma^2 = np(1-p)$. Es decir:
    \begin{equation*}
        X\sim \cc{N}(np,np(1-p))
    \end{equation*}

    Empíricamente se ha comprobado que esta aproximación es buena si $n\geq 30$ y $0.1\leq p\leq 0.9$.
\end{prop}


\begin{prop}[Aproximación de la Poisson por la Normal]
    Sea $X$ una variable aleatoria tal que $X\sim P(\lambda)$. Entonces, si $\lambda$ es suficientemente grande, se tiene que $X$
    se puede aproximar por una distribución normal con parámetros $\mu = \lambda$ y $\sigma^2 = \lambda$. Es decir:
    \begin{equation*}
        X\sim \cc{N}(\lambda,\lambda)
    \end{equation*}

    Empíricamente se ha comprobado que esta aproximación es buena si $\lambda\geq 10$.
\end{prop}

% // TODO: Aproximaciones por la Normal


\subsubsection{Corrección por Continuidad}

Notemos que en muchos casos, como en las dos aproximaciones anteriores, se trata de aproximar una variable aleatoria discreta por una continua.
En estos casos, es posible caer en el siguiente error.
Supongamos $X$ una variable aleatoria discreta que sigue una distribución binomial,
y sea $x_i$ un valor de la variable aleatoria con $P[X=x_i]>0$. Al aproximarlo por una normal, se tendría que $P[X=x_i] = 0$, ya que la normal es continua.
Esto es incoherente, por lo que se introduce una \emph{corrección por continuidad}.

Esta corrección por continuidad siempre se realiza sumando o restando 0.5 (Este valor se ha establecido así porque, empíricamente, se ha comprobado que mejora la aproximación.) a los extremos de la desigualdad (según convenga).
Lo que buscaremos es cubrir los valores de la variable aleatoria discreta en un intervalo de la variable aleatoria continua. Veamos algunos ejemplos:
\begin{itemize}
    \item Para aproximar $P[X= x_i]$ en la binomial, se calculará con la expresión dada por $P[x_i-0.5 \leq X\leq x_i+0.5]$ en la normal.
    \item Para aproximar $P[X\leq x_i]$ en la binomial, se calculará $P[X\leq x_i+0.5]$ en la normal.
    \item Para aproximar $P[X\geq x_i]$ en la binomial, se calculará $P[X\geq x_i-0.5]$ en la normal.
\end{itemize}


\section{Distribución Exponencial}

\begin{definicion}[Distribución Exponencial]
    Una variable aleatoria continua $X$ sigue una distribución exponencial con parámetro $\lambda\in \bb{R}^+$, si su función de densidad es:
    \begin{equation*}
        f_X(x) = \begin{cases}
            \lambda e^{-\lambda x} & x\geq 0\\
            0 & x<0
        \end{cases}
    \end{equation*}
    Lo denotaremos como $X\sim \exp(\lambda)$.
\end{definicion}

Comprobemos ahora que efectivamente es una función de densidad.
\begin{proof}
    La función de densidad debe cumplir las siguientes propiedades:
    \begin{itemize}
        \item $f_X(x)\geq 0$ para todo $x\in \mathbb{R}$.
        
        Esto es directo puesto que el término exponencial siempre es positivo.
        
        \item $\displaystyle \int_{-\infty}^{\infty} f_X(x) \, dx = 1$.
        \begin{align*}
            \int_{-\infty}^{\infty} f_X(x) \, dx &= \int_{0}^{\infty} \lambda e^{-\lambda x}~dx = \lambda \int_{0}^{\infty} e^{-\lambda x}~dx = \left[ -e^{-\lambda x} \right]_{0}^{\infty}
            = 1
        \end{align*}
    \end{itemize}
\end{proof}

Veamos algunas aplicaciones de esta distribución:
\begin{itemize}
    \item La distribución exponencial se utiliza para modelar el tiempo aleatorio entre dos fallos consecutivos en \emph{fiabilidad}
    o entre dos llegadas consecutivas en \emph{teoría de colas}.

    \item También se aplica en la modelización de tiempos aleatorios de supervivencia (\emph{Análisis de Supervivencia}).
    
    \item En general, $X$ suele representar un tiempo aleatorio transcurrido entre dos
    sucesos, que se producen de forma aleatoria y consecutiva en el tiempo.
    Dichos sucesos se contabilizan mediante un proceso de Poisson homogéneo.
    El parámetro $\lm$ representa la razón de ocurrencia de dichos sucesos, que
    en este caso es constante.
    % // TODO: Usos de la exponencial. Por qué esa f.densidad?
\end{itemize}

\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \exp(\lambda)$. Entonces, su función de distribución es:
    \begin{equation*}
        F_X(x) = \begin{cases}
            1-e^{-\lambda x} & x\geq 0\\
            0 & x<0
        \end{cases}
    \end{equation*}
\end{prop}
\begin{proof}
    Distinguimos dos casos:
    \begin{itemize}
        \item Si $x<0$, entonces $F_X(x) = 0$.
        \item Si $x\geq 0$, entonces:
        \begin{align*}
            F_X(x) &= \int_{0}^{x} \lambda e^{-\lambda t}~dt
            = \int_{0}^{x} \lambda e^{-\lambda t}~dt
            = \left[ -e^{-\lambda t} \right]_{0}^{x}
            = \left( -e^{-\lambda x} + 1 \right) = 1-e^{-\lambda x}
        \end{align*}
    \end{itemize}
\end{proof}


\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \exp(\lambda)$. Su función generatriz de momentos es:
    \begin{equation*}
        M_X(t) = \dfrac{\lambda}{\lambda-t} \quad \text{si } t<\lambda
    \end{equation*}
\end{prop}
\begin{proof}
    Tenemos que:
    \begin{align*}
        M_X(t) &= E\left[e^{tX}\right] = \int_{0}^{\infty} e^{tx} \lambda e^{-\lambda x}~dx
        = \int_{0}^{\infty} \lambda e^{(t-\lambda)x}~dx
        = \left[ \dfrac{\lambda e^{(t-\lambda)x}}{t-\lambda} \right]_{0}^{\infty}
    \end{align*}

    Para que la integral sea convergente, necesitamos que $t-\lambda<0$, es decir, $t<\lambda$. Tenemos entonces:
    \begin{align*}
        M_X(t) &= \left[ \dfrac{\lambda e^{(t-\lambda)x}}{t-\lambda} \right]_{0}^{\infty}
        = 0 - \dfrac{\lambda e^{0}}{t-\lambda} = \dfrac{\lambda}{\lambda-t}
    \end{align*}
\end{proof}

\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \exp(\lambda)$. Entonces, sus momentos no centrados de orden $k\in \bb{N}$ son:
    \begin{equation*}
        E[X^k] = \dfrac{k!}{\lambda^k}
    \end{equation*}
\end{prop}
\begin{proof}
    Demostramos por inducción sobre $k$ que:
    \begin{equation*}
        \dfrac{d^k}{dt^k} M_X(t) = \dfrac{k!\cdot \lm}{(\lambda-t)^{k+1}}
    \end{equation*}
    \begin{itemize}
        \item \ul{Caso base}: $k=1$.
        \begin{equation*}
            \dfrac{d}{dt} M_X(t) = \dfrac{\lambda}{(\lambda-t)^2} = \dfrac{1! \cdot \lm}{(\lambda-t)^{1+1}}
        \end{equation*}

        \item \ul{Supuesto cierto para $k$, demostramos para $k+1$}:
        \begin{align*}
            \dfrac{d^{k+1}}{dt^{k+1}} M_X(t) &= \dfrac{d}{dt} \left( \dfrac{d^k}{dt^k} M_X(t) \right)
            = \dfrac{d}{dt} \left( \dfrac{k!\cdot \lm}{(\lambda-t)^{k+1}} \right)
            = \dfrac{k!\cdot \lm}{(\lambda-t)^{2k+2}}\cdot (k+1)(\lambda-t)^k
            =\\&= \dfrac{(k+1)k! \cdot \lm}{(\lambda-t)^{k+2}}
            = \dfrac{(k+1)! \cdot \lm}{(\lambda-t)^{k+2}}
        \end{align*}        
    \end{itemize}

    Por tanto, una vez demostrado este resultado, tenemos que:
    \begin{align*}
        E[X^k] &= \dfrac{d^k}{dt^k} M_X(t) \Big|_{t=0}
        = \dfrac{k! \cdot \lm}{(\lambda-0)^{k+1}}
        = \dfrac{k!}{\lambda^k}
    \end{align*}
\end{proof}

Como consecuencia, tenemos que:
\begin{equation*}
    E[X] = \dfrac{1}{\lambda} \qquad \Var[X] = E[X^2] - E[X]^2 = \dfrac{2}{\lambda^2} - \dfrac{1}{\lambda^2} = \dfrac{1}{\lambda^2}
\end{equation*}


\begin{prop}[Falta de memoria]
    Sea $X$ una variable aleatoria tal que $X\sim \exp(\lambda)$. Entonces, se cumple la propiedad de falta de memoria:
    \begin{equation*}
        P(X\geq t + s \mid X\geq s) = P(X\geq t) \qquad \forall t,s \in \bb{R^+}
    \end{equation*}
    \begin{proof}
        Tenemos que:
        \begin{align*}
            P(X\geq t + s \mid X\geq s) &= \frac{P(X\geq t+s, X\geq s)}{P(X\geq s)} = \frac{P(X\geq t+s)}{P(X\geq s)} \AstIg\\
            \AstIg & \dfrac{e^{-\lm(t+s)}}{e^{-\lm s}} = e^{-\lm t} \AstIg P(X\geq t)
        \end{align*}
        donde en $(\ast)$ hemos usado que:
        \begin{equation*}
            P(X\geq x) = 1-P(X<x) = 1-P(X\leq x) = 1-(1-e^{-\lm x}) = e^{-\lm x}
        \end{equation*}
    \end{proof}
    % // TODO : Interpretación Falta de memoria de exponencial?
\end{prop}

La gráfica de la función de densidad de la exponencial es decreciente y asintótica al eje de abscisas, como podemos ver en la Figura \ref{fig:exp}.
\begin{figure}[H]
    \centering
    \begin{tikzpicture}
        \begin{axis}[
            no markers, domain=0:3, samples=80,
            axis lines*=left, xlabel=$x$, ylabel=$f_X(x)$,
            height=5cm, width=12cm,
            xtick={0,1,2,3}, ytick=\empty,
            enlargelimits=false, clip=false, axis on top,
            grid = major
        ]

            % Bucle para distintos valores de lambda, con leyenda
            \addplot [very thick,cyan!50!black] {1*exp(-1*x)};
            \addlegendentry{$\lambda=1.0$}

            \addplot [very thick,red!50!black] {2*exp(-2*x)};
            \addlegendentry{$\lambda=2.0$}

            \addplot [very thick,green!50!black] {0.5*exp(-0.5*x)};
            \addlegendentry{$\lambda=0.5$}
        \end{axis}
    \end{tikzpicture}
    \caption{Función de densidad de una v. a. con distribución exponencial.}
    \label{fig:exp}
\end{figure}

\subsection{Relación con la Distribución Poisson}

La distribución exponencial está estrechamente relacionada con la distribución de Poisson.
\begin{itemize}
    \item Sea $Y$ una variable aleatoria que indica
    el número de sucesos aleatorios que ocurren en un intervalo de tiempo de longitud $t$
    cuando la razón de ocurrencia de dichos sucesos es $\lambda$. Entonces, $Y\sim \cc{P}(\lambda t)$.

    \item Sea $X$ una variable aleatoria que indica el tiempo que transcurre hasta que se produce el primer suceso aleatorio,
    o bien el tiempo que transcurre entre dos sucesos consecutivos, cuando la razón de ocurrencia de dichos sucesos es $\lambda$.
    Entonces, $X\sim \exp(\lambda)$.
\end{itemize}

Su relación es la siguiente:
\begin{equation*}
    P[Y=0] = e^{-\lambda t} = P[X\geq t] = 1-P[X<t] = 1-1+e^{-\lambda t} = e^{-\lambda t}
\end{equation*}
Esto es coherente, ya que la probabilidad de que no se produzca ningún suceso en un intervalo de tiempo de longitud $t$ ($P[Y=0]$) es la misma que la probabilidad de que
el tiempo que transcurra hasta que se produzca el primer suceso sea mayor que $t$ ($P[X\geq t]$).

\section{Distribución de Erlang}

\begin{definicion}[Distribución de Erlang]
    Una variable aleatoria continua $X$ sigue una distribución de Erlang con parámetros $n\in \bb{N}$ y $\lambda\in \bb{R}^+$,
    si modela el tiempo que transcurre hasta que se producen $n$ sucesos aleatorios consecutivos, cuando la razón de ocurrencia de dichos sucesos es $\lambda$.
    Lo denotaremos como $X\sim \cc{E}(n,\lambda)$.
\end{definicion}
\begin{observacion}
    La distribución de Erlang es una generalización de la distribución exponencial.
    En efecto, si $n=1$, entonces la distribución de Erlang se reduce a la distribución exponencial.
\end{observacion}

\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \cc{E}(n,\lambda)$. Entonces, su función de densidad es:
    \begin{equation*}
        f_X(x) = \begin{cases}
            \dfrac{\lambda^n}{\Gamma(n)}x^{n-1}e^{-\lambda x} & x\geq 0\\
            0 & x<0
        \end{cases}
    \end{equation*}
    donde $\Gamma(n) = (n-1)!$.
    % // TODO: Por qué f.densidad de Erlang?
\end{prop}
\begin{proof}
    La función de densidad debe cumplir las siguientes propiedades:
    \begin{itemize}
        \item $f_X(x)\geq 0$ para todo $x\in \mathbb{R}$.
        
        Esto es directo puesto que el término exponencial siempre es positivo.
        
        \item $\displaystyle \int_{-\infty}^{\infty} f_X(x) \, dx = 1$.
        \begin{align*}
            \int_{-\infty}^{\infty} f_X(x) \, dx &= \int_{0}^{\infty} \dfrac{\lambda^n}{\Gamma(n)}x^{n-1}e^{-\lambda x}~dx
            = \dfrac{\lambda^n}{\Gamma(n)} \int_{0}^{\infty} x^{n-1}e^{-\lambda x}~dx
        \end{align*}

        Para calcular la última integral, realizamos inducción sobre $n$ para demostrar que:
        \begin{equation*}
            \int_{0}^{\infty} x^{n-1}e^{-\lambda x}~dx = \dfrac{(n-1)!}{\lambda^n} \qquad
            \forall n\in \bb{N}
        \end{equation*}
        \begin{itemize}
            \item \ul{Caso base}: $n=1$.
            \begin{equation*}
                \int_{0}^{\infty} e^{-\lambda x}~dx = \left[ -\dfrac{e^{-\lambda x}}{\lambda} \right]_{0}^{\infty} = \dfrac{1}{\lambda}
            \end{equation*}

            \item \ul{Supuesto cierto para $n$, demostramos para $n+1$}:
            \begin{align*}
                \int_{0}^{\infty} x^{n}e^{-\lambda x}~dx &=
                \MetInt{u(x) = x^n \quad v'(x) = e^{-\lambda x}}{u'(x) = nx^{n-1} \quad v(x) = -\dfrac{e^{-\lambda x}}{\lambda}}
                =\\&= \left[ -\dfrac{x^n e^{-\lambda x}}{\lambda} \right]_{0}^{\infty} + \dfrac{n}{\lambda} \int_{0}^{\infty} x^{n-1}e^{-\lambda x}~dx
                \AstIg \dfrac{n}{\lambda} \dfrac{(n-1)!}{\lambda^n} = \dfrac{n!}{\lambda^{n+1}}
            \end{align*}
            donde en $(\ast)$ hemos usado la hipótesis de inducción.
        \end{itemize}
        Por tanto, tenemos que:
        \begin{align*}
            \int_{-\infty}^{\infty} f_X(x) \, dx &=
            \dfrac{\lambda^n}{\Gamma(n)} \cdot \dfrac{(n-1)!}{\lambda^n} = 1
        \end{align*}
    \end{itemize}
\end{proof}

\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \cc{E}(n,\lambda)$. Entonces, su función generatriz de momentos es:
    \begin{equation*}
        M_X(t) = \left( \dfrac{\lambda}{\lambda-t} \right)^n \quad \text{si } t<\lambda
    \end{equation*}
\end{prop}
\begin{proof}
    Tenemos que:
    \begin{align*}
        M_X(t) &= E\left[e^{tX}\right] = \int_{0}^{\infty} e^{tx} \dfrac{\lambda^n}{\Gamma(n)}x^{n-1}e^{-\lambda x}~dx
        = \dfrac{\lambda^n}{\Gamma(n)} \int_{0}^{\infty} x^{n-1}e^{(t-\lambda)x}~dx
    \end{align*}
    
    Mediante una inducción análoga a la realizada en la demostración anterior, podemos demostrar (asumiendo que $t<\lambda$) que:
    \begin{equation*}
        \int_{0}^{\infty} x^{n-1}e^{(t-\lambda)x}~dx = \dfrac{\Gamma(n)}{(\lambda-t)^n} \qquad \forall n\in \bb{N}
    \end{equation*}

    Por tanto, tenemos que:
    \begin{align*}
        M_X(t) &= \dfrac{\lambda^n}{\Gamma(n)} \cdot \dfrac{\Gamma(n)}{(\lambda-t)^n} = \left( \dfrac{\lambda}{\lambda-t} \right)^n
    \end{align*}
\end{proof}


\section{Distribución Gamma}

\subsection{Función Gamma}
Previamente al estudio de la distribución Gamma, vamos a estudiar la función Gamma, que es la función que da nombre a la distribución.
Esta es:
\Func{\Gamma}{\bb{R}^+}{\bb{R}^+}{x}{\displaystyle \int_{0}^{\infty} t^{x-1}e^{-t}~dt}

Algunas propiedades son:
\begin{enumerate}
    \item $\Gamma(1) = 1$. \label{prop:gamma1}
    \begin{align*}
        \Gamma(1) &= \int_{0}^{\infty} e^{-t}~dt = \left[ -e^{-t} \right]_{0}^{\infty} = 1
    \end{align*}

    \item $\Gamma(x+1) = x\Gamma(x),\qquad \forall x\in \bb{R}^+$. \label{prop:gamma2}
    
    Mediante integración por partes, tenemos que:
    \begin{align*}
        \Gamma(x+1) &= \int_{0}^{\infty} t^{x}e^{-t}~dt
        = \MetInt{u(t) = t^x \quad v'(t) = e^{-t}}{u'(t) = xt^{x-1} \quad v(t) = -e^{-t}}
        =\\&= \left[ -t^xe^{-t} \right]_{0}^{\infty} + x\int_{0}^{\infty} t^{x-1}e^{-t}~dt
        = x\Gamma(x)
    \end{align*}

    \item $\Gamma(n) = (n-1)!,\qquad \forall n\in \bb{N}$. \label{prop:gamma3}
    
    Se deduce directamente de las dos propiedades anteriores.

    \item \label{prop:gamma4}
    Si $\lm\in \bb{R}^+$, entonces $\displaystyle \int_{0}^{\infty} t^{x-1}e^{-\lm t}~dt = \dfrac{\Gamma(x)}{\lm^x}$.
    
    Hacemos el cambio de variable $t = \nicefrac{u}{\lm}$:
    \begin{align*}
        \int_{0}^{\infty} t^{x-1}e^{-\lm t}~dt
        &= \MetInt{t=\nicefrac{u}{\lm}}{\frac{dt}{du} = \nicefrac{1}{\lm}}
        = \int_{0}^{\infty} \left( \dfrac{u}{\lm} \right)^{x-1}e^{-u}~\dfrac{du}{\lm}
        =\\&= \dfrac{1}{\lm^x} \int_{0}^{\infty} u^{x-1}e^{-u}~du
        = \dfrac{\Gamma(x)}{\lm^x}
    \end{align*}

    \item $\Gamma\left(\nicefrac{1}{2}\right) = \sqrt{\pi}$. \label{prop:gamma5}
    \begin{align*}
        \Gamma\left(\nicefrac{1}{2}\right) &= \int_{0}^{\infty} t^{\nicefrac{-1}{2}}e^{-t}~dt
        = \MetInt{t = \nicefrac{u^2}{2}}{\nicefrac{dt}{du} = u}
        = \int_{0}^{\infty}\sqrt{2}\cdot u^{-1}\cdot  e^{\nicefrac{-u^2}{2}}\cdot u~du
        =\\&= \sqrt{2} \int_{0}^{\infty} e^{\nicefrac{-u^2}{2}}~du
    \end{align*}

    Como la función $x\mapsto e^{-x^2}$ es par, tenemos que:
    \begin{align*}
        \Gamma\left(\nicefrac{1}{2}\right) &= \sqrt{2} \int_{0}^{\infty} e^{\nicefrac{-u^2}{2}}~du
        = \dfrac{\sqrt{2}}{2} \int_{-\infty}^{\infty} e^{\nicefrac{-u^2}{2}}~du
        = \dfrac{1}{\sqrt{2}} \int_{-\infty}^{\infty} e^{\nicefrac{-u^2}{2}}~du
        =\\&= \dfrac{\red{\sqrt{\pi}}}{\sqrt{2\red{\pi}}} \int_{-\infty}^{\infty} e^{\nicefrac{-u^2}{2}}~du
        = \sqrt{\pi}\cdot \int_{-\infty}^{\infty} \dfrac{1}{\sqrt{2\pi}} e^{\nicefrac{-u^2}{2}}~du
        \AstIg \sqrt{\pi}
    \end{align*}
    donde en $(\ast)$ hemos usado que el integrando es la función de densidad de una distribución $\cc{N}(0,1)$.
\end{enumerate}

\subsection{Distribución Gamma}

\begin{definicion}[Distribución Gamma]
    Una variable aleatoria continua $X$ sigue una distribución Gamma con parámetros $u,\lm\in \bb{R}^+$,
    si su función de densidad es:
    \begin{equation*}
        f_X(x) = \begin{cases}
            \dfrac{\lambda^u}{\Gamma(u)}x^{u-1}e^{-\lambda x} & x\geq 0\\
            0 & x<0
        \end{cases}
    \end{equation*}
    Lo denotaremos como $X\sim \Gamma(u,\lambda)$.
    \begin{itemize}
        \item El parámetro $u$ se conoce como \emph{parámetro de forma}.
        \item El parámetro $\lm$ se conoce como \emph{parámetro de escala}.
    \end{itemize}
\end{definicion}
\begin{observacion}
    La distribución Gamma es una generalización de la distribución de Erlang.
    En efecto, si $u\in \bb{N}$, entonces la distribución Gamma se reduce a la distribución de Erlang.
\end{observacion}

\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \Gamma(u,\lambda)$. Entonces,
    su función de densidad cumple las condiciones de una función de densidad.
\end{prop}
\begin{proof}
    La función de densidad debe cumplir las siguientes propiedades:
    \begin{itemize}
        \item $f_X(x)\geq 0$ para todo $x\in \mathbb{R}$.
        
        Esto es directo puesto que el término exponencial siempre es positivo.
        
        \item $\displaystyle \int_{-\infty}^{\infty} f_X(x) \, dx = 1$.
        \begin{align*}
            \int_{-\infty}^{\infty} f_X(x) \, dx &= \int_{0}^{\infty} \dfrac{\lambda^u}{\Gamma(u)}x^{u-1}e^{-\lambda x}~dx
            = \dfrac{\lambda^u}{\Gamma(u)} \int_{0}^{\infty} x^{u-1}e^{-\lambda x}~dx
            \AstIg \\& \AstIg \dfrac{\lambda^u}{\Gamma(u)}\cdot \dfrac{\Gamma(u)}{\lambda^u} = 1
        \end{align*}
        donde en $(\ast)$ hemos usado la propiedad \ref{prop:gamma4} de la función Gamma.
    \end{itemize}
\end{proof}

\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \Gamma(u,\lambda)$. Entonces, su función generatriz de momentos es:
    \begin{equation*}
        M_X(t) = \left( \dfrac{\lambda}{\lambda-t} \right)^u \quad \text{si } t<\lambda
    \end{equation*}
\end{prop}
\begin{proof}
    Tenemos que:
    \begin{align*}
        M_X(t) &= E\left[e^{tX}\right] = \int_{0}^{\infty} e^{tx} \dfrac{\lambda^u}{\Gamma(u)}x^{u-1}e^{-\lambda x}~dx
        = \dfrac{\lambda^u}{\Gamma(u)} \int_{0}^{\infty} x^{u-1}e^{(t-\lambda)x}~dx
    \end{align*}
    
    Usando de nuevo la propiedad \ref{prop:gamma4} de la función Gamma, como $\lm-t>0$, tenemos:
    \begin{align*}
        M_X(t) &= \dfrac{\lambda^u}{\Gamma(u)} \cdot \dfrac{\Gamma(u)}{(\lambda-t)^u} = \left( \dfrac{\lambda}{\lambda-t} \right)^u
    \end{align*}
\end{proof}

\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \Gamma(u,\lambda)$. Entonces, sus momentos no centrados de orden $k\in \bb{N}$ son:
    \begin{equation*}
        E[X^k] = \dfrac{\Gamma(u+k)}{\lambda^k~\Gamma(u)}
    \end{equation*}
\end{prop}
\begin{proof}
    Hay dos maneras de demostrar este resultado:
    \begin{description}
        \item[Opción 1] Usar la función generatriz de momentos.
        
        Demostramos por inducción sobre $k$ que:
        \begin{equation*}
            \dfrac{d^k}{dt^k} M_X(t) = \left(\dfrac{\lambda}{\lambda-t}\right)^u \cdot \dfrac{\prod\limits_{i=0}^{k-1}(u+i)}{(\lambda-t)^k}
            \qquad \forall k\in \bb{N}
        \end{equation*}
        \begin{itemize}
            \item \ul{Caso base}: $k=1$.
            \begin{equation*}
                \dfrac{d}{dt} M_X(t)
                = u\left( \dfrac{\lambda}{\lambda-t} \right)^{u-1} \cdot \dfrac{\lambda}{(\lambda-t)^2}
                = \left( \dfrac{\lambda}{\lambda-t} \right)^u \cdot \dfrac{u}{\lambda-t}
            \end{equation*}

            \item \ul{Supuesto cierto para $k$, demostramos para $k+1$}:
            \begin{align*}
                \dfrac{d^{k+1}}{dt^{k+1}} M_X(t) &= \dfrac{d}{dt} \left( \dfrac{d^k}{dt^k} M_X(t) \right)
                = \dfrac{d}{dt} \left( \left(\dfrac{\lambda}{\lambda-t}\right)^u \cdot \dfrac{\prod\limits_{i=0}^{k-1}(u+i)}{(\lambda-t)^k} \right)
                =\\&= \left(\dfrac{\lambda}{\lambda-t}\right)^u \cdot \dfrac{u}{\lm-t}
                \cdot \dfrac{\prod\limits_{i=0}^{k-1}(u+i)}{(\lm-t)^k}
                + \left(\dfrac{\lambda}{\lambda-t}\right)^u \cdot \dfrac{\prod\limits_{i=0}^{k-1}(u+i)}{(\lambda-t)^{k+1}} \cdot k
                =\\&= \left(\dfrac{\lambda}{\lambda-t}\right)^u \cdot \dfrac{\prod\limits_{i=0}^{k}(u+i)}{(\lambda-t)^{k+1}}
            \end{align*}        
        \end{itemize}

        Por tanto, una vez demostrado este resultado, tenemos que:
        \begin{align*}
            E[X^k] &= \dfrac{d^k}{dt^k} M_X(t) \Big|_{t=0}
            = \left(\dfrac{\lambda}{\lambda-0}\right)^u \cdot \dfrac{\prod\limits_{i=0}^{k-1}(u+i)}{(\lambda-0)^k}
            = \dfrac{\prod\limits_{i=0}^{k-1}(u+i)}{\lambda^k} = \dfrac{\Gamma(u+k)}{\lambda^k~\Gamma(u)}
        \end{align*}

        \item[Opción 2] Usar la definición de los momentos no centrados.
        \begin{align*}
            E[X^k] &= \int_{0}^{\infty} x^k \dfrac{\lambda^u}{\Gamma(u)}x^{u-1}e^{-\lambda x}~dx
            = \dfrac{\lambda^u}{\Gamma(u)} \int_{0}^{\infty} x^{u+k-1}e^{-\lambda x}~dx
            \AstIg \\ &\AstIg \dfrac{\Gamma(u+k)\lm^u}{\lambda^{u+k}~\Gamma(u)}
            = \dfrac{\Gamma(u+k)}{\lambda^k~\Gamma(u)}
        \end{align*}
        donde en $(\ast)$ hemos usado la propiedad \ref{prop:gamma4} de la función Gamma.
    \end{description}
\end{proof}

Como consecuencia, tenemos que:
\begin{equation*}
    E[X] = \dfrac{u}{\lambda} \qquad
    \Var[X] = E[X^2] - E[X]^2 = \dfrac{u(u+1)}{\lambda^2} - \dfrac{u^2}{\lambda^2} = \dfrac{u}{\lambda^2}
\end{equation*}


Tiene muchas aplicaciones en experimentos o fenómenos aleatorios que tienen asociadas v.a. que siempre son no negativas
y cuyas distribuciones son sesgadas a la derecha.


\section{Distribución Beta}

Previamente al estudio de la distribución Beta, vamos a introducir la función Beta, que es la función que da nombre a la distribución.

\subsection{Función Beta}

\begin{definicion}[Función Beta]
    La función Beta es una función definida como:
    \Func{\beta}{\bb{R}^+ \times \bb{R}^+}{\bb{R}^+}{(p,q)}{\displaystyle \int_{0}^{1} t^{p-1}(1-t)^{q-1}~dt}
\end{definicion}

Algunas propiedades son:
\begin{enumerate}
    \item Es simétrica. Es decir, $\forall p,q\in \bb{R}^+$, se cumple que $\beta(p,q) = \beta(q,p)$.
    \begin{align*}
        \beta(p,q) &= \int_{0}^{1} t^{p-1}(1-t)^{q-1}~dt
        = \MetInt{t=1-u}{dt=-du}
        = -\int_{1}^{0} (1-u)^{p-1}u^{q-1}~du
        =\\&= \int_{0}^{1} (1-u)^{p-1}u^{q-1}~du
        = \beta(q,p)
    \end{align*}

    \item $\beta(p,q) = \dfrac{\Gamma(p)\Gamma(q)}{\Gamma(p+q)}$.
    
    Esta demostración no se incluye por ser requerir de integración en varias variables, siendo por tanto de mayor complejidad.
\end{enumerate}


\subsection{Distribución Beta}

\begin{definicion}[Distribución Beta]
    Una variable aleatoria continua $X$ sigue una distribución Beta con parámetros $p,q\in \bb{R}^+$,
    si su función de densidad es:
    \begin{equation*}
        f_X(x) = \begin{cases}
            \dfrac{1}{\beta(p,q)}x^{p-1}(1-x)^{q-1} & x\in [0,1]\\
            0 & x\notin [0,1]
        \end{cases}
    \end{equation*}
    Lo denotaremos como $X\sim \beta(p,q)$.
\end{definicion}

Comprobemos que la función de densidad cumple las condiciones de una función de densidad.
\begin{proof}
    La función de densidad debe cumplir las siguientes propiedades:
    \begin{itemize}
        \item $f_X(x)\geq 0$ para todo $x\in \mathbb{R}$.
        
        Esto es directo puesto que los términos $x^{p-1}$, $(1-x)^{q-1}$ y $\beta(p,q)$ siempre son positivos.
        
        \item $\displaystyle \int_{-\infty}^{\infty} f_X(x) \, dx = 1$.
        \begin{align*}
            \int_{-\infty}^{\infty} f_X(x) \, dx &= \int_{0}^{1} \dfrac{1}{\beta(p,q)}x^{p-1}(1-x)^{q-1}~dx
            = \dfrac{1}{\beta(p,q)} \int_{0}^{1} x^{p-1}(1-x)^{q-1}~dx
            =\\& = \dfrac{1}{\beta(p,q)}\cdot \beta(p,q) = 1
            \qedhere
        \end{align*}
    \end{itemize}
\end{proof}

\begin{prop}[Simetría]
    Sea $X$ una variable aleatoria tal que $X\sim \beta(p,q)$. Entonces, $1-X\sim \beta(q,p)$.
\end{prop}
\begin{proof}
    Calculemos la función de densidad de $Y=1-X$ usando el Teorema de Cambio de Variable:
    \begin{align*}
        P[Y=y] &= P[X=1-y] = \dfrac{1}{\beta(p,q)}(1-y)^{p-1}y^{q-1}
        = \dfrac{1}{\beta(q,p)}y^{q-1}(1-y)^{p-1}
    \end{align*}
    Tenemos por tanto que $1-X\sim \beta(q,p)$.
\end{proof}


\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \beta(p,q)$. Entonces, sus momentos no centrados de orden $k\in \bb{N}$ son:
    \begin{equation*}
        E[X^k] = \dfrac{\beta(p+k,q)}{\beta(p,q)}
    \end{equation*}
\end{prop}
\begin{proof}
    Usamos la propiedad de la función Beta que relaciona la función Beta con la función Gamma:
    \begin{align*}
        E[X^k] &= \int_{0}^{1} x^k \dfrac{1}{\beta(p,q)}x^{p-1}(1-x)^{q-1}~dx
        = \dfrac{1}{\beta(p,q)} \int_{0}^{1} x^{p+k-1}(1-x)^{q-1}~dx
        =\\&= \dfrac{\beta(p+k,q)}{\beta(p,q)}
    \end{align*}
\end{proof}

Como consecuencia, tenemos que:
\begin{align*}
    E[X] &= \dfrac{\beta(p+1,q)}{\beta(p,q)}
    = \dfrac{\Gamma(p+1)\Gamma(q)}{\Gamma(p+q+1)} \cdot \dfrac{\Gamma(p+q)}{\Gamma(p)\Gamma(q)} = \dfrac{\Gamma(p+1)}{\Gamma(p)} \cdot \dfrac{\Gamma(p+q)}{\Gamma(p+q+1)} = \dfrac{p}{p+q}
\end{align*}

Para la varianza tenemos el siguiente resultado.
\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \beta(p,q)$. Entonces, su varianza es:
    \begin{equation*}
        \Var[X] = \dfrac{pq}{(p+q)^2(p+q+1)}
    \end{equation*}
\end{prop}
\begin{proof} Tenemos que:
    \begin{align*}
        E[X^2] &= \dfrac{\beta(p+2,q)}{\beta(p,q)}
        = \dfrac{\Gamma(p+2)\Gamma(q)}{\Gamma(p+q+2)} \cdot \dfrac{\Gamma(p+q)}{\Gamma(p)\Gamma(q)} = \dfrac{\Gamma(p+2)}{\Gamma(p)} \cdot \dfrac{\Gamma(p+q)}{\Gamma(p+q+2)} = \dfrac{p(p+1)}{(p+q)(p+q+1)} \\
        \Var[X] &= E[X^2] - E[X]^2 = \dfrac{p(p+1)}{(p+q)(p+q+1)} - \dfrac{p^2}{(p+q)^2}
        = \dfrac{p(p+1)(p+q) -p^2(p+q+1)}{(p+q)^2(p+q+1)} =\\
        &= \dfrac{p(p+q)\left[ p+1-p \right]-p^2}{(p+q)^2(p+q+1)} = \dfrac{pq}{(p+q)^2(p+q+1)}
    \end{align*}
\end{proof}


Respecto a la forma que toma la función de densidad de la distribución Beta, podemos ver que toma formas muy variadas en función de los valores de los parámetros $p$ y $q$.
Esto es muy útil para modelar situaciones muy diversas. Tenemos los siguientes ejemplos:
\begin{itemize}
    \item Si $p=q$, entonces la función de densidad es simétrica respecto a $x=\nicefrac{1}{2}$.
    \item Si $p=q=1$, entonces $X\sim \cc{U}(0,1)$.
    \item Si $p<q$, entonces la función de densidad es asimétrica a la derecha, y viceversa.
    \item Si $p<1$ y $q\geq 1$, es decreciente y cóncava, mientras que si $p\geq 1$ y $q<1$, es creciente y convexa.
    \item Si $p,q>1$, entonces tiene solo un máximo.
    \item Si $p,q<1$, entonces tiene solo un mínimo.
\end{itemize}

Por tanto, como hemos descrito, puede tomar formas muy diversas.