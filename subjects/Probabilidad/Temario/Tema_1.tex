\chapter{Distribuciones de Probabilidad Continua}

En el presente capítulo, se estudiarán las distribuciones de probabilidad continua más importantes. Al igual que 
en la asignatura de EDIP se vieron para variables aleatorias discretas, en este tema se presentarán las más
relevantes para variables aleatorias continuas.

\section{Distribución Uniforme Continua}

\begin{definicion}[Distribución Uniforme Continua]
    Una variable aleatoria continua $X$ sigue una distribución uniforme en el intervalo $[a,b]$, con $a,b\in \bb{R}$, $a<b$, si su función de densidad
    toma un valor constante en dicho intervalo, siendo nula fuera de él.
    Lo denotaremos como $X\sim \cc{U}(a,b)$.
\end{definicion}

\begin{prop}
    Sea $X\sim \cc{U}(a,b)$, entonces su función de densidad es:
    \Func{f}{\bb{R}}{[0,1]}{x}{\frac{1}{b-a}}
\end{prop}
\begin{proof}
    Sea $f$ la función de densidad de $X$. Para que sea una función de densidad, debe cumplir:
    \begin{equation*}
        \int_{-\infty}^{\infty} f(x)dx = \int_{a}^{b} f(x)dx = 1
    \end{equation*}

    Como $f$ es constante en $[a,b]$, sea entonces $f(x) = k$ para $x\in [a,b]$. Entonces:
    \begin{equation*}
        \int_{a}^{b} f(x)dx =
        \int_{a}^{b} kdx = k\int_{a}^{b} dx = k(b-a) = 1 \Longrightarrow k = \frac{1}{b-a}
    \end{equation*}

    Por tanto, la función de densidad de $X$ es:
    \begin{equation*}
        f(x) = \frac{1}{b-a} \quad \forall x\in [a,b]
    \end{equation*}
\end{proof}

\begin{prop}
    Sea $X\sim \cc{U}(a,b)$, entonces su función de distribución es:
    \Func{F_X}{\bb{R}}{[0,1]}{x}{\begin{cases}
        0 & x < a\\
        \dfrac{x-a}{b-a} & x\in [a,b]\\
        1 & x > b
    \end{cases}}
\end{prop}
\begin{proof}
    Tenemos que:
    \begin{align*}
        F_X(x) = \int_{-\infty}^x f(t)~dt
        = \int_{a}^{x} \dfrac{1}{b-a}~dt
        = \dfrac{1}{b-a} \int_a^x ~dt
        = \dfrac{x-a}{b-a}
    \end{align*}
\end{proof}

Como consecuencia inmediata a la proposición anterior, vemos como definición alternativa
que, si $X$ es una variable aleatoria continua tal que $X\sim \cc{U}(a,b)$, entonces se tiene que
la probabilidad de que $X$ tome un valor en un intervalo $[c,d]$, con $a\leq c\leq d\leq b$, es
directamente proporcional a la longitud del intervalo.

\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \cc{U}(a,b)$. Su función generatriz de momentos es:
    \begin{equation*}
        M_X(t) = \dfrac{e^{tb} - e^{ta}}{(b-a)t} \quad t\neq 0
    \end{equation*}
    Para $t=0$, $M_X(0) = 1$.
\end{prop}
\begin{proof}
    Veamos en primer lugar el caso $t=0$. Aunque ya esté demostrado en el temario de EDIP, esto es una propiedad
    general de las funciones generatrices de momentos, ya que:
    \begin{equation*}
        M_X(0) = E\left[e^{0X}\right] = E[1] = 1
    \end{equation*}

    Para $t\neq 0$, tenemos que:
    \begin{align*}
        M_X(t) &= E\left[e^{tX}\right] = \int_{a}^{b} e^{tx} \dfrac{1}{b-a}~dx = \dfrac{1}{b-a} \int_{a}^{b} e^{tx}~dx =\\
        &= \dfrac{1}{b-a} \left[ \dfrac{e^{tx}}{t} \right]_{a}^{b} = \dfrac{e^{tb} - e^{ta}}{(b-a)t}
    \end{align*}
\end{proof}

Calculemos ahora los momentos de una variable aleatoria $X$ tal que $X\sim \cc{U}(a,b)$.
\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \cc{U}(a,b)$. Entonces, su momento no centrado de orden $k\in \bb{N}\cup \{0\}$ es:
    \begin{equation*}
        m_k = E[X^k] = \dfrac{b^{k+1} - a^{k+1}}{(k+1)(b-a)}
    \end{equation*}
\end{prop}
\begin{proof}
    Tenemos que:
    \begin{align*}
        m_k = E[X^k] &= \int_{a}^{b} x^k \dfrac{1}{b-a}~dx = \dfrac{1}{b-a} \int_{a}^{b} x^k~dx =\\
        &= \dfrac{1}{b-a} \left[ \dfrac{x^{k+1}}{k+1} \right]_{a}^{b} = \dfrac{b^{k+1} - a^{k+1}}{(k+1)(b-a)}
    \end{align*}
\end{proof}

Como consecuencia, tenemos que:
\begin{equation*}
    m_1 = E[X] = \dfrac{b^2 - a^2}{2(b-a)} = \dfrac{b+a}{2}
\end{equation*}

\begin{prop}
    Sea $X$ una variable aleatoria tal que $X\sim \cc{U}(a,b)$. Entonces, su momento centrado de orden $k\in \bb{N}$ es:
    \begin{equation*}
        \mu_k = \begin{cases}
            0 & k \text{ impar}\\
            \dfrac{(b-a)^k}{(k+1)2^k} & k \text{ par}
        \end{cases}
    \end{equation*}
\end{prop}
\begin{proof}
    Tenemos que:
    \begin{align*}
        \mu_k &= E[(X-m_1)^k] = E\left[\left(X-\dfrac{a+b}{2}\right)^k\right] = \int_{a}^{b} \left(x-\dfrac{a+b}{2}\right)^k \dfrac{1}{b-a}~dx =\\
        &= \dfrac{1}{b-a} \int_{a}^{b} \left(x-\dfrac{a+b}{2}\right)^k~dx
        = \dfrac{1}{b-a} \left[ \dfrac{\left(x-\dfrac{a+b}{2}\right)^{k+1}}{k+1} \right]_{a}^{b}
        =\\&= \dfrac{\left(b-\dfrac{a+b}{2}\right)^{k+1} - \left(a-\dfrac{a+b}{2}\right)^{k+1}}{(k+1)(b-a)}
        = \dfrac{\left(\dfrac{b-a}{2}\right)^{k+1} - \left(-\dfrac{b-a}{2}\right)^{k+1}}{(k+1)(b-a)}
    \end{align*}

    Distinguimos ahora en función de la paridad de $k$:
    \begin{itemize}
        \item Si $k$ es impar, entonces $\mu_k = 0$.
        \item Si $k$ es par, entonces $\mu_k = \dfrac{(b-a)^k}{(k+1)2^k}$.
    \end{itemize}
\end{proof}


Algunos ejemplos de su utilidad son los siguientes:
\begin{itemize}
    \item La distribución uniforme proporciona una representación adecuada para
    redondear las diferencias que surgen al medir cantidades físicas entre los
    valores observados y los reales.
    Por ejemplo, si el peso de una persona se redondea al $kg$ más cercano,
    entonces la diferencia entre el peso observado y el real será algún valor entre
    $-0.5$ y $0.5~kg$. Es común que el error de redondeo siga entonces una distribución
    $\cc{U}(-0.5,0.5)$.
    
    \item La generación de números aleatorios en un intervalo $[a,b]$ debe seguir una distribución
    $\cc{U}(a,b)$.
\end{itemize}


\section{Distribución Normal}

Esta es la distribución de probabilidad más importante en la Teoía de la Probabilidad y la Estadística Matemática.

\begin{definicion}[Distribución Normal]
    Una variable aleatoria continua $X$ sigue una distribución normal con parámetros $\mu\in \bb{R}$ y $\sigma^2 \in \bb{R}^+$, si su función de densidad es:
    \begin{equation*}
        f_X(x) = \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}, \quad x\in \bb{R}
    \end{equation*}
    donde $\sigma=+\sqrt{\sigma^2}$.
    Lo denotaremos como $X\sim \cc{N}(\mu,\sigma^2)$.
\end{definicion}

A pesar de darse como definición, hemos de demostrar que efectivamente es una función de densidad.
Para ello, incluimos el siguiente Lema, cuya demostración no se incluye por su complejidad,
al requerir de integración en varias variables con cambio a coordenadas polares.
\begin{lema}[Integral de Gauss]
    Sea $a,b\in \bb{R}^+$, entonces:
    \begin{equation*}
        \int_{-\infty}^{\infty} e^{-a(x+b)^2} \, dx = \sqrt{\dfrac{\pi}{a}}
    \end{equation*}
\end{lema}

\begin{proof}
    La función $f_X$ debe cumplir las siguientes propiedades:
    \begin{itemize}
        \item $f_X(x)\geq 0$ para todo $x\in \mathbb{R}$.
        
        Esto es directo puesto que el término exponencial siempre es positivo.
        
        \item $\displaystyle \int_{-\infty}^{\infty} f_X(x) \, dx = 1$.
        \begin{align*}
            \int_{-\infty}^{\infty} f_X(x) \, dx &= \int_{-\infty}^{\infty} \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\dfrac{(x-\mu)^2}{2\sigma^2}} \, dx = \dfrac{1}{\sqrt{2\pi}\sigma} \int_{-\infty}^{\infty} e^{-\dfrac{(x-\mu)^2}{2\sigma^2}} \, dx =\\
            &= \MetInt{x=\sigma t +\mu}{\dfrac{dx}{dt} = \sigma} = \dfrac{1}{\sqrt{2\pi}\cancel{\sigma}} \int_{-\infty}^{\infty} e^{-\dfrac{t^2}{2}} \cdot \cancel{\sigma}\, dt \AstIg \dfrac{1}{\sqrt{2\pi}} \cdot \sqrt{2\pi} = 1
        \end{align*}
        donde en $(\ast)$ hemos aplicado la Integral de Gauss con $a=\nicefrac{1}{2}$ y $b=0$.
    \end{itemize}
\end{proof}


\begin{prop}[Tipificación de la Normal]
    Sea $X\sim \cc{N}(\mu,\sigma^2)$. Entonces, la variable aleatoria $Z = \dfrac{X-\mu}{\sigma}$ se dice que es la variable aleatoria tipificada de $X$.
    Se cumple que:
    \begin{enumerate}
        \item $Z\sim \cc{N}(0,1)$.
        \item $P[X\leq x] = P\left[Z\leq \dfrac{x-\mu}{\sigma}\right]$ para todo $x\in \bb{R}$.
    \end{enumerate}
\end{prop}
\begin{proof}
    Demostramos cada uno de los puntos:
    \begin{enumerate}
        \item Para esto, hay que emplear el Teorema de Cambio de Variable de
        variable aleatoria continua a variable aleatoria continua. Tenemos que $Re_X = \bb{R}$, y definimos por comodidad la siguiente función:
        \Func{g}{\bb{R}}{\bb{R}}{x}{\dfrac{x-\mu}{\sigma}}

        Tenemos por tanto que $Z=g(X)$, y como $g$ es biyectiva tenemos que $Re_Z = Re_X = \bb{R}$.
        La inversa de $g$ es:
        \Func{g^{-1}}{\bb{R}}{\bb{R}}{x}{\sigma x + \mu}

        La función de densidad de $Z$ es, por tanto:
        \begin{align*}
            f_Z(z) &= f_X(g^{-1}(z))\left|(g^{-1})(z)\right| = f_X(\sigma z + \mu)\cdot \sigma = \dfrac{1}{\sqrt{2\pi}\cancel{\sigma}}e^{-\dfrac{(\sigma z+\bcancel{\mu} -\bcancel{\mu})^2}{2\sigma^2}} \cdot \cancel{\sigma} =\\&= \dfrac{1}{\sqrt{2\pi}}e^{-\dfrac{z^2}{2}}
        \end{align*}

        Por tanto, identificando términos, tenemos que $Z\sim \cc{N}(0,1)$.

        \item Para demostrar esto, tenemos que:
        \begin{align*}
            P[X\leq x] &= \int_{-\infty}^{x} f_X(t)~dt = \int_{-\infty}^{x} \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\dfrac{(t-\mu)^2}{2\sigma^2}}~dt \\
            P\left[Z\leq \dfrac{x-\mu}{\sigma}\right] &= \int_{-\infty}^{\frac{x-\mu}{\sigma}} f_Z(t)~dt = \int_{-\infty}^{\frac{x-\mu}{\sigma}} \dfrac{1}{\sqrt{2\pi}} e^{-\dfrac{t^2}{2}}~dt
        \end{align*}

        Resolvamos la primera integral meidante el Cambio de Variable siguiente:
        \begin{equation*}
            \MetInt{t = \sigma u + \mu}{\dfrac{dt}{du} = \sigma} \qquad
            \left\{
                \begin{array}{l}
                    \text{Cuando } t = -\infty, u = -\infty\\
                    \text{Cuando } t = x, u = \dfrac{x-\mu}{\sigma}
                \end{array}
            \right.
        \end{equation*}

        Por tanto, tenemos que:
        \begin{align*}
            P[X\leq x] &= \int_{-\infty}^{x} \dfrac{1}{\sqrt{2\pi}\sigma} e^{-\dfrac{(t-\mu)^2}{2\sigma^2}}~dt
            \AstIg \int_{-\infty}^{\frac{x-\mu}{\sigma}} \dfrac{1}{\sqrt{2\pi}\cancel{\sigma}} e^{-\dfrac{u^2}{2}}\cdot \cancel{\sigma} ~ du
            =\\&= \int_{-\infty}^{\frac{x-\mu}{\sigma}} \dfrac{1}{\sqrt{2\pi}} e^{-\dfrac{u^2}{2}}~du
            = P\left[Z\leq \dfrac{x-\mu}{\sigma}\right]
        \end{align*}
        donde en $(\ast)$ hemos empleado el cambio de variable.
    \end{enumerate}
\end{proof}

\begin{prop}
    Sea $X\sim \cc{N}(\mu,\sigma^2)$. Entonces, su función generatriz de momentos es:
    \begin{equation*}
        M_X(t) = e^{\mu t + \frac{\sigma^2 t^2}{2}}
    \end{equation*}
\end{prop}
\begin{proof}
    Demostramos en primer lugar el caso para la variable $Z\sim \cc{N}(0,1)$:
    \begin{align*}
        M_Z &= E\left[e^{tZ}\right]
        = \int_\bb{R} e^{tz} \dfrac{1}{\sqrt{2\pi}} e^{-\dfrac{z^2}{2}}~dz
        = \dfrac{1}{\sqrt{2\pi}} \int_\bb{R} e^{tz-\dfrac{z^2}{2}}~dz
        = \dfrac{1}{\sqrt{2\pi}} \int_\bb{R} e^{-\dfrac{z^2-2tz + t^2-t^2}{2}}~dz
    \end{align*}
\end{proof}