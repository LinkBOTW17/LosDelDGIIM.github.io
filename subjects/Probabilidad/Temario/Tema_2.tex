\chapter{Vectores Aleatorios}

Hasta ahora, hemos estudiado variable aleatoria unidimensional.
En este capítulo, vamos a estudiar variables aleatorias multidimensionales, es decir, vectores aleatorios.
Para ello, al igual que como hicimos con las variables aleatorias unidimensionales, hemos de definir en primer lugar la $\sigma-$álgebra de Borel en $\mathbb{R}^n$.
\begin{definicion}
    Sea $\mathbb{R}^n$ el espacio euclídeo de dimensión $n$.
    La $\sigma-$álgebra de Borel en $\mathbb{R}^n$, notada por $\cc{B}^n$, es la $\sigma-$álgebra generada por todos los intervalos de $\bb{R}^n$.
\end{definicion}

En particular, en Análisis Matemático II vimos que esta $\sigma-$álgebra está formada por los intervalos:
\begin{equation*}
    \left]-\infty, x\right] := \left]-\infty, x_1\right] \times \cdots \times \left]-\infty, x_n\right], \quad x = (x_1, \ldots, x_n) \in \mathbb{R}^n.
\end{equation*}

Además, en los presentes apuntes, usaremos la relación parcial de orden en $\mathbb{R}^n$ siguiente.
\begin{notacion}
    Dado $x,y\in \mathbb{R}^n$, notaremos:
    \begin{equation*}
        x\leq y \iff x_i\leq y_i, \quad \forall i=1, \ldots, n.
    \end{equation*}
    Esta es parcial, ya que no podemos comparar ciertos elementos, como $(1, 2)$ y $(2, 1)$.\\

    Gráficamente, en el plano tenemos que $x\leq z'$ si y solo si $x$ está a la izquierda y por debajo de $x'$.
    \begin{figure}[H]
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                axis lines = left,
                xlabel = $x$,
                ylabel = $y$,
                xmin = 0, xmax = 3,
                ymin = 0, ymax = 3,
                xtick = \empty,
                ytick = \empty
            ]   
                \addplot[only marks] coordinates {(2.7,2.2)} node[above]{$x'$};
                \addplot[only marks] coordinates {(1,0.5)} node[above]{$x$};
            \end{axis}
        \end{tikzpicture}
        \caption{Relación de orden en $\mathbb{R}^2$, donde $x\leq x'$.}
    \end{figure}
\end{notacion}

Veamos ahora el equivalente a variable aleatoria en el caso multidimensional.
\begin{definicion}[Vector aleatorio]
    Un vector aleatorio $X=(X_1, \ldots, X_n)$ de un espacio de probabilidad $(\Omega, \mathcal{A}, P)$ se define como una función medible:
    \begin{equation*}
        X~:~(\Omega, \mathcal{A}, P) \to (\mathbb{R}^n, \mathcal{B}^n)
    \end{equation*}
    tal que se cumple que:
    \begin{equation*}
        X^{-1}(B)\in \mathcal{A}, \quad \forall B\in \mathcal{B}^n.
    \end{equation*}
\end{definicion}

Es decir:
\begin{align*}
    X^{-1}(\left]-\infty, x\right]) &= \{\omega\in \Omega \mid X_1(\omega)\leq x_1, \ldots, X_n(\omega)\leq x_n\} \in \mathcal{A}
    \quad \forall x\in \mathbb{R}^n.
\end{align*}

Además, considerando cada una de las componentes por separado, como cada componente de una función medible es medible, se tiene la siguiente caracterización de forma directa.
\begin{teo}
    Sea $X=(X_1, \ldots, X_n): (\Omega, \mathcal{A}, P) \to (\mathbb{R}^n, \mathcal{B}^n)$. Entonces:
    \begin{equation*}
        X \text{ es un vector aleatorio} \iff X_i \text{ es una variable aleatoria } \forall i=1, \ldots, n.
    \end{equation*}
\end{teo}

Introducimos ahora la función de distribución de un vector aleatorio, que será el equivalente a función de distribución (o función masa de probabilidad) en el caso unidimensional.
\begin{definicion}[Distribución de probabilidad]
    Sea $X$ un vector aleatorio. La distribución de probabilidad de $X$ es la medida de probabilidad en $\mathbb{R}^n$ definida por:
    \Func{P_X}{\mathcal{B}^n}{[0,1]}{B}{P_X(B) = P(X^{-1}(B)), \quad \forall B\in \mathcal{B}^n.}
\end{definicion}

\begin{notacion}
    Al igual que en el caso unidimensional, dado $B\in \mathcal{B}^n$, tenemos:
    \begin{equation*}
        X^{-1}(B) = \{\omega\in \Omega \mid X(\omega)\in B\}
    \end{equation*}
    Por tanto, denotaremos $P_X(B)$ por $P[X\in B]$.
\end{notacion}

\begin{prop}
    Sea $X$ un vector aleatorio. Entonces, la distribución $P_X$ es una medida de probabilidad sobre $(\mathbb{R}^n, \mathcal{B}^n)$.
\end{prop}
\begin{proof} Veamos que se cumplen las tres propiedades de la Axiomática de Kolmogorov:
    \begin{enumerate}
        \item \ul{No negatividad}: $P_X(B) = P(X^{-1}(B)) \geq 0,\qquad \forall B\in \mathcal{B}^n$ ya que $P$ es una medida de probabilidad.
        \item \ul{Suceso seguro}: $P_X(\mathbb{R}^n) = P(X^{-1}(\mathbb{R}^n)) = P(\Omega) = 1$.
        \item \ul{$\sigma-$aditividad}: Sean $B_1, B_2, \ldots \in \mathcal{B}^n$ disjuntos dos a dos. Entonces, como $X^{-1}(B_1), X^{-1}(B_2), \ldots$ son disjuntos dos a dos, se tiene:
        \begin{align*}
            P_X\left(\bigcup_{i=1}^\infty B_i\right) &= P\left(X^{-1}\left(\bigcup_{i=1}^\infty B_i\right)\right) = P\left(\bigcup_{i=1}^\infty X^{-1}(B_i)\right)\AstIg\\
            &\AstIg \sum_{i=1}^\infty P(X^{-1}(B_i)) = \sum_{i=1}^\infty P_X(B_i).
        \end{align*}
        donde en $(\ast)$ hemos usado la propiedad de $\sigma-$aditividad de la medida de probabilidad $P$.
    \end{enumerate}
\end{proof}


Así, tenemos que todo vector aleatorio $X$ transforma el espacio de probabilidad $(\Omega, \mathcal{A}, P)$ en el espacio de probabilidad $(\mathbb{R}^n, \mathcal{B}^n, P_X)$.\\

Al igual que en el caso unidimensional, definimos la función de distribución de un vector aleatorio a partir de la distribución de probabilidad.
\begin{definicion}[Función de distribución]
    Sea $X$ un vector aleatorio. La función de distribución de $X$ es la función:
    \Func{F_X}{\mathbb{R}^n}{[0,1]}{x}{F_X(x) = P[X\leq x] = P_X(\left]-\infty, x\right])}

    Si $X=(X_1, \ldots, X_n)$, entonces denotaremos:
    \begin{equation*}
        F_X(x) = P[X_1\leq x_1, \ldots, X_n\leq x_n].
    \end{equation*}
\end{definicion}

Algunas de las propiedades que cumple esta son:
\begin{enumerate}
    \item Es monótona no decreciente en cada una de sus componentes. Es decir, $\forall~i=1, \ldots, n$ y $\forall~x_1, \dots, x_{i-i}, x_{i+1}, \dots, x_n \in \bb{R}$ se tiene que:
    \begin{equation*}
        x_i \leq x_i'
        \Longrightarrow
        F_X(x_1, \ldots, x_{i-1}, x_i, x_{i+1}, \ldots, x_n) \leq F_X(x_1, \ldots, x_{i-1}, x_i', x_{i+1}, \ldots, x_n).
    \end{equation*}

    \item Es continua por la derecha en cada una de sus componentes. Es decir, $\forall~i=1, \ldots, n$ y $\forall~x_1, \dots, x_{i-i}, x_{i+1}, \dots, x_n \in \bb{R}$ se tiene que:
    \begin{equation*}
        \forall x_i\in \bb{R},\qquad F_X(x_1, \ldots, x_{i-1}, x_i, x_{i+1}, \ldots, x_n) = \lim_{x\to x_i^+} F_X(x_1, \ldots, x_{i-1}, x, x_{i+1}, \ldots, x_n).
    \end{equation*}

    \item $\forall i=1, \ldots, n$ y $\forall x_1, \dots, x_{i-i}, x_{i+1}, \dots, x_n \in \bb{R}$ se tiene que:
    \begin{equation*}
        \lim_{x\to -\infty} F_X(x_1, \ldots, x_{i-1}, x, x_{i+1}, \ldots, x_n) = 0.
    \end{equation*}

    \item Tenemos que:
    \begin{equation*}
        \lim_{\substack{x_i\to +\infty\\i=1, \ldots, n}} F_X(x_1, \ldots, x_n) = 1.
    \end{equation*}

    \item $\forall x_1, \ldots, x_n \in \bb{R}$ y $\forall \veps_1, \ldots, \veps_n\in \bb{R}^+$ se tiene que:
    \begin{align*}
        F_X&(x_1+\veps_1, \ldots, x_n+\veps_n) -\\
        &- \sum_{i=1}^n F_X(x_1 + \veps_1, \ldots, x_{i-1} + \veps_{i-1}, x_i, x_{i+1} + \veps_{i+1}, \ldots, x_n+\veps_n) +\\
        &+ \sum_{\substack{i,j=1\\i<j}}^n F_X(x_1 + \veps_1, \ldots, x_{i-1} + \veps_{i-1}, x_i, x_{i+1} + \veps_{i+1},
        \ldots\\&\hspace{2cm}\ldots, x_{j-1} + \veps_{j-1}, x_j, x_{j+1} + \veps_{j+1}, \ldots, x_n+\veps_n) +\\
        &+\dots+(-1)^n F_X(x_1, \ldots, x_n) \geq 0
    \end{align*}

    % // TODO: Propiedades de la función de distribución multidimensional
    % https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T06_FuncionDistribucion.pdf
\end{enumerate}

Estas propiedades caracterizan la función de distribución de los vectores aleatorios. Es decir,
dada una función que cumple estas propiedades, es la función de distribución de un vector aleatorio.\\

Al igual que ocurría con variables aleatorias unidimensionales, puesto que $P_X$ es una medida de probabilidad, podemos calcular de forma sencilla la probabilidad de intervalos bidimensionales.
\begin{itemize}
    \item $P[a < X_1 \leq b, X_2 \in I] = P[X_1 \leq b, X_2 \in I] - P[X_1 \leq a, X_2 \in I]$.
    \item $P[a < X_1 < b, X_2 \in I] = P[X_1 < b, X_2 \in I] - P[X_1 \leq a, X_2 \in I]$.
    \item $P[X_1 \leq b, c < X_2 \leq d] = P[X_1 \leq b, X_2 \leq d] - P[X_1 \leq b, X_2 \leq c]$.
    \item $P[X_1 \leq b, c \leq X_2 < d] = P[X_1 \leq b, X_2 < d] - P[X_1 \leq b, X_2 < c]$.
    % https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T06_IntervalosFDMulti.pdf#[0,{%22name%22:%22Fit%22}]
\end{itemize}



\section{Clasificación de vectores aleatorios}

Al igual que en el caso unidimensional, podemos clasificar los vectores aleatorios en discretos y continuos. Esto se hace en función de la naturaleza de los valores que toma el vector aleatorio.
\begin{definicion}[Recorrido de un vector aleatorio]
    Sea $X=(X_1, \ldots, X_n)$ un vector aleatorio. El recorrido de $X$ es el conjunto de valores que toma el vector aleatorio:
    \begin{equation*}
        E_X = \{x\in \mathbb{R}^n \mid \exists \omega\in \Omega \text{ tal que } X(\omega) = x\}
        = Img(X).
    \end{equation*}
\end{definicion}

Usando el recorrido de una variable aleatoria unidimensional, tenemos que:
\begin{equation*}
    E_X = \prod_{i=1}^n E_{X_i}.
\end{equation*}

Veamos ahora que el recorrido de un vector aleatorio es el único conjunto cuya probabilidad es $1$. Es decir:
\begin{prop}
    Sea $X=(X_1, \ldots, X_n)$ un vector aleatorio, y sea $A\subset \mathbb{R}^n$. Entonces:
    \begin{equation*}
        P[X\in A] = 1 \iff A = E_X.
    \end{equation*}
\end{prop}
\begin{proof} Demostramos mediante doble implicación.
    \begin{itemize}
        \item[$\Longleftarrow$)] Supongamos que $A = E_X$. Veamos ahora que $P[X\in E_X] = 1$. Tenemos que:
        \begin{equation*}
            P[X\in E_X] = P[X^{-1}(E_X)] = P[\Omega] = 1.
        \end{equation*}

        \item[$\Longrightarrow$)] Supongamos que $P[X\in A] = 1$. Demostramos que $A = E_X$ por doble inclusión.
        \begin{description}
            \item[$\subset$)] Tenemos que:
            \begin{equation*}
                P[X\in A\setminus E_X]
                = P[X\in A] - P[X\in E_X] = 1-P[X\in E_X]
            \end{equation*}

            Usando lo demostrado justo anteriormente, tenemos que $P[X\in E_X] = 1$, luego $P[X\in A\setminus E_X] = 0$;
            es decir, $A\setminus E_X = \emptyset$. Por tanto, $A\subset E_X$.
            \item[$\supset$)] Como $P[X\in E_X]\leq 1$ por definición y, al ser una probabilidad, es una función creciente, tenemos que $E_X\subset A$.
        \end{description}
    \end{itemize}
\end{proof}


\subsection{Vectores aleatorios discretos}
\begin{definicion}
    Un vector aleatorio $X=(X_1, \ldots, X_n)$ es discreto si su recorrido es finito o numerable.
\end{definicion}

Veamos ahora la siguiente caracterización de vectores aleatorios discretos.
\begin{teo}
    Un vector aleatorio $X=(X_1, \ldots, X_n)$ es discreto si y solo si cada una de sus componentes $X_i$ es discreta.
\end{teo}
\begin{proof}~
    \begin{itemize}
        \item[$\Longrightarrow$)] Supongamos que $X$ es discreto. Entonces, su recorrido es finito o numerable. Por tanto, el recorrido de cada una de sus componentes también es finito o numerable.
        \item[$\Longleftarrow$)] Supongamos que cada una de las componentes de $X$ es discreta. Entonces, el recorrido de $X$ es el producto cartesiano de los recorridos de sus componentes, que es finito o numerable.
    \end{itemize}
\end{proof}

Como en el caso de variables unidimensionales, los vectores de tipo discreto se manejan a partir de su función masa de probabilidad, y el tratamiento de este tipo de vectores es totalmente análogo al de las variables discretas.
\begin{definicion}
    Sea $X=(X_1, \ldots, X_n)$ un vector aleatorio discreto. La función masa de probabilidad de $X$ es la función:
    \Func{p_X}{E_X}{[0,1]}{x}{p_X(x) = P[X=x]}
\end{definicion}

Esta satisface:
\begin{enumerate}
    \item $p_X(x)\geq 0$.
    \item $\sum\limits_{x\in E_X} p_X(x) = 1$.
\end{enumerate}

La función de distribución de un vector aleatorio discreto se define por tanto como:
\begin{equation*}
    F_X(x) = P[X\leq x] = \sum_{\substack{t\in E_X\\t\leq x}} P[X=t]
\end{equation*}



\begin{ejemplo}
    Sea el experimento aleatorio de lanzar un dado, y sean las siguientes variables aleatorias:
    \begin{align*}
        X_1 &= \left\{
        \begin{array}{ll}
            -1 & \text{si sale impar} \\
            1 & \text{si sale par}
        \end{array}
        \right.\\
        X_2 &= \left\{
        \begin{array}{ll}
            -2 & \text{si sale 1, 2, 3} \\
            0 & \text{si sale 4} \\
            3 & \text{si sale 5, 6}
        \end{array}
        \right.
    \end{align*}

    Considerado el vector aleatorio $X=(X_1, X_2)$, se pide:
    \begin{enumerate}
        \item Calcular la función masa de probabilidad de $X$.
        
        Tenemos que:
        \begin{align*}
            E_{X_1} &= \{-1, 1\},\\
            E_{X_2} &= \{-2, 0, 3\},
        \end{align*}

        Por tanto,
        \begin{align*}
            E_X &= E_{X_1} \times E_{X_2} = \{(-1, -2), (-1, 0), (-1, 3), (1, -2), (1, 0), (1, 3)\}.
        \end{align*}

        Tenemos por tanto que:
        \begin{itemize}
            \item $P[X=(-1, -2)] = P[\text{sale impar y 1, 2, 3}] = \nicefrac{2}{6}$.
            \item $P[X=(-1, 0)] = P[\text{sale impar y 4}] = \nicefrac{0}{6}=0$.
            \item $P[X=(-1, 3)] = P[\text{sale impar y 5, 6}] = \nicefrac{1}{6}$.
            \item $P[X=(1, -2)] = P[\text{sale par y 1, 2, 3}] = \nicefrac{1}{6}$.
            \item $P[X=(1, 0)] = P[\text{sale par y 4}] = \frac{1}{6}$.
            \item $P[X=(1, 3)] = P[\text{sale par y 5, 6}] = \nicefrac{1}{6}$.
        \end{itemize}


        Podemos resumir esta información como
        \begin{table}[H]
            \centering
            \begin{tabular}{c | c c c | c }
                \scriptsize{$X_1 \backslash X_2$} & $-2$ & 0 & 3 &\phantom{\scriptsize{$X_1 \backslash X_2$}}\\
                \hline
                $-1$ & \nicefrac{2}{6} & 0 & \nicefrac{1}{6}&\\
                1 & \nicefrac{1}{6} & \nicefrac{1}{6} & \nicefrac{1}{6}&\\
                \hline
                &&&&\\
            \end{tabular}
        \end{table}

        \item Calcular la función de distribución de $X$.
        
        Tenemos que:
        \begin{equation*}
            F_X(x_1, x_2) = \begin{cases}
                0 & x_1 < -1 \text{ o } x_2 < -2 \\
                \nicefrac{2}{6} & x_1 \in \left[-1, 1\right[, x_2 \in \left[-2, 3\right[ \\
                \nicefrac{3}{6} & x_1 \in \left[-1, 1\right], x_2\geq 3 \\
                \nicefrac{3}{6} & x_1\geq 1, x_2 \in \left[-2, 0\right[ \\
                \nicefrac{4}{6} & x_1\geq 1, x_2 \in \left[0, 3\right[ \\
                1 & x_1\geq 1, x_2\geq 3
            \end{cases}
        \end{equation*}
        
        \item Calcular $P[X_1 + X_2 \leq 1]$.
        
        En este caso, los valores de $X_1 + X_2$ que cumplen que $X_1 + X_2 \leq 1$ son:
        \begin{equation*}
            B=\{(-1, -2), (1, -2), (1, 0)\}.
        \end{equation*}

        Por tanto,
        \begin{align*}
            P[X_1 + X_2 \leq 1] &= P[X\in B] = P[X=(-1, -2)] + P[X=(1, -2)] + P[X=(1, 0)] =\\&= \nicefrac{2}{6} + \nicefrac{1}{6} + \nicefrac{1}{6} = \nicefrac{4}{6}.
        \end{align*}
    \end{enumerate}
\end{ejemplo}


\subsection{Vectores aleatorios continuos}

\begin{definicion}
    Un vector aleatorio $X=(X_1, \ldots, X_n)$ es continuo si existe una función integrable no negativa $f_X:\mathbb{R}^n\to \mathbb{R}$ tal que su función de distribución es:
    \begin{equation*}
        F_X(x_1, \ldots, x_n) = \int_{-\infty}^{x_1}\cdots\int_{-\infty}^{x_n} f_X(t_1, \ldots, t_n) d t_n \cdots dt_1,
        \quad \forall x_1, \ldots, x_n \in \mathbb{R}.
    \end{equation*}
    A la función $f_X$ se le llama función de densidad de probabilidad de $X$.
\end{definicion}

Además, si $f_X$ es continua en un punto $x=(x_1, \ldots, x_n)\in \mathbb{R}^n$, entonces la función de distribución $F_X$ es derivable en ese punto y se tiene que:
\begin{equation*}
    \frac{\partial^n F_X}{\partial x_1\cdots \partial x_n}(x) = f_X(x).
\end{equation*}
% // TODO: Función de densidad de un vector aleatorio continuo

Esta función $f_X$, por definición, cumple las siguientes propiedades:
\begin{enumerate}
    \item $f_X(x)\geq 0$.
    \item Es integrable en $\mathbb{R}^n$.
    \item $\int_{\mathbb{R}^n} f_X(x)~d x = 1$.
    \begin{equation*}
        \int_{-\infty}^{+\infty}f_X(t)~d t =
        \lim_{x\to +\infty} \int_{-\infty}^{x} f_X(t) = \lim_{x\to +\infty} F_X(x) \AstIg 1.
    \end{equation*}
    donde en $(\ast)$ hemos usado una de las propiedades de la función de distribución.
\end{enumerate}

La función de densidad determina la función de distribución de un vector aleatorio continuo, y por tanto su distribución de probabilidad.
\begin{equation*}
    P_X(B) = P[X\in B]= \int_B f_X(x)~d x, \quad \forall B\in \mathcal{B}^n.
\end{equation*}
% // TODO: Distribución de probabilidad de un vector aleatorio continuo

Debido a lo estudiado en Análisis Matemático II, sabemos que si $E\subset \mathbb{R}^n$ es un conjunto numerable, entonces $P[X\in E] = 0$.


Al igual que en el caso de vectores discretos, tenemos el siguiente resultado.
\begin{teo} \label{teo:caract_cont}
    Sea vector aleatorio $X=(X_1, \ldots, X_n)$ continuo. Entonces, cada una de sus componentes $X_i$ es continua.
    % https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T06_VectorContinuo.pdf
\end{teo}
\begin{proof}
    Fijemos $i\in \{1, \ldots, n\}$, y sea $F_{X_i}$ la función de distribución de $X_i$. Entonces, tenemos que:
    \begin{align*}
        F_{X_i}(x_i) &= P[X_i\leq x_i] = P[X_1\in \mathbb{R}, \ldots, X_{i-1}\in \mathbb{R}, X_i\leq x_i, X_{i+1}\in \mathbb{R}, \ldots, X_n\in \mathbb{R}].=\\
        &= \int_{\bb{R}} \cdots
        \int_{\bb{R}}\int_{-\infty}^{x_i}\int_{\bb{R}} \cdots
        \int_{\bb{R}}
        f_X(t_1, \ldots, t_n) d t_n \cdots dt_{i+1} dt_i dt_{i-1} \cdots dt_1.
        =\\
        &= \int_{-\infty}^{x_i}\int_{\bb{R}} \cdots
        \int_{\bb{R}}
        f_X(t_1, \ldots, t_n) d t_n \cdots dt_{i+1} dt_{i-1} \cdots dt_1 \cdot dt_i
    \end{align*}
    Definimos por tanto:
    \begin{equation*}
        f_{X_i}(x_i) = \int_{\bb{R}} \cdots
        \int_{\bb{R}}
        f_X(t_1, \ldots, t_n) d t_n \cdots dt_{i+1} dt_{i-1} \cdots dt_1.
    \end{equation*}


    Por tanto, tenemos que:
    \begin{equation*}
        F_{X_i}(x_i) = \int_{-\infty}^{x_i} f_{X_i}(t_i)~d t_i.
    \end{equation*}
    Como $f_i$ es integrable y no negativa, tenemos que es la función de densidad de probabilidad de $X_i$. Por tanto, $X_i$ es una variable aleatoria continua.
\end{proof}



\section{Distribuciones marginales}

Dado un vector aleatorio $X=(X_1, \ldots, X_n)$, podemos estudiar las distribuciones de sus componentes por separado. Estas se conocen como distribuciones marginales.

\begin{definicion}
    Sea $X=(X_1, \ldots, X_n)$ un vector aleatorio. La distribución marginal de $X_i$ es la distribución de probabilidad de la variable aleatoria $X_i$. A la distribución de probabilidad de $X$ se le llama distribución conjunta de $X$.
\end{definicion}

Veamos cómo obtener la función de distribución de una variable aleatoria a partir de la función de distribución de un vector aleatorio.
\begin{prop}
    Sea $X=(X_1, \ldots, X_n)$ un vector aleatorio con función de distribución $F_X$. Entonces, la función de distribución de $X_i$ es:
    \begin{equation*}
        F_{X_i}(x_i) = \lim_{\substack{t_j\to +\infty,\\j\neq i}} F_X(t_1, \ldots, t_{i-1}, x_i, t_{i+1}, \ldots, t_n).
    \end{equation*}
    % https://www.ugr.es/~cdpye/CursoProbabilidad/pdf/P_T06_FDMarginal.pdf
\end{prop}
\begin{proof}
    Esta propiedad se deduce de la continuidad de las funciones de probabilidad (Axiomática de Kolmogorov) y del hecho de que:
    \begin{equation*}
        \bb{R} = \lim_{t\to \infty} ]-\infty, t].
    \end{equation*}

    Tenemos que:
    \begin{align*}
        F_{X_i}(x_i) &= P[X_i\leq x_i] = P[X_1\in \bb{R}, \ldots, X_{i-1}\in \bb{R}, X_i\leq x_i, X_{i+1}\in \bb{R}, \ldots, X_n\in \bb{R}] \AstIg\\
        &\AstIg
        \lim_{\substack{t_j\to +\infty,\\j\neq i}}
        P_X\left(]-\infty, t_1]\times \ldots \times  ]-\infty, t_{i-i}]\times ]-\infty, x_i] \times ]-\infty, t_{i+1}] \times \ldots \times ]-\infty, t_n]\right) =\\
        &= \lim_{\substack{t_j\to +\infty,\\j\neq i}} F_X(t_1, \ldots, t_{i-1}, x_i, t_{i+1}, \ldots, t_n).
    \end{align*}
    donde en $(\ast)$ hemos usado la propiedad de continuidad de la función de probabilidad.
\end{proof}

\subsubsection{Caso discreto}

En el caso de vectores aleatorios discretos, la función de masa de probabilidad de una variable aleatoria se obtiene a partir de la función de masa de probabilidad del vector aleatorio.

\begin{prop} \label{prop:dist_marginal_disc}
    Sea $X=(X_1, \ldots, X_n)$ un vector aleatorio discreto con función de masa de probabilidad $p_X$. Entonces, la función de masa de probabilidad de $X_i$ es:
    \begin{equation*}
        p_{X_i}(x_i) = \sum_{\substack{t=(t_1,\dots,t_n)\in E_X\\t_i=x_i}} p_X(t).
    \end{equation*}
    % // TODO: Función de masa de probabilidad marginal
\end{prop}

\subsubsection{Caso continuo}

En el caso de vectores aleatorios continuos, la función de densidad de probabilidad de una variable aleatoria se obtiene a partir de la función de densidad de probabilidad del vector aleatorio.

\begin{prop} \label{prop:dist_marginal_cont}
    Sea $X=(X_1, \ldots, X_n)$ un vector aleatorio continuo con función de densidad de probabilidad $f_X$. Entonces, la función de densidad de probabilidad de $X_i$ es:
    \begin{equation*}
        f_{X_i}(x_i) = \int_{\bb{R}^{n-1}} f_X(x_1, \ldots, x_{i-1}, x_i, x_{i+1}, \ldots, x_n)~d x_1 \cdots d x_{i-1} d x_{i+1} \cdots d x_n.
    \end{equation*}
\end{prop}
Notemos que esta demostración se ha hecho en el Teorema~\ref{teo:caract_cont}.


\section{Distribuciones condicionadas}

Dado un vector aleatorio $X=(X_1, \ldots, X_n)$, podemos estudiar la distribución de una de sus componentes condicionada a que otra de sus componentes tome un valor concreto.

\subsubsection{Caso discreto}
\begin{definicion}
    Sea $X=(X_1, \ldots, X_n)$ un vector aleatorio discreto, $X_i$ una de sus componentes y $x_i^\ast\in \bb{R}$ tal que $P[X_i=x_i^\ast]>0$. La distribución condicionada de $(X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n)$ a $X_i=x_i^\ast$ es la distribución con función de masa de probabilidad:
    \begin{align*}
        P&[X_1=x_1, \ldots, X_{i-1}=x_{i-1}, X_{i+1}=x_{i+1}, \ldots, X_n=x_n \mid X_i=x_i^\ast] =\\&\qquad = \frac{P[X_1=x_1, \ldots, X_{i-1}=x_{i-1}, X_i=x_i^\ast, X_{i+1}=x_{i+1}, \ldots, X_n=x_n]}{P[X_i=x_i^\ast]},\\
        &\qquad \forall (x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n)\mid (x_1, \ldots, x_{i-1}, x_i^\ast, x_{i+1}, \ldots, x_n)\in E_X.
    \end{align*}
\end{definicion}

Comprobemos que esta definición efectivamente es una función masa de probabilidad.
\begin{proof}~
    \begin{enumerate}
        \item En primer lugar, toma valores no negativos, puesto que es el cociente de dos valores no negativos.
        \item Veamos ahora que suma $1$. Para ello, por la Proposición~\ref{prop:dist_marginal_disc}, tenemos que la siguiente sumatoria es la distribución marginal de $X_i$:
        \begin{equation*}
            \sum_{\mathclap{\substack{(x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n) \mid \\(x_1, \ldots, x_{i-1}, x_i^\ast, x_{i+1}, \ldots, x_n)\in E_X}}} P[X_1=x_1, \ldots, X_{i-1}=x_{i-1}, X_i=x_i^\ast, X_{i+1}=x_{i+1}, \ldots, X_n=x_n] = P[X_i=x_i^\ast].
        \end{equation*}
    
        Por tanto, tenemos que:
        \begin{multline*}
            \sum_{\mathclap{\substack{(x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n) \mid \\(x_1, \ldots, x_{i-1}, x_i^\ast, x_{i+1}, \ldots, x_n)\in E_X}}} P[X_1=x_1, \ldots, X_{i-1}=x_{i-1}, X_{i+1}=x_{i+1}, \ldots, X_n=x_n \mid X_i=x_i^\ast]= 1
        \end{multline*}
    \end{enumerate}
\end{proof}

\subsubsection{Caso continuo}
\begin{definicion}
    Sea $X=(X_1, \ldots, X_n)$ un vector aleatorio continuo, $X_i$ una de sus componentes y $x_i^\ast\in \bb{R}$ tal que $f_{X_i}(x_i^\ast)>0$. La distribución condicionada de $(X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n)$ a $X_i=x_i^\ast$ es la distribución con función de densidad de probabilidad:
    \begin{align*}
        f_{X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n \mid X_i=x_i^\ast}(x_1, \ldots, x_{i-1}, x_{i+1}, \ldots, x_n) =
        \dfrac{f_X(x_1, \ldots, x_{i-1}, x_i^\ast, x_{i+1}, \ldots, x_n)}{f_{X_i}(x_i^\ast)}.
    \end{align*}
\end{definicion}

Comprobemos que efectivamente dicha función se trata de una función de densidad.
\begin{proof}~
    \begin{enumerate}
        \item $f_{X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n \mid X_i=x_i^\ast}$ es no negativa, por ser cociente de funciones de densidad.
        \item Es integrable (se deja como ejercicio al lector).
        \item Veamos que integra $1$:
        \begin{align*}
            \int_{\bb{R}^{n-1}}& \dfrac{f_X(x_1, \ldots, x_{i-1}, x_i^\ast, x_{i+1}, \ldots, x_n)}{f_{X_i}(x_i^\ast)} 
            = \dfrac{\displaystyle \int_{\bb{R}^{n-1}} f_X(x_1, \ldots, x_{i-1}, x_i^\ast, x_{i+1}, \ldots, x_n)}{f_{X_i}(x_i^\ast)} 
            \AstIg \\ &\AstIg
            \dfrac{f_{X_i}(x_i^\ast)}{f_{X_i}(x_i^\ast)} = 1
        \end{align*}
        donde en $(\ast)$ hemos usado la Proposición~\ref{prop:dist_marginal_cont}, ya que es la función de densidad de la marginal de $X_i$.
    \end{enumerate}
\end{proof}