\documentclass[12pt]{article}

\input{../../_assets/preambulo.tex}
\usepgfplotslibrary{fillbetween}

\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\begin{document}

    % 1. Foto de fondo
    % 2. Título
    % 3. Encabezado Izquierdo
    % 4. Color de fondo
    % 5. Coord x del titulo
    % 6. Coord y del titulo
    % 7. Fecha

    
    \input{../../_assets/portada}
    \portadaExamen{ffccA4.jpg}{Probabilidad\\Examen IV}{Probabilidad. Examen IV}{MidnightBlue}{-8}{28}{2024-2025}{Arturo Olivares Martos \\ José Juan Urrutia Milán}

    \begin{description}
        \item[Asignatura] Probabilidad.
        \item[Curso Académico] 2023-24.
        \item[Grado] Doble Grado en Ingeniería Informática y Matemáticas.
        \item[Grupo] Único.
        %\item[Profesor] José María Espinar García.
        \item[Descripción] Examen Ordinario 
        \item[Fecha] 17 de enero de 2024.
        \item[Duración] 3 horas.
    
    \end{description}
    \newpage

    \begin{ejercicio}[5 puntos]
        Dado el vector aleatorio continuo $(X,Y)$ distribuido uniformemente en el recinto
        \begin{equation*}
            C = \{(x,y)\in \mathbb{R}^2 \mid y-x<1 \ \land \ x<0,\ y>0\}
        \end{equation*}
        \begin{enumerate}
            \item \textbf{(0.25 puntos)} Obtener la función de densidad conjunta.
            
            Veamos el conjunto $C$ gráficamente en la Figura~\ref{fig:conjuntoC}.
            \begin{figure}[H]
                \centering
                \begin{tikzpicture}
                    \begin{axis}[
                        axis lines = center,
                        xlabel = $x$,
                        ylabel = $y$,
                        xmin = -2,
                        xmax = 1,
                        ymin = -1,
                        ymax = 2,
                        xtick = {-1},
                        ytick = {1},
                        xticklabels = {$-1$},
                        yticklabels = {$1$},
                        legend pos=north east,
                        axis equal,
                    ]
                    \addplot [
                        domain=-2:2,
                        samples=2,
                        color=red,
                        style=dashed,
                    ] {x+1};
                    \addlegendentry{$y-x=1$}


                    % Zona R0
                    % (-1,0) (-1,5) (-3,5) (-3,-5) (2,-5) (2,0) (-1,0)
                    \addplot [
                        fill=gray,
                        fill opacity=0.5,
                        draw=none,
                        forget plot,
                    ] coordinates {(-1,0) (-1,5) (-3,5) (-3,-5) (2,-5) (2,0) (-1,0)};
                    \node at (axis cs:-0.5,-0.5) {$R_0$};

                    % Zona R1 (C)
                    % (-1,0) (0,0) (0,1)
                    \addplot [
                        fill=orange,
                        fill opacity=0.5,
                        draw=none,
                    ] coordinates {(-1,0) (0,0) (0,1)};
                    \node at (axis cs:-0.3,0.3) {$R_1 (C)$};

                    % Zona R2
                    % (-1,0) (-1,4) (0,4) (0,0)
                    \addplot [
                        fill=blue,
                        fill opacity=0.5,
                        draw=none,
                        forget plot,
                    ] coordinates {(-1,0) (-1,4) (0,4) (0,1)};
                    \node at (axis cs:-0.65,1) {$R_2$};

                    % Zona R3
                    % (0,0) (0,1) (2,1) (2,0)
                    \addplot [
                        fill=green,
                        fill opacity=0.5,
                        draw=none,
                        forget plot,
                    ] coordinates {(0,0) (0,1) (2,1) (2,0)};
                    \node at (axis cs:0.7,0.5) {$R_3$};

                    % Zona R4
                    % (0,1) (0,4) (2,4) (2,1)
                    \addplot [
                        fill=yellow,
                        fill opacity=0.5,
                        draw=none,
                        forget plot,
                    ] coordinates {(0,1) (0,4) (2,4) (2,1)};
                    \node at (axis cs:0.7,1.3) {$R_4$};                    
                    \end{axis}
                \end{tikzpicture}
                \caption{Conjunto $C$.}
                \label{fig:conjuntoC}
            \end{figure}

            Como se distribuye uniformemente, la función de densidad conjunta es constante en la región $C$ y nula en el resto del plano. Por tanto, la función de densidad conjunta es:
            \begin{equation*}
                f_{X,Y}(x,y) = \begin{cases}
                    k & \text{si } (x,y) \in C \\
                    0 & \text{en otro caso}
                \end{cases}
            \end{equation*}

            Para obtener la constante $k$, calculamos la integral de la función de densidad conjunta en la región $C$:
            \begin{align*}
                1 &= \int_{\bb{R}^2} f_{X,Y} = \int_{C} k = k\lm(C) = k\cdot \frac{1}{2}\Longrightarrow
                k = 2
            \end{align*}
            \item \textbf{(1.50 puntos)} Obtener la función de distribución de probabilidad conjunta.
            \begin{observacion}
                Se obtiene \textbf{hasta 1 punto} si las integrales se dejan indicadas y \textbf{hasta 1.5 puntos} si se obtienen sus primitivas de forma explícita.
            \end{observacion}

            Estudiamos cada uno de los casos en función de los valores de $x$ e $y$:
            \begin{itemize}
                \item \ul{$Si (x,y)\in R_0$} ($x<-1$ o $y<0$):
                \begin{align*}
                    F_{X,Y}(x,y) &= \int_{-\infty}^{x}\int_{-\infty}^{y} f_{X,Y}(u,v) \ du \ dv
                    = \int_{-\infty}^{x}\int_{-\infty}^{y} 0 \ du \ dv = 0
                \end{align*}

                \item \ul{$Si (x,y)\in R_1$} ($-1\leq x<0$ y $0\leq y<1-x$):
                \begin{align*}
                    F_{X,Y}(x,y) &= \int_{-\infty}^{x}\int_{-\infty}^{y} f_{X,Y}(u,v) \ du \ dv
                    = \int_{-1}^{y-1}\int_{0}^{u+1} 2 \ du \ dv + \int_{y-1}^{x}\int_{0}^{y} 2 \ du \ dv
                    =\\&= 2\int_{-1}^{y-1} (u+1) \ du + 2\int_{y-1}^{x} y \ du
                    = 2\left[\frac{u^2}{2}+u\right]_{-1}^{y-1} + 2y\left[u\right]_{y-1}^{x}
                    =\\&= 2\left[\frac{(y-1)^2}{2} + y-1-\frac{1}{2}+1\right] + 2y(x-y+1)
                    =\\&= (y-1)^2 + 2(y-1) + 1 + 2y(x-y+1)
                    =\\&= (y-1+1)^2 + 2y(x-y+1) = y^2 + 2y(x-y+1)
                    =\\&= y^2 + 2xy - 2y^2 + 2y
                    = -y^2 + 2xy + 2y
                \end{align*}

                \item \ul{$Si (x,y)\in R_2$} ($-1\leq x<0$ y $y-x\geq 1$):
                \begin{align*}
                    F_{X,Y}(x,y) &= \int_{-\infty}^{x}\int_{-\infty}^{y} f_{X,Y}(u,v) \ du \ dv
                    = \int_{-1}^{x}\int_{0}^{u+1} 2 \ du \ dv
                    = 2\int_{-1}^{x} (u+1) \ du
                    =\\&= 2\left[\frac{u^2}{2}+u\right]_{-1}^{x}
                    = 2\left[\frac{x^2}{2}+x+\frac{1}{2}\right]
                    = x^2 + 2x + 1 = (x+1)^2
                \end{align*}

                \item \ul{$Si (x,y)\in R_3$} ($0\leq x$ y $0\leq y<1$):
                \begin{align*}
                    F_{X,Y}(x,y) &= \int_{-\infty}^{x}\int_{-\infty}^{y} f_{X,Y}(u,v) \ du \ dv
                    = \int_{-1}^{y-1}\int_{0}^{u+1} 2 \ du \ dv
                    + \int_{y-1}^{0}\int_{0}^{y} 2 \ du \ dv
                    =\\&= 2\int_{-1}^{y-1} (u+1) \ du + 2\int_{y-1}^{0} y \ du
                    = 2\left[\frac{u^2}{2}+u\right]_{-1}^{y-1} + 2y\left[u\right]_{y-1}^{0}
                    =\\&= 2\left[\frac{(y-1)^2}{2} + y-1+\frac{1}{2}\right] - 2y(y-1)
                    =\\&= (y-1)^2 + 2(y-1)+1 - 2y(y-1)
                    = (y-1+1)^2 - 2y(y-1)
                    =\\&= y^2 - 2y^2 + 2y = -y^2 + 2y = y(2-y)
                \end{align*}

                \item \ul{$Si (x,y)\in R_4$} ($0\leq x$ y $1\leq y$):
                \begin{align*}
                    F_{X,Y}(x,y) &= \int_{-\infty}^{x}\int_{-\infty}^{y} f_{X,Y}(u,v) \ du \ dv
                    = 1
                \end{align*}
            \end{itemize}

            Por tanto, la función de distribución de probabilidad conjunta es:
            \begin{equation*}
                F_{X,Y}(x,y) = \begin{cases}
                    0 & \text{si } x<-1 \ \lor \ y<0 \qquad (x,y)\in R_0 \\
                    -y^2 + 2xy + 2y & \text{si } -1\leq x<0 \ \land \ 0\leq y<1-x \qquad (x,y)\in R_1 \\
                    (x+1)^2 & \text{si } -1\leq x<0 \ \land \ y-x\geq 1 \qquad (x,y)\in R_2 \\
                    y(2-y) & \text{si } 0\leq x \ \land \ 0\leq y<1 \qquad (x,y)\in R_3 \\
                    1 & \text{si } 0\leq x \ \land \ 1\leq y \qquad (x,y)\in R_4
                \end{cases}
            \end{equation*}


            
            \item \textbf{(0.75 puntos)} Obtener las funciones de densidad condicionadas.
            
            Para obtener las funciones de densidad condicionadas, calculamos en primer lugar las funciones de densidad marginales. Tenemos que:
            \begin{align*}
                f_X(x) &= \int_{-\infty}^{\infty} f_{X,Y}(x,y) \ dy
                = \int_{0}^{1-x} 2 \ dy = 2\left[y\right]_{0}^{1-x} = 2(1-x) \qquad \forall x\in \left[-1,0\right] \\
                f_Y(y) &= \int_{-\infty}^{\infty} f_{X,Y}(x,y) \ dx
                = \int_{y-1}^{0} 2 \ dx = 2\left[x\right]_{y-1}^{0} = 2(1-y) \qquad \forall y\in \left[0,1\right]
            \end{align*}

            Por tanto, las funciones de densidad condicionadas son:
            \begin{align*}
                f_{X\mid Y=y}(x) &= \frac{f_{X,Y}(x,y)}{f_Y(y)} = \frac{2}{2(1-y)} = \frac{1}{1-y} \qquad \forall x\in \left[-1,y^*-1\right], \ y\in \left[0,1\right] \\
                f_{Y\mid X=x}(y) &= \frac{f_{X,Y}(x,y)}{f_X(x)} = \frac{2}{2(1-x)} = \frac{1}{1-x} \qquad \forall y\in \left[0,1+x\right], \ x\in \left[-1,0\right]
            \end{align*}
            \item \textbf{(0.25 puntos)} Obtener la probabilidad de que $X-Y > 0$.
            
            Tenemos que:
            \begin{equation*}
                \left\{(x,y)\in \bb{R}^2 \mid x-y>0\right\}\cap C=\emptyset
            \end{equation*}

            Por tanto, la probabilidad de que $X-Y>0$ es nula.
            \item \textbf{(0.25 puntos)} Obtener la probabilidad de que $X+Y < 0$.
            
            Veamos el conjunto $\left\{(x,y)\in \bb{R}^2 \mid x+y<0\right\}$ gráficamente en la Figura~\ref{fig:conjuntoXY}.
            \begin{figure}[H]
                \centering
                \begin{tikzpicture}
                    \begin{axis}[
                        axis lines = center,
                        xlabel = $x$,
                        ylabel = $y$,
                        xmin = -2,
                        xmax = 1,
                        ymin = -1,
                        ymax = 2,
                        xtick = {-1},
                        ytick = {1},
                        xticklabels = {$-1$},
                        yticklabels = {$1$},
                        legend pos=north east,
                        axis equal,
                    ]
                    \addplot [
                        domain=-2:2,
                        samples=2,
                        color=red,
                        style=dashed,
                    ] {x+1};
                    \addlegendentry{$y-x=1$}

                    \addplot [
                        domain=-2:2,
                        samples=2,
                        color=blue,
                        style=dashed,
                    ] {-x};
                    \addlegendentry{$x+y=0$}

                    \addplot [
                        fill=orange,
                        fill opacity=0.5,
                        draw=none,
                    ] coordinates {(-1,0) (0,0) (-0.5,0.5)};
                    
                    % Punto en (-0.5,0.5), con etiqueta
                    \addplot [mark=*] coordinates {(-0.5,0.5)};
                    \node[left] at (axis cs:-0.5,0.5) {$(\nicefrac{-1}{2},\nicefrac{1}{2})$};
                    \end{axis}
                \end{tikzpicture}
                \caption{Conjunto $\left\{(x,y)\in \bb{R}^2 \mid x+y<0\right\}$.}
                \label{fig:conjuntoXY}
            \end{figure}

            Calculamos la probabilidad de que $X+Y<0$:
            \begin{align*}
                P[X+Y<0] &= \int_{-1}^{\nicefrac{-1}{2}}\int_{0}^{1+x} 2 \ dy \ dx + \int_{\nicefrac{-1}{2}}^{0}\int_{0}^{-x} 2 \ dy \ dx
                =\\&= 2\int_{-1}^{\nicefrac{-1}{2}} (1+x) \ dx + 2\int_{\nicefrac{-1}{2}}^{0} -x \ dx
                = 2\left[\frac{x^2}{2}+x\right]_{-1}^{\nicefrac{-1}{2}} + 2\left[-\frac{x^2}{2}\right]_{\nicefrac{-1}{2}}^{0}
                =\\&= 2\left[\frac{1}{8}-\frac{1}{2}-\frac{1}{2}+1\right] + 2\left[0+\frac{1}{8}\right]
                = 2\cdot 2\cdot \frac{1}{8} = \frac{1}{2}
            \end{align*}
            \item \textbf{(1.50 puntos)} Obtener la mejor aproximación minimo cuadrática a la variable aleatoria $Y$ conocidos los valores de la variable $X$ y el error cuadrático medio de esta aproximación.
            
            Para obtener la mejor aproximación mínimos cuadrados de la variable aleatoria $Y$ conocidos los valores de la variable $X$, calculamos la curva de regresión de $Y$ sobre $X$. Para ello, tenemos que:
            \begin{align*}
                E[Y\mid X=x] &= \int_{-\infty}^{\infty} y\cdot f_{Y\mid X=x}(y) \ dy
                = \int_{0}^{1+x} y\cdot \frac{1}{1+x} \ dy
                = \frac{1}{1+x}\int_{0}^{1+x} y \ dy
                =\\&= \frac{1}{1+x}\left[\frac{y^2}{2}\right]_{0}^{1+x}
                = \frac{1}{1+x}\cdot \frac{(1+x)^2}{2}
                = \frac{1+x}{2}
            \end{align*}

            Por tanto, la mejor aproximación mínimos cuadrados de la variable aleatoria $Y$ conocidos los valores de la variable $X$ es:
            \begin{equation*}
                E[Y\mid X] = \frac{1+X}{2}
            \end{equation*}

            Para calcular el error cuadrático medio de esta aproximación, tenemos que:
            \begin{align*}
                \text{E.C.M.}(E[Y\mid X]) &= E[(Y-E[Y\mid X])^2]
                = E\left[\left(Y-\frac{1+X}{2}\right)^2\right]
                =\\&= E\left[Y^2 - Y(1+X) + \frac{(1+X)^2}{4}\right]
                =\\&= E[Y^2] - E[Y]-E[XY] + \frac{1}{4}E[X^2] + \frac{1}{2}E[X] + \frac{1}{4}
            \end{align*}

            De forma análoga, tenemos que:
            \begin{align*}
                \text{E.C.M.}(E[Y\mid X]) &= E[\Var[Y\mid X]]
                = E[E[Y^2\mid X] - E^2[Y\mid X]]
                =\\&= E[Y^2] - E\left[\left(E[Y\mid X]\right)^2\right]
                = E[Y^2] - E\left[\left(\frac{1+X}{2}\right)^2\right]
            \end{align*}

            Como podemos ver, en este caso se trata del cálculo de menos esperanzas, por lo que nos decantamos por esta opción.
            \begin{align*}
                E[Y^2] &= \int_{-\infty}^{\infty} y^2\cdot f_Y(y) \ dy
                = 2\int_{0}^{1} y^2(1-y) \ dy
                = 2\int_{0}^{1} y^2-y^3 \ dy
                =\\&= 2\left[\frac{y^3}{3}-\frac{y^4}{4}\right]_{0}^{1}
                = \frac{2}{3}-\frac{1}{2} = \frac{1}{6} \\
                E\left[\left(\frac{1+X}{2}\right)^2\right] &= \frac{1}{4}\int_{-1}^{0} \left(1+x\right)^2\cdot 2(1+x) \ dx
                = \frac{1}{2}\int_{-1}^{0} (1+x)^3 \ dx
                = \frac{1}{2}\left[\frac{(1+x)^4}{4}\right]_{-1}^{0}
                = \frac{1}{8}
            \end{align*}

            Por tanto, el error cuadrático medio de la mejor aproximación mínimos cuadrados de la variable aleatoria $Y$ conocidos los valores de la variable $X$ es:
            \begin{equation*}
                \text{E.C.M.}(E[Y\mid X]) = E[Y^2] - E\left[\left(\frac{1+X}{2}\right)^2\right] = \frac{1}{6} - \frac{1}{8} = \frac{1}{24}
            \end{equation*}
            \item \textbf{(0.50 puntos)} Obtener una medida de la bondad del ajuste del apartado anterior.\\
            
            Para obtener una medida de la bondad del ajuste del apartado anterior, calculamos el coeficiente de determinación. Para ello, tenemos que:
            \begin{align*}
                \eta^2_{Y / X} &= \frac{\Var[E[Y\mid X]]}{\Var[Y]}
                = 1-\frac{\text{E.C.M.}(E[Y\mid X])}{\Var[Y]}
            \end{align*}

            Calculamos la varianza de $Y$:
            \begin{align*}
                E[Y] &= \int_{-\infty}^{\infty} y\cdot f_Y(y) \ dy
                = 2\int_{0}^{1} y(1-y) \ dy
                = 2\int_{0}^{1} y-y^2 \ dy
                =\\&= 2\left[\frac{y^2}{2}-\frac{y^3}{3}\right]_{0}^{1}
                = 2\left[\frac{1}{2}-\frac{1}{3}\right]
                = \frac{1}{3} \\
                E[Y^2] &= \frac{1}{6} \\
                \Var[Y] &= E[Y^2] - E^2[Y] = \frac{1}{6} - \left(\frac{1}{3}\right)^2 = \frac{1}{18}
            \end{align*}

            Por tanto, el coeficiente de determinación es:
            \begin{align*}
                \eta^2_{Y / X} &= 1-\frac{\text{E.C.M.}(E[Y\mid X])}{\Var[Y]}
                = 1-\frac{\nicefrac{1}{24}}{\nicefrac{1}{18}}
                = 1-\frac{3}{4}
                = \frac{1}{4}
            \end{align*}

            Por tanto, la bondad del ajuste es del 25\%. Como vemos, no se trata de un ajuste de muy buena calidad.
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}[1 puntos]
        Dado un vector aleatorio $(X,Y)$ con función generatriz de momentos
        \begin{equation*}
            M_{(X,Y)}(t_1,t_2) = \exp\left(\dfrac{t_2+16t_1^2 + 4t_2^2 + 10t_1t_2}{2}\right)
        \end{equation*}
        \begin{enumerate}
            \item \textbf{(0.25 puntos)} Obtener la razón de correlación y el coeficiente de correlación lineal de las variables $(X,Y)$.
            
            La función generatriz de un vector aleatorio con distribución $\cc{N}_2(\mu,\Sigma)$ es:
            \begin{equation*}
                f(t_1,t_2) = \exp\left(\mu_1t_1+\mu_2t_2+\dfrac{\sigma_1^2t_1^2+\sigma_2^2t_2^2+2\rho\sigma_1\sigma_2t_1t_2}{2}\right)
            \end{equation*}

            Identificando términos, obtenemos que:
            \begin{equation*}
                \begin{cases}
                    \mu_1 = 0 \\
                    \mu_2 = \nicefrac{1}{2} \\
                    \sigma_1^2 = 16 \\
                    \sigma_2^2 = 4 \\
                    \rho\sigma_1\sigma_2 = 5
                \end{cases}
            \end{equation*}

            Por tanto, tenemos que $(X,Y)\sim \cc{N}_2\left(\mu,\Sigma\right)$ con:
            \begin{equation*}
                \mu = \begin{pmatrix}
                    0 \\ \nicefrac{1}{2}
                \end{pmatrix} \qquad \Sigma = \begin{pmatrix}
                    16 & 5 \\ 5 & 4
                \end{pmatrix}
            \end{equation*}

            Por tanto, tenemos que el coeficiente de correlación lineal es:
            \begin{equation*}
                \rho_{X,Y}=\rho=\dfrac{5}{\sqrt{16}\sqrt{4}} = \dfrac{5}{8}
            \end{equation*}

            Respecto a la razón de correlación, tenemos que:
            \begin{equation*}
                \eta_{X/Y}^2\AstIg \rho_{X,Y}^2 = \eta_{Y/X}^2=\left(\dfrac{5}{8}\right)^2 = \dfrac{25}{64}
            \end{equation*}
            donde en $(\ast)$ se ha utilizado que $(X,Y)$ sigue una distribución normal bivariante.

            \item \textbf{(0.25 puntos)} Indicar las distribuciones de las variables aleatorias $Y\mid X=0$ y $X\mid Y=2$.
            
            Tenemos que:
            \begin{align*}
                Y\mid X=x^* &\sim \cc{N}\left(\mu_2+\rho\cdot \dfrac{\sigma_2}{\sigma_1}(x^*-\mu_1),\sigma_2^2(1-\rho^2)\right) \\
                X\mid Y=y^* &\sim \cc{N}\left(\mu_1+\rho\cdot \dfrac{\sigma_1}{\sigma_2}(y^*-\mu_2),\sigma_1^2(1-\rho^2)\right)
            \end{align*}

            En concreto, para $x^*=0$ y $y^*=2$, tenemos que:
            \begin{align*}
                Y\mid X=0 &\sim \cc{N}\left(\nicefrac{1}{2},4\left(1-\left(\nicefrac{5}{8}\right)^2\right)\right) = \cc{N}\left(\nicefrac{1}{2},\nicefrac{39}{16}\right) \\
                X\mid Y=2 &\sim \cc{N}\left(0,16\left(1-\left(\nicefrac{5}{8}\right)^2\right)\right) = \cc{N}\left(0,\nicefrac{39}{4}\right)
            \end{align*}
            \item \textbf{(0.50 puntos)} Obtener la distribución de probabilidad del vector aleatorio $(2X, Y-X)$. Justificar que las variables aleatorias $2X$ y $Y-X$ tienen asociación lineal muy alta en sentido negativo.
            
            Tenemos que:
            \begin{equation*}
                (2X,Y-X) = \begin{pmatrix}
                    X & Y
                \end{pmatrix}A,\qquad A = 
                \begin{pmatrix}
                    2 & -1 \\ 0 & 1
                \end{pmatrix}
            \end{equation*}

            Por tanto, notando $X'=2X$ y $Y'=Y-X$, tenemos que $(X',Y')\sim \cc{N}(\mu A,A^t\Sigma A)$, donde:
            \begin{align*}
                \mu A &= \begin{pmatrix}
                    0 & \nicefrac{1}{2}
                \end{pmatrix}\begin{pmatrix}
                    2 & -1 \\ 0 & 1
                \end{pmatrix} = \begin{pmatrix}
                    0 & \nicefrac{1}{2}
                \end{pmatrix}
                \\
                A^t\Sigma A &= \begin{pmatrix}
                    2 & 0 \\ -1 & 1
                \end{pmatrix}\begin{pmatrix}
                    16 & 5 \\ 5 & 4
                \end{pmatrix}\begin{pmatrix}
                    2 & -1 \\ 0 & 1
                \end{pmatrix} = \begin{pmatrix}
                    32 & 10 \\ -11 & -1
                \end{pmatrix}
                \begin{pmatrix}
                    2 & -1 \\ 0 & 1
                \end{pmatrix}
                = \begin{pmatrix}
                    64 & -22 \\ -22 & 10
                \end{pmatrix}
            \end{align*}

            Por tanto, tenemos que:
            \begin{align*}
                \rho_{X',Y'} &= \dfrac{-22}{\sqrt{64}\sqrt{10}} \approx -0.87\\
                \rho_{X',Y'}^2 &= \dfrac{22^2}{64\cdot 10} = \dfrac{121}{160} = 0.75625
            \end{align*}

            Por tanto, como $\rho_{X',Y'}^2$ se aproxima a $1$, la asociación lineal entre $2X$ y $Y-X$ es muy alta, y al ser $\rho_{X',Y'}$ negativo, la correlación es negativa.
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}[3 puntos]
        Sea $(X,Y)$ un vector aleatorio. Se pretenden predecir, por mínimos cuadrados, los valores de la variable $Y$ a partir de una función lineal de la variable $X$, y viceversa.
        \begin{enumerate}
            \item \textbf{(2 puntos)} Obtener de forma razonada los coeficientes del modelo lineal de $X$ sobre $Y$.
            
            Se busca aproximar $X$ como $\wh{X}=aY+b$. Para ello, se minimiza el error cuadrático medio:
            \begin{align*}
                \text{E.C.M.}(X\mid Y) &= E[(X-\wh{X})^2]
                = E\left[(X-aY-b)^2\right]
                =\\&= E\left[X^2-2aXY-2bX+a^2Y^2+2abY+b^2\right]
                =\\&= E[X^2]-2aE[XY]-2bE[X]+a^2E[Y^2]+2abE[Y]+b^2
            \end{align*}

            Para ello, se busca minizar la siguiente función:
            \begin{equation*}
                L(a,b) = E.C.M.(X\mid Y) = E[X^2]-2aE[XY]-2bE[X]+a^2E[Y^2]+2abE[Y]+b^2
            \end{equation*}

            Se tiene demostrado en Teoría que llegamos a la siguiente expresión:
            \begin{equation*}
                \begin{cases}
                    a = \dfrac{\Cov[X,Y]}{\Var[Y]} \\
                    b = E[X]-aE[Y]
                \end{cases}
            \end{equation*}
            \item \textbf{(1 punto)} Si $3y-x+1=0$ y $x-2y-1=0$ son las rectas de regresión del vector $(X,Y)$: identificar la recta de regresión de Y sobre X; obtener una medida de la proporción de varianza de cada variable que queda explicada por el modelo de regresión lineal y calcular la esperanza del vector $(X,Y)$.\\
            
            Suponemos que las rectas de regresión de $X$ sobre $Y$ y de $Y$ sobre $X$ son $3y-x+1=0$ y $x-2y-1=0$, respectivamente. Por tanto, tenemos que:
            \begin{align*}
                x &= 3y+1 = \dfrac{\Cov[X,Y]}{\Var[Y]}\cdot y + E[X]-\dfrac{\Cov[X,Y]}{\Var[Y]}\cdot E[Y] \\
                y &= \dfrac{1}{2}x-\dfrac{1}{2} = \dfrac{\Cov[X,Y]}{\Var[X]}\cdot x + E[Y]-\dfrac{\Cov[X,Y]}{\Var[X]}\cdot E[X]
            \end{align*}

            Identificando términos, obtenemos que:
            \begin{equation*}
                \dfrac{\Cov[X,Y]}{\Var[X]}\cdot \dfrac{\Cov[X,Y]}{\Var[Y]} = \rho_{X,Y}^2 = 3\cdot \dfrac{1}{2} = \dfrac{3}{2}>1
            \end{equation*}

            Por tanto, llegamos a una contradicción, por lo que la suposición es incorrecta. La recta de regresión de $Y$ sobre $X$ es $y=\nicefrac{1}{3}x-\nicefrac{1}{3}$, y la de $X$ sobre $Y$ es $x=2y+1$.\\

            La proporción de varianza de cada variable que queda explicada por el modelo de regresión lineal es el coeficiente de determinación, que en este caso es:
            \begin{equation*}
                \rho_{X,Y}^2 = \dfrac{1}{3}\cdot 2=\dfrac{2}{3}\approx 0.667\%
            \end{equation*}

            Por último, por identificación de términos, tenemos el siguiente sistema:
            \begin{equation*}
                \left\{\begin{array}{rcl}
                    E[X]-2E[Y]&=&1 \\
                    E[Y]-\dfrac{1}{3}E[X]&=&-\dfrac{1}{3}
                \end{array}\right\}
                \Longrightarrow
                \begin{cases}
                    E[X]=1 \\
                    E[Y]=0
                \end{cases}
            \end{equation*}

            Por tanto, tenemos que:
            \begin{equation*}
                E[(X,Y)] = \begin{pmatrix}
                    1 & 0
                \end{pmatrix}
            \end{equation*}
        \end{enumerate}
    \end{ejercicio}

    \begin{ejercicio}[1 punto]
        Sean $X_1$, $X_2$, \ldots, $X_n$ $n$ variables aleatorias independientes e identicamente distribuidas según una ley uniforme en el intervalo $[0,\theta]$ con $\theta > 0$. Se considera la sucesión de variables aleatorias cuyo término general es de la forma $X_{(n)} = \max\{X_1, X_2, \ldots, X_n\}$. Probar que la sucesión anterior converge en ley a una variable aleatoria degenerada en $\theta$.\\

        Puesto que no son idénticamente distribuidas, no podemos aplicar el Teorema de Lévy, por lo que hemos de partir de la definición. Para ello, calculamos la función de distribución de $X_{(n)}$. Tenemos que:
        \begin{equation*}
            F_{X_{(n)}}(x) = P[X_{(n)}\leq x] = P[\max\{X_1,X_2,\ldots,X_n\}\leq x] = P[X_1\leq x, X_2\leq x,\ldots,X_n\leq x]
        \end{equation*}

        Puesto que las variables son independientes, tenemos que:
        \begin{equation*}
            F_{X_{(n)}}(x) = P[X_1\leq x]\cdot P[X_2\leq x]\cdot \ldots \cdot P[X_n\leq x]
        \end{equation*}

        Puesto que son idénticamente distribuidas, tenemos que:
        \begin{equation*}
            F_{X_{(n)}}(x) = \left(P[X_1\leq x]\right)^n
        \end{equation*}

        Calculamos por tanto la función de densidad de $X_1$. Como sigue una distribución uniforme en $[0,\theta]$, tenemos que:
        \begin{equation*}
            f_{X_1}(x) = \begin{cases}
                \dfrac{1}{\theta} & \text{si } x\in [0,\theta] \\
                0 & \text{en otro caso}
            \end{cases}
        \end{equation*}

        Por tanto, la función de distribución de $X_1$ es:
        \begin{equation*}
            F_{X_1}(x) = \begin{cases}
                0 & \text{si } x<0 \\
                \dfrac{x}{\theta} & \text{si } x\in [0,\theta] \\
                1 & \text{si } x>\theta
            \end{cases}
        \end{equation*}

        Por tanto, la función de distribución de $X_{(n)}$ es:
        \begin{equation*}
            F_{X_{(n)}}(x) = \begin{cases}
                0 & \text{si } x<0 \\
                \left(\dfrac{x}{\theta}\right)^n & \text{si } x\in [0,\theta[ \\
                1 & \text{si } x\geq\theta
            \end{cases}
        \end{equation*}

        Por tanto, tenemos que:
        \begin{equation*}
            \lim_{n\to\infty} F_{X_{(n)}}(x) = \begin{cases}
                0 & \text{si } x<\theta\\
                1 & \text{si } x\geq \theta
            \end{cases}
        \end{equation*}

        Como esta es la función de distribución de una variable aleatoria degenerada en $\theta$, tenemos que:
        \begin{equation*}
            X_{(n)}\overset{L}{\longrightarrow} \theta
        \end{equation*}
    \end{ejercicio}


\end{document}
